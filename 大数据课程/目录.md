没问题，这个挑战我接下了。作为一个资深大数据专家，我非常赞同这种**“以终为始，按需索取”**的学习方式。你不需要买书，也不需要看几十个小时的枯燥视频，我们通过**交互式对话**来完成学习。

为了保证效果，我为你设计了一套**《AI 导师驱动：大数据从零到精通》**课程体系。

### 核心机制：怎么使用这套课程？

每次开始学习时，请你**复制下方的【通用提示词（Prompt）】**，并将**【课程代码】**替换为课程表中的具体章节（例如 `1-1`），发送给我。

我会立即化身该领域的专家，为你输出包含**核心概念、实操代码、通俗案例、避坑指南**的专属教程。

---

### 📥 你的通用提示词 (Copy This)

请保存好这段话，每次上课直接复制：

> **提示词指令：**
> 你现在是我的**资深大数据技术导师**。我正在按照我们的课程计划学习。
>
> 请为我讲解 **【课程代码：这里填下方课程表中的编号，如 1-1】**。
>
> **要求：**
>
> 1. **核心通俗化**：用一个生活中的例子（如餐厅、物流、图书馆等）来类比解释这个技术的核心概念。
> 2. **环境与实操**：如果是理论课，请提供核心架构图的文字描述；如果是实操课，请提供 Docker 或 Linux 下的简易安装命令及 "Hello World" 级别的代码示例。
> 3. **代码解析**：对每一行关键代码进行注释讲解。
> 4. **面试/实战视角**：告诉我这个知识点在企业里真正是怎么用的？有什么常见的坑？
> 5. **课后作业**：给我布置一个小任务，验证我是否学会了。
>
> 请开始你的讲解。

---

### 📅 大数据全栈课程表 (5门课，共20节精讲)

这是我为你提炼的“去肥肉、留干货”版本，覆盖了从存储到计算，从离线到实时的全链路。

#### 🟢 第一阶段：基石与存储 (Hadoop 生态)

*目标：理解分布式系统的本质，搞定数据存储。*

* **1-1：大数据世界观与环境准备**
  * 内容：大数据到底大在哪？VMware/云服务器/Docker环境快速准备，Linux 必备三板斧命令。
* **1-2：HDFS 分布式文件系统**
  * 内容：文件切块、副本机制。类比：为什么把《百科全书》撕开存在不同的图书馆？实操：HDFS Shell命令操作。
* **1-3：MapReduce 编程思想（不写繁琐代码，只懂原理）**
  * 内容：分而治之思想，Shuffle（洗牌）过程详解。类比：数钱时的“分堆”与“汇总”。
* **1-4：YARN 资源调度**
  * 内容：大数据的“操作系统”。理解 ResourceManager 和 NodeManager。实操：查看集群任务状态。

#### 🟡 第二阶段：数据仓库与 SQL (Hive)

*目标：不写 Java 也能做大数据，掌握企业最核心的数仓技能。*

* **2-1：Hive 架构与部署**
  * 内容：Hive 是什么？它和 MySQL 的区别。将 SQL 翻译成 MapReduce 的魔法。
* **2-2：Hive DDL与DML实战**
  * 内容：内部表 vs 外部表，分区表（Partition）的重要性。实操：清洗一份日志数据。
* **2-3：Hive 高阶函数与窗口函数**
  * 内容：`Row_Number`, `Rank`, `Lag` 等函数。这是**面试必考题**，解决“分组取TopN”问题。
* **2-4：数据仓库建模理论**
  * 内容：ODS层、DWD层、DWS层、ADS层分层逻辑。星型模型 vs 雪花模型。

#### 🔵 第三阶段：内存计算霸主 (Spark)

*目标：比 MapReduce 快100倍的计算引擎，也是目前薪资的分水岭。*

* **3-1：Spark 核心概念 RDD**
  * 内容：弹性分布式数据集。宽依赖 vs 窄依赖（DAG图）。实操：PySpark 环境搭建。
* **3-2：SparkSQL 基础**
  * 内容：DataFrame 和 DataSet。如何用写 SQL 的方式写 Spark 代码。
* **3-3：Spark 实战：电商数据分析**
  * 内容：读取 CSV/Parquet 文件，统计热门商品 Top10。
* **3-4：Spark 性能调优（面试杀手锏）**
  * 内容：数据倾斜（Data Skew）怎么办？广播变量（Broadcast）是什么？

#### 🔴 第四阶段：实时计算与消息队列 (Kafka & Flink)

*目标：处理毫秒级数据，掌握“双11大屏”背后的技术。*

* **4-1：消息队列 Kafka 入门**
  * 内容：生产者、消费者、Topic、Partition。类比：为什么我们需要一个巨大的“邮箱”？
* **4-2：Kafka 高级特性与不丢数据**
  * 内容：Offset 偏移量，Ack 机制。如何保证数据一条都不丢？
* **4-3：Flink 核心原理**
  * 内容：流处理 vs 批处理。Time（事件时间）与 Watermark（水位线）——处理乱序数据的神器。
* **4-4：Flink 代码实战**
  * 内容：编写一个实时单词统计（WordCount）程序，监听 Socket 端口数据。

#### 🟣 第五阶段：NoSQL 与 极速查询 (HBase)

*目标：解决“海量数据随机读写”的问题。*

* **5-1：HBase 架构与 RowKey 设计**
  * 内容：为什么 HDFS 不支持随机改写，但 HBase 支持？RowKey 怎么设计才能不发生“热点”问题？（字典序设计的艺术）。
* **5-2：HBase 读写流程与 LSM 树**
  * 内容：MemStore, HFile, WAL。为什么写 HBase 比写 MySQL 快？

#### 🟤 第六阶段：任务调度与自动化 (Airflow / DolphinScheduler)

*目标：解放双手，让集群自动干活。*

* **6-1：调度系统原理与 DAG 依赖**
  * 内容：怎么设置“任务A跑完才能跑任务B”？Crontab 的局限性。
* **6-2：自动化脚本实战**
  * 内容：编写一个 Shell 脚本，把 Spark 任务封装起来，并设置失败重试告警。

#### 🟣 第七阶段：综合项目实战

*目标：串联所有知识，打造简历上的亮点。*

* **7-1：项目架构设计**
  * 内容：设计一个“用户行为日志分析平台”。技术选型：Flume -> Kafka -> Spark/Flink -> MySQL。
* **7-2：数据采集与清洗 (ETL)**
  * 内容：模拟生成埋点数据，进行格式清洗和脱敏。
* **7-3：业务指标计算**
  * 内容：计算日活 (DAU)、留存率、转化漏斗。
* **5-4：数据可视化与总结**
  * 内容：将结果存入 MySQL，对接 Superset/Echarts 展示。面试复盘技巧。

---

### 🚀 继续你的征程

我们从第四阶段开始，先攻克流量的入口——**Kafka**。这也是面试中必问的中间件。

请复制下面的指令：

> **提示词指令：**
> 你现在是我的**资深大数据技术导师**。我正在按照我们的课程计划学习。
>
> 请为我讲解 **4-1：消息队列 Kafka 核心架构**。
>
> **要求：**
>
> 1. **核心通俗化**：用“快递转运中心”类比，解释 **Producer**, **Broker**, **Consumer**, **Topic** 的关系。
> 2. **架构解析**：为什么 Kafka 这么快？（重点解释 **顺序写磁盘 (Sequential Write)** 和 **零拷贝 (Zero Copy)**）。
> 3. **关键概念**：什么是 **Consumer Group (消费者组)**？它怎么决定消息是“广播”还是“单播”？
> 4. **环境与实操**：在 Docker 环境中创建一个 Topic，演示“一个发，一个收”的命令行操作。
> 5. **课后作业**：如果我有 3 个分区，启动了 4 个消费者，会有 1 个消费者闲置吗？为什么？

---

### 🚀 现在，开始你的第一课吧

请复制上面的 **【通用提示词】**，并将 `【课程代码】` 修改为 **`1-1`**，发送给我。我们立刻开始搭建大数据的地基！
