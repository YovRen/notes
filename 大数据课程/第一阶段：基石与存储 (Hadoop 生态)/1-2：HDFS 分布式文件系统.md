欢迎来到第二课！在上一节课中，你已经熟悉了 Linux 的操作。今天，我们要正式进入大数据的大门，学习大数据的“地基”——**HDFS (Hadoop Distributed File System)**。

如果不理解 HDFS，你就无法理解为什么大数据能处理 PB 级（1PB = 1024TB）的数据。

---

### 1. 核心通俗化：拆解《百科全书》

**问题场景**：
假设你有一本**100万页**的超级《百科全书》（这就是一个超大文件，比如 10TB），你家里的书架（单机硬盘）根本放不下，而且就算放下了，你想找某一页也翻不动。

**HDFS 的解决方案**：

1. **切块 (Block)**：
   你拿剪刀把这本书剪开，每 **128页** 订成一个小册子（在大数据里，默认一个 Block 是 128MB）。

   * *结果*：1个超大文件变成了成千上万个小碎块。
2. **分散存储 (DataNode)**：
   你把这些小册子，分别寄存在朋友 A、朋友 B、朋友 C 的家里。

   * *角色*：你的朋友们就是 **DataNode（数据节点）**，他们负责真正干苦力，存数据。
3. **记录目录 (NameNode)**：
   你自己手里拿一个小本子，记录：“第1册在朋友A家，第2册在朋友B家……”。

   * *角色*：你手里的小本子就是 **NameNode（名称节点）**。
   * *重要性*：**如果你的小本子丢了，虽然书还在朋友家，但没人知道怎么拼回去，这本书就废了。**（NameNode 是集群的心脏）。
4. **备份机制 (Replication)**：
   朋友 A 家可能会着火（硬盘损坏）。所以，每一个小册子，你都复印了 3 份，分别放在不同的朋友家。

   * *默认策略*：HDFS 默认副本数是 **3**。坏了一台机器？没关系，其他机器上还有备份。

---

### 2. 环境与实操：HDFS 初体验

由于搭建一个完整的分布式 Hadoop 集群非常繁琐（涉及网络配置、SSH免密等），为了让你专注于“使用”而非“运维”，我们继续使用 Docker 拉取一个**已经配置好的单节点 Hadoop 环境**。

#### 步骤一：启动 Hadoop 容器

请在你的终端执行以下命令（下载可能需要一点时间，请耐心等待）：

```bash
# 拉取并启动一个集成了 Hadoop 的容器，并直接进入命令行
# 这里的 /etc/bootstrap.sh 是为了启动 Hadoop 的后台服务（NameNode/DataNode）
docker run -it sequenceiq/hadoop-docker:2.7.1 /etc/bootstrap.sh -bash
```

当出现 `bash-4.1#` 这样的提示符时，说明 Hadoop 已经启动成功了！

#### 步骤二：HDFS 架构图（文字版）

想象一下你现在的环境：

```text
       +------------------+
       |   Client (你)    |  <-- 发送命令：hdfs dfs -put ...
       +--------+---------+
                | 询问存哪？
                v
       +------------------+
       |  NameNode (管家) |  <-- 维护目录树 (Metadata)
       +--------+---------+
                | 指挥
      /         |          \
+----------+  +----------+  +----------+
| DataNode |  | DataNode |  | DataNode |
| (存储A)  |  | (存储B)  |  | (存储C)  |
+----------+  +----------+  +----------+
```

---

### 3. 代码解析：HDFS 核心命令实战

HDFS 的命令设计非常友好，它和 Linux 命令（ls, cp, rm）几乎一模一样，只是前面加了前缀 `hdfs dfs`。

#### 实战演练

**1. 准备本地数据**
先在容器的本地（Linux系统里）造一个文件。

```bash
echo "Hello Big Data" > local_file.txt
```

**2. 上传到 HDFS (Put)**
这是最常用的操作，把数据从“本地”搬运到“大数据集群”。

```bash
# 语法：hdfs dfs -put [本地源路径] [HDFS目标路径]
hdfs dfs -put local_file.txt /
```

**3. 查看 HDFS 文件列表 (Ls)**

```bash
# 语法：hdfs dfs -ls [HDFS路径]
hdfs dfs -ls /
```

* **输出解析**：你会看到类似 `-rw-r--r-- 1 root supergroup 15 ... /local_file.txt`。
* 注意：这里列出的不是你硬盘上的文件，而是**分布式文件系统**里的文件。

**4. 查看文件内容 (Cat)**

```bash
hdfs dfs -cat /local_file.txt
```

**5. 下载回本地 (Get)**
把数据从集群取回来。

```bash
# 语法：hdfs dfs -get [HDFS源路径] [本地目标路径]
hdfs dfs -get /local_file.txt downloaded_file.txt
```

---

### 4. 面试与实战视角

这里有 **3 个**在面试和工作中含金量极高的知识点：

**1. HDFS 小文件问题（面试必问！）**

* **场景**：如果你存了 1 亿个 1KB 的小文件，会发生什么？
* **后果**：NameNode 会崩溃。因为不管文件多小，NameNode 都要在内存里记一条元数据（约150字节）。1 亿个文件会把 NameNode 的内存撑爆，导致整个集群不可用。
* **解决**：在数据进入 HDFS 之前，必须进行**合并**（把小文件拼成大文件）。

**2. 这里的“删除”不是真删除**

* **实战**：当你执行 `hdfs dfs -rm /file` 时，HDFS 默认会把文件移动到 `/trash`（回收站）目录，就像 Windows 一样。你可以配置保留时间（比如 1 天），这期间能救命。

**3. 切块大小为什么是 128MB？**

* **原理**：为了最小化寻址开销。如果块太小（比如 1MB），硬盘磁头就要不停地跳来跳去寻找文件开始的位置，效率极低。大数据追求的是**一次寻址，长久读取**。

---

### 5. 课后作业

~~请在刚才启动的 Hadoop 容器中完成以下操作，并将**第 3 步的输出结果**回复给我：~~

1. ~~**创建目录**：在 HDFS 的根目录下创建一个名为 `user_data` 的文件夹。（提示：命令是 `hdfs dfs -mkdir /目录名`）。~~
2. ~~**上传**：再次创建一个包含你自己名字的本地文件 `name.txt`，把它上传到刚才创建的 `/user_data` 文件夹里。~~
3. ~~**检查**：使用命令列出 `/user_data` 文件夹里的内容。~~
4. ~~**清理**：尝试删除这个 HDFS 上的文件夹。（提示：删除非空目录需要加 `-r`，类似 Linux）。~~

~~等待你的作业！做完这个，你就完成了大数据的“存储”入门。~~
