ä½ å¥½ï¼æ¬¢è¿æ¥åˆ°ç¬¬ä¸‰é˜¶æ®µã€‚

å¦‚æœè¯´ MapReduce æ˜¯å¤§æ•°æ®çš„â€œç»¿çš®ç«è½¦â€ï¼ˆè™½ç„¶æ…¢ä½†èƒ½æ‹‰è´§ï¼‰ï¼Œé‚£ä¹ˆ **Spark** å°±æ˜¯å¤§æ•°æ®çš„**â€œæ³•æ‹‰åˆ©â€**ã€‚

åœ¨ä»Šå¤©çš„é¢è¯•ä¸­ï¼Œ**ä¸ä¼š Spark åŸºæœ¬ç­‰äºå¤±ä¸š**ã€‚è®©æˆ‘ä»¬ç›´æ¥æ‹†è§£å®ƒçš„æ ¸å¿ƒå¼•æ“â€”â€”**RDD**ã€‚

---

### 1. æ ¸å¿ƒå¯¹æ¯”ï¼šSpark ä¸ºä»€ä¹ˆæ¯” MapReduce å¿«ï¼Ÿ

é¢è¯•æ ‡å‡†ç­”æ¡ˆé€šå¸¸æ˜¯ï¼š**â€œSpark åŸºäºå†…å­˜è®¡ç®—ï¼ŒMapReduce åŸºäºç£ç›˜è®¡ç®—ã€‚â€** ä½†è¿™å¤ªä¹¦é¢äº†ï¼Œæˆ‘ä»¬çœ‹çœ‹åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆã€‚

#### ğŸ¢ MapReduce çš„å·¥ä½œæ–¹å¼ï¼ˆä¹’ä¹“çƒæ¨¡å¼ï¼‰

æƒ³è±¡ä½ è¦åšä¸€é“å¤æ‚çš„èœï¼Œåˆ†ä¸ºåˆ‡èœã€ç‚’èœã€æ‘†ç›˜ä¸‰æ­¥ã€‚

* **Step 1**ï¼šæŠŠèœåˆ‡å¥½ -> **æ”¾å›å†°ç®±ï¼ˆå†™å…¥ç£ç›˜ï¼‰**ã€‚
* **Step 2**ï¼šä»å†°ç®±æ‹¿å‡ºåˆ‡å¥½çš„èœ -> ç‚’ç†Ÿ -> **æ”¾å›å†°ç®±ï¼ˆå†™å…¥ç£ç›˜ï¼‰**ã€‚
* **Step 3**ï¼šä»å†°ç®±æ‹¿å‡ºç‚’å¥½çš„èœ -> æ‘†ç›˜ -> **æ”¾å›å†°ç®±ï¼ˆå†™å…¥ç£ç›˜ï¼‰**ã€‚
* **ç—›ç‚¹**ï¼šæ—¶é—´å…¨æµªè´¹åœ¨**â€œå¼€å†°ç®±é—¨ã€å…³å†°ç®±é—¨â€**ï¼ˆç£ç›˜ I/Oï¼‰ä¸Šäº†ã€‚

#### âš¡ Spark çš„å·¥ä½œæ–¹å¼ï¼ˆæµæ°´çº¿æ¨¡å¼ï¼‰

* **Process**ï¼šæŠŠèœåˆ‡å¥½ -> **ç›´æ¥æ‰”ç»™**æ—è¾¹çš„å¨å¸ˆç‚’ç†Ÿ -> **ç›´æ¥æ‰”ç»™**æ—è¾¹çš„æœåŠ¡å‘˜æ‘†ç›˜ -> æœ€åå†å­˜ç›˜ã€‚
* **ä¼˜åŠ¿**ï¼šä¸­é—´ç»“æœå…¨éƒ¨åœ¨**å†…å­˜ï¼ˆRAMï¼‰**é‡Œä¼ é€’ï¼Œæ ¹æœ¬ä¸è½åœ°ã€‚
* **ç»“æœ**ï¼šé€Ÿåº¦æ¯” MapReduce å¿« **10 åˆ° 100 å€**ã€‚

---

### 2. æ¦‚å¿µé€šä¿—åŒ–ï¼šä»€ä¹ˆæ˜¯ RDDï¼Ÿ

**RDD** å…¨ç§°æ˜¯ **Resilient Distributed Dataset**ï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰ã€‚å®ƒæ˜¯ Spark ä¸­æœ€åŸºæœ¬çš„æ•°æ®å•ä½ã€‚

æˆ‘ä»¬ç”¨**â€œä¸­å¤®å¨æˆ¿åšæŠ«è¨â€**æ¥ç±»æ¯” RDD çš„ä¸‰ä¸ªç‰¹æ€§ï¼š

1. **Dataset (æ•°æ®é›†)**ï¼š
   è¿™å°±æ˜¯é‚£ä¸€å›¢å·¨å¤§çš„**é¢å›¢**ï¼ˆæ•°æ®æœ¬èº«ï¼‰ã€‚
2. **Distributed (åˆ†å¸ƒå¼çš„)**ï¼š
   é¢å›¢å¤ªå¤§äº†ï¼Œä¸€ä¸ªæ¡Œå­æ”¾ä¸ä¸‹ã€‚
   æˆ‘ä»¬æŠŠé¢å›¢åˆ‡æˆ 100 ä¸ªå°é¢å›¢ï¼Œåˆ†ç»™ 100 ä¸ªå¨å¸ˆï¼ˆèŠ‚ç‚¹ï¼‰åœ¨å„è‡ªçš„æ¡Œå­ä¸Šæ‰ã€‚

   * *è¿™å°±æ˜¯ RDD çš„åˆ†åŒº (Partition) æ¦‚å¿µã€‚*
3. **Resilient (å¼¹æ€§çš„/æŠ—é€ çš„) â€”â€” æœ€éš¾ç†è§£ï¼Œä¹Ÿæœ€æ ¸å¿ƒ**
   å¦‚æœç¬¬ 50 å·å¨å¸ˆçªç„¶æ™•å€’äº†ï¼ˆæœºå™¨æŒ‚äº†ï¼‰ï¼Œä»–æ‰‹é‡Œçš„é¢å›¢ä¸¢äº†ï¼Œæ€ä¹ˆåŠï¼Ÿ

   * **MapReduce**ï¼šæ²¡åŠæ³•ï¼Œå¿…é¡»è¦æŠŠæ•´ä¸ªä»»åŠ¡é‡åšä¸€éã€‚
   * **Spark (RDD)**ï¼šSpark æœ‰ä¸€ä¸ª**â€œè®°è´¦æœ¬â€ (DAG/Lineage)**ã€‚å®ƒä¸è®°æ•°æ®æœ¬èº«ï¼Œå®ƒè®°**â€œæ“ä½œæ­¥éª¤â€**ã€‚
   * Spark ä¼šè¯´ï¼šâ€œä¸ç”¨æ€•ï¼æˆ‘çœ‹äº†ä¸€ä¸‹è´¦æœ¬ï¼Œç¬¬ 50 å·é¢å›¢æ˜¯ç”±åŸæ–™ A åŠ æ°´ B æ‰æˆçš„ã€‚æ¢ä¸ªæ–°å¨å¸ˆï¼Œç…§ç€è¿™ä¸ªæ­¥éª¤**åªé‡åšè¿™ä¸€å—é¢å›¢**å°±è¡Œäº†ï¼â€
   * *è¿™å°±æ˜¯ RDD çš„å®¹é”™æ€§ã€‚*

---

### 3. æ ¸å¿ƒåŸç†ï¼šæ‡’æ±‰ Spark (Transformation vs Action)

Spark éå¸¸â€œæ‡’â€ï¼Œç”šè‡³å¯ä»¥è¯´æœ‰äº›â€œé¸¡è´¼â€ã€‚å®ƒçš„ç®—å­ï¼ˆOperatorï¼‰åˆ†ä¸ºä¸¤ç±»ï¼š

#### ğŸ› ï¸ Transformation (è½¬æ¢) â€”â€” åªåŠ¨å£ï¼Œä¸åŠ¨æ‰‹

* **ä¾‹å­**ï¼š`map` (å˜æ ·å­), `filter` (è¿‡æ»¤), `flatMap` (æ‹†å¼€)ã€‚
* **è¡Œä¸º**ï¼šä½ å‘Šè¯‰ Sparkï¼šâ€œæŠŠè¿™äº›æ•°å­—ä¹˜ä»¥ 2ï¼Œå†æŠŠå¤§äº 10 çš„æŒ‘å‡ºæ¥ã€‚â€
* **Spark çš„ååº”**ï¼šå®ƒæ ¹æœ¬ä¸ä¼šå»ç®—ï¼å®ƒåªæ˜¯åœ¨å°æœ¬æœ¬ä¸Šç”»äº†ä¸ªå›¾ï¼Œè®°å½•ä¸‹ï¼šâ€œä¸€ä¼šå¦‚æœè€æ¿è®©æˆ‘å¹²æ´»ï¼Œæˆ‘è¦å…ˆä¹˜ 2 å†è¿‡æ»¤ã€‚â€ï¼ˆè¿™å«**æ„å»º DAG æœ‰å‘æ— ç¯å›¾**ï¼‰ã€‚

#### ğŸ’¥ Action (è¡ŒåŠ¨) â€”â€” è€æ¿å‘è¯äº†

* **ä¾‹å­**ï¼š`collect` (æ”¶é›†ç»“æœ), `count` (æ•°æ•°), `saveAsTextFile` (ä¿å­˜)ã€‚
* **è¡Œä¸º**ï¼šä½ å‘Šè¯‰ Sparkï¼šâ€œæŠŠç»“æœæ‰“å°ç»™æˆ‘çœ‹ï¼â€
* **Spark çš„ååº”**ï¼šå®ƒä¸€çœ‹èº²ä¸è¿‡å»äº†ï¼Œæ‰ä¼šæ ¹æ®åˆšæ‰ç”»çš„å›¾ï¼ŒçœŸæ­£å¯åŠ¨ CPU å¼€å§‹è®¡ç®—ã€‚

**ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼Ÿ**
**ä¸ºäº†ä¼˜åŒ–ï¼**
å¦‚æœä½ å†™äº† `filter(è¿‡æ»¤å…¨æ ¡å­¦ç”Ÿ).take(10)`ï¼ˆåªè¦å‰ 10 ä¸ªäººï¼‰ã€‚

* å¦‚æœä¸æ‡’ï¼šSpark ä¼šå‚»å‚»åœ°æŠŠå…¨æ ¡ 10 ä¸‡äººå…ˆè¿‡æ»¤ä¸€éï¼Œå†å–å‰ 10 ä¸ªã€‚
* å› ä¸ºæ‡’ï¼šSpark çœ‹åˆ°æœ€ååªéœ€è¦ 10 ä¸ªï¼Œå®ƒä¼šåœ¨è¿‡æ»¤åˆ°ç¬¬ 10 ä¸ªäººæ—¶**ç«‹åˆ»åœæ­¢**ã€‚è¿™å°±å«**æƒ°æ€§è®¡ç®— (Lazy Evaluation)**ã€‚

---

### 4. ç¯å¢ƒä¸å®æ“ï¼šPySpark åˆä½“éªŒ

Spark å¯ä»¥ç”¨ Java, Scala, Python å†™ã€‚Python å†™èµ·æ¥æœ€å¿«ï¼Œå« **PySpark**ã€‚

è¯·åœ¨ä½ çš„ Docker å®¹å™¨ä¸­è¾“å…¥ `pyspark` è¿›å…¥äº¤äº’å¼å‘½ä»¤è¡Œï¼Œæˆ–è€…ç›´æ¥çœ‹ä¸‹é¢çš„ä»£ç é€»è¾‘ï¼š

```python
from pyspark import SparkContext

# 1. åˆå§‹åŒ–å…¥å£ (å¦‚æœæ˜¯äº¤äº’å¼å‘½ä»¤è¡Œï¼Œsc å˜é‡å·²ç»è‡ªåŠ¨åˆ›å»ºå¥½äº†)
# sc = SparkContext("local", "MyFirstApp")

# 2. åˆ›å»º RDD (Parallelize)
# æŠŠä¸€ä¸ªæ™®é€šçš„ Python åˆ—è¡¨ï¼Œå˜æˆåˆ†å¸ƒå¼çš„ RDD "é¢å›¢"
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# 3. Transformation (è½¬æ¢)
# map: è®©æ¯ä¸ªå…ƒç´ éƒ½ä¹˜ä»¥ 10
# æ³¨æ„ï¼šè¿è¡Œè¿™è¡Œä»£ç æ—¶ï¼Œå±å¹•ä¸Šä»€ä¹ˆéƒ½ä¸ä¼šå‘ç”Ÿï¼å› ä¸º Spark åœ¨å·æ‡’ã€‚
rdd_map = rdd.map(lambda x: x * 10)

# 4. Transformation (å†è½¬æ¢)
# filter: åªè¦å¤§äº 20 çš„
rdd_filter = rdd_map.filter(lambda x: x > 20)

# 5. Action (è¡ŒåŠ¨)
# collect: è¿™æ˜¯ä¸€ä¸ªè¡ŒåŠ¨ç®—å­ï¼Œå®ƒä¼šè§¦å‘çœŸæ­£çš„è®¡ç®—ï¼ŒæŠŠåˆ†å¸ƒå¼çš„æ•°æ®æ‹‰å›åˆ°ä½ é¢å‰
result = rdd_filter.collect()

print(result)
# è¾“å‡º: [30, 40, 50]
```

---

### 5. è¯¾åä½œä¸šï¼šç«çœ¼é‡‘ç›

è¯·çœ‹ä¸‹é¢è¿™æ®µä»£ç ï¼Œå›ç­”æˆ‘çš„é—®é¢˜ï¼š

```python
# å‡è®¾ sc å·²ç»åˆå§‹åŒ–
# ç¬¬ä¸€æ­¥
rdd1 = sc.parallelize(["hello spark", "hello world", "big data"])

# ç¬¬äºŒæ­¥
rdd2 = rdd1.flatMap(lambda line: line.split(" "))

# ç¬¬ä¸‰æ­¥
rdd3 = rdd2.map(lambda word: (word, 1))

# ç¬¬å››æ­¥
print("ä»»åŠ¡é€»è¾‘æ„å»ºå®Œæˆ")

# ç¬¬äº”æ­¥
count_result = rdd3.count()
print(count_result)
```

**é—®é¢˜ï¼š**

1. ä»£ç è¿è¡Œåˆ°**â€œç¬¬å››æ­¥â€**çš„æ—¶å€™ï¼Œé›†ç¾¤é‡Œçš„ CPU å¼€å§‹å¤„ç†å­—ç¬¦ä¸²äº†å—ï¼Ÿï¼ˆæ˜¯ / å¦ï¼‰
2. å“ªä¸€æ­¥çœŸæ­£è§¦å‘äº†æ‰€æœ‰è®¡ç®—ï¼Ÿ

è¯·å›å¤ä½ çš„ç­”æ¡ˆã€‚
