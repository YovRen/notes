你好！欢迎来到 **Spark 阶段的“封神之战”** —— **3-4：性能调优**。

能不能写出代码，决定了你能不能通过**笔试**；
能不能讲清楚调优，决定了你能不能通过**面试**（拿到高薪）。

面试官问：“你的 Spark 任务跑得慢，你是怎么优化的？” 如果你只能回答“加内存”，那基本就没戏了。今天我教你三个真正的杀手锏。

---

### 1. 核心痛点：数据倾斜 (Data Skew)

这是大数据领域**最臭名昭著**的问题，90% 的 OOM（内存溢出）和任务卡死都是因为它。

#### ⚖️ 生活类比：工作分配不均

假设你是包工头（Driver），你雇了 10 个工人（Executor）去搬 1000 块砖。

* **正常情况**：每人搬 100 块。大家同时干完，耗时 10 分钟。
* **数据倾斜**：
  * Worker 1 分配了 **991 块**砖。
  * Worker 2~10 每人只分配了 **1 块**砖。
  * **结果**：9 个工人 10 秒钟就干完了，在旁边抽烟看戏。Worker 1 累吐血了还没搬完。
  * **任务总耗时**：取决于最慢的那个人（短板效应）。任务会卡在 99% 的进度上几个小时不动，最后 Worker 1 内存爆了，任务失败。

**技术原理**：
在使用 `groupBy` 或 `join` 时，Spark 会根据 Key 把数据 Shuffle 到不同的节点。如果某一个 Key（比如 Key="Beijing"）的数据量远超其他 Key，接收这个 Key 的节点就会被撑死。

---

### 2. 解决方案：Key 加盐 (Salting)

怎么解决“北京”数据太多的问题？**把“北京”拆开！**

**核心思想**：
给倾斜的 Key 加上随机后缀（加盐），把它分散到不同的节点上处理，算完后再聚合。

**操作步骤**：

1. **加盐 (Salting)**：

   * 原始数据：`("Beijing", 1), ("Beijing", 1)...` 都在一堆。
   * 操作：随机生成 0-9 的数字作为后缀。
   * 结果：`("Beijing_0", 1)`, `("Beijing_1", 1)`, `("Beijing_9", 1)`...
   * **效果**：原本 1 亿条 "Beijing" 被打散成了 10 份，均匀分发给了 10 个节点。
2. **局部聚合 (Local Aggregation)**：

   * 各节点分别算：`("Beijing_0", 1000)`, `("Beijing_1", 1200)`...
3. **去盐 + 全局聚合 (Global Aggregation)**：

   * 把后缀去掉：全变成了 `("Beijing", 1000)`, `("Beijing", 1200)`。
   * 再做一次 sum：得到最终结果。

---

### 3. 优化技巧：广播变量 (Broadcast Variable)

这是消除 Shuffle、提升 Join 性能的神器。

#### 📖 生活类比：查字典

**场景**：老师（Driver）让全班 100 个学生（Executor）把作文里的生僻字注上音。字典（小表）只有一本，在老师手里。

* **普通 Join (Shuffle Join)**：

  * 学生每遇到一个生词，都要跑到讲台前查字典。
  * **后果**：讲台挤爆了，全是人（网络拥堵），效率极低。
* **广播 Join (Broadcast Join)**：

  * 老师把字典**复印 100 份**，发给每个学生人手一份。
  * **后果**：学生坐在座位上就能查（本地内存查找），一步都不用走。
  * **效果**：彻底消除了网络传输（Shuffle）。

**技术定义**：
当一张表很小（默认 10MB 以下，企业里可以调大到 100MB 甚至 1GB）时，Spark 会自动（或者你手动指定）把这张**小表完整地复制**到每一个计算节点的**内存**中。这样大表在 Join 时，完全不需要通过网络传输数据。

---

### 4. 实战判断：大小表 Join 策略

**场景**：

* **表A (Orders)**：10 亿行（大表）。
* **表B (Cities)**：100 行（小表，只有城市ID和城市名）。

**绝对禁止**：普通 Join (Shuffle Join)。

* 这会导致 10 亿行数据在网络间飞来飞去，集群直接瘫痪。

**必须使用**：**Broadcast Join (广播连接/Map端连接)**。

* **代码示例**：

  ```python
  from pyspark.sql.functions import broadcast

  # 显式告诉 Spark：把 cities_df 给我广播出去！
  result = orders_df.join(broadcast(cities_df), "city_id")
  ```

* **结果**：Cities 表会被复制到所有节点，10 亿行订单数据在本地就能匹配到城市名，速度快如闪电。

---

### 5. 课后作业：策略选择题

请阅读以下场景，选择最合适的优化策略（单选）：

**场景**：
你需要统计**全中国所有用户**的男女比例。

* **表A (Users)**：包含 14 亿条用户记录，字段有 `user_id`, `gender_code` (0或1)。
* **表B (Gender_Dict)**：性别字典表，只有 2 行数据：`{0: "女", 1: "男"}`。

**问题**：你应该使用哪种方式把 `gender_code` 翻译成中文的 "男/女"？

A. 使用普通的 `join`，让 Spark 自己决定。
B. 使用 Key 加盐 (Salting) 策略。
C. 使用广播变量 (Broadcast Join) 将表 B 广播。
D. 增加更多的机器节点。

请回复你的选项及理由。
