{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 12. 简单神经网络 (Simple Neural Network)\n",
    "\n",
    "### 网络结构\n",
    "- 输入层 -> 隐藏层（Sigmoid激活）-> 输出层（Sigmoid激活）\n",
    "- 适用于二分类问题\n",
    "\n",
    "### 前向传播\n",
    "1. 隐藏层: Z1 = W1*X + b1, A1 = sigmoid(Z1)\n",
    "2. 输出层: Z2 = W2*A1 + b2, A2 = sigmoid(Z2)\n",
    "\n",
    "### 反向传播（链式法则）\n",
    "1. 输出层梯度: dZ2 = A2 - y\n",
    "2. 隐藏层梯度: dZ1 = (W2^T * dZ2) * sigmoid'(Z1)\n",
    "3. 权重更新: W -= learning_rate * dW\n",
    "\n",
    "### 应用场景\n",
    "- XOR问题（需隐藏层）\n",
    "- 二分类问题\n",
    "- 神经网络基础学习\n",
    "\n",
    "### 复杂度\n",
    "- 训练: O(epochs * samples * (features + hidden_size))\n",
    "- 预测: O(features * hidden_size)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"初始化网络参数\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 权重初始化 (Xavier初始化)\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.1\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid激活函数\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # 防止溢出\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"Sigmoid导数: σ'(z) = σ(z) * (1 - σ(z))\"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 隐藏层\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = self.sigmoid(self.Z1)\n",
    "        \n",
    "        # 输出层\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        \n",
    "        return self.A2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        num_samples = X.shape[1]\n",
    "        \n",
    "        # 输出层梯度\n",
    "        dZ2 = self.A2 - y  # 二元交叉熵 + Sigmoid的导数简化形式\n",
    "        self.dW2 = (1 / num_samples) * np.dot(dZ2, self.A1.T)\n",
    "        self.db2 = (1 / num_samples) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        # 隐藏层梯度\n",
    "        dA1 = np.dot(self.W2.T, dZ2)\n",
    "        dZ1 = dA1 * self.sigmoid_derivative(self.Z1)\n",
    "        self.dW1 = (1 / num_samples) * np.dot(dZ1, X.T)\n",
    "        self.db1 = (1 / num_samples) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"使用梯度下降更新参数\"\"\"\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate, print_cost=True):\n",
    "        \"\"\"完整的训练循环\"\"\"\n",
    "        for i in range(epochs):\n",
    "            # 前向传播\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # 计算损失 (二元交叉熵)\n",
    "            cost = -np.mean(y * np.log(predictions + 1e-8) + (1 - y) * np.log(1 - predictions + 1e-8))\n",
    "            \n",
    "            # 反向传播\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            # 更新参数\n",
    "            self.update_parameters(learning_rate)\n",
    "            \n",
    "            if print_cost and i % 1000 == 0:\n",
    "                print(f\"Cost after epoch {i}: {cost:.6f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测\"\"\"\n",
    "        predictions = self.forward(X)\n",
    "        return (predictions > 0.5).astype(int)\n",
    "\n",
    "# 测试：解决XOR问题\n",
    "if __name__ == \"__main__\":\n",
    "    # XOR数据集\n",
    "    X_train = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "    y_train = np.array([[0, 1, 1, 0]])\n",
    "    \n",
    "    # 创建并训练模型\n",
    "    nn = SimpleNN(input_size=2, hidden_size=4, output_size=1)\n",
    "    nn.train(X_train, y_train, epochs=10000, learning_rate=0.5)\n",
    "    \n",
    "    # 测试\n",
    "    predictions = nn.predict(X_train)\n",
    "    accuracy = np.mean(predictions == y_train) * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71943afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 11. Transformer 注意力机制详解\n",
    "\n",
    "### 四种注意力机制对比\n",
    "\n",
    "#### 1. 多头注意力 (MHA - Multi-Head Attention)\n",
    "- **特点**: 完整的自注意力，每个Query对应一个完整的K-V序列\n",
    "- **参数**: num_heads = 8（标准）\n",
    "- **计算**: Q = [B,L,D], K = [B,L,D], V = [B,L,D] -> Output = [B,L,D]\n",
    "- **复杂度**: O(n²) 其中n是序列长度\n",
    "\n",
    "#### 2. 多查询注意力 (MQA - Multi-Query Attention)\n",
    "- **特点**: 所有Query共享一套K-V\n",
    "- **参数**: 只有一组K和V（不分头）\n",
    "- **优势**: 内存占用少，KV缓存小，适合生成任务\n",
    "- **应用**: LLaMA 2, Falcon等\n",
    "\n",
    "#### 3. 分组查询注意力 (GQA - Grouped-Query Attention)\n",
    "- **特点**: Query分组，每组共享一套K-V\n",
    "- **参数**: num_kv_heads < num_q_heads（如n_heads=8, kv_heads=2）\n",
    "- **优势**: 平衡MHA和MQA，介于两者之间\n",
    "- **应用**: Gemini, Llama 3.1等\n",
    "\n",
    "#### 4. 线性注意力 (Linear Attention)\n",
    "- **特点**: 将Softmax改为其他激活函数（如ELU），实现O(n)复杂度\n",
    "- **原理**: Q*V通过数学变形避免显式计算注意力矩阵\n",
    "- **应用**: 长序列处理\n",
    "\n",
    "#### 5. Flash Attention\n",
    "- **特点**: 优化的CUDA内存访问模式\n",
    "- **优势**: 不改变算法，只改进内存利用，速度快3-4倍\n",
    "- **核心**: 分块处理和在线Softmax\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    \"\"\"多头注意力 (Multi-Head Attention)\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MHA, self).__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_o = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        \n",
    "        # 投影后分头 -> [B, H, L, D_h]\n",
    "        query = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = self.w_k(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = self.w_v(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力 -> [B, H, L, L]\n",
    "        attention = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if attn_mask is not None:\n",
    "            attention += attn_mask.unsqueeze(1)\n",
    "        \n",
    "        # Softmax和加权求和\n",
    "        attn_weights = self.softmax(attention)\n",
    "        output = torch.matmul(attn_weights, value)  # [B, H, L, D_h]\n",
    "        \n",
    "        # 合并头 -> [B, L, D]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.w_o(output)\n",
    "        return output\n",
    "\n",
    "class MQA(nn.Module):\n",
    "    \"\"\"多查询注意力 (Multi-Query Attention)\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MQA, self).__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(embed_dim, embed_dim)  # Query仍然分头\n",
    "        self.w_k = nn.Linear(embed_dim, self.head_dim)  # Key只有一份\n",
    "        self.w_v = nn.Linear(embed_dim, self.head_dim)  # Value只有一份\n",
    "        self.w_o = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        \n",
    "        # Q分头，K-V不分头\n",
    "        query = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = self.w_k(key).unsqueeze(1)  # [B, 1, L, D_h]\n",
    "        value = self.w_v(value).unsqueeze(1)  # [B, 1, L, D_h]\n",
    "        \n",
    "        # 注意力计算 -> [B, H, L, L]\n",
    "        attention = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if attn_mask is not None:\n",
    "            attention += attn_mask.unsqueeze(1)\n",
    "        \n",
    "        attn_weights = self.softmax(attention)\n",
    "        output = torch.matmul(attn_weights, value)  # [B, H, L, D_h]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.w_o(output)\n",
    "        return output\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    \"\"\"分组查询注意力 (Grouped-Query Attention)\"\"\"\n",
    "    def __init__(self, embed_dim, num_q_heads, num_kv_heads):\n",
    "        super(GQA, self).__init__()\n",
    "        assert embed_dim % num_q_heads == 0\n",
    "        assert num_q_heads % num_kv_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.num_groups = num_q_heads // num_kv_heads  # 每个KV头服务的Q头数\n",
    "        self.head_dim = embed_dim // num_q_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_k = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
    "        self.w_v = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
    "        self.w_o = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        \n",
    "        query = self.w_q(query).view(batch_size, seq_len, self.num_q_heads, self.head_dim).transpose(1, 2)\n",
    "        key = self.w_k(key).view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        value = self.w_v(value).view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 复制KV使其与Q保持相同的头数\n",
    "        key = key.unsqueeze(2).repeat(1, 1, self.num_groups, 1, 1).view(batch_size, self.num_q_heads, seq_len, self.head_dim)\n",
    "        value = value.unsqueeze(2).repeat(1, 1, self.num_groups, 1, 1).view(batch_size, self.num_q_heads, seq_len, self.head_dim)\n",
    "        \n",
    "        attention = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if attn_mask is not None:\n",
    "            attention += attn_mask.unsqueeze(1)\n",
    "        \n",
    "        attn_weights = self.softmax(attention)\n",
    "        output = torch.matmul(attn_weights, value).transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.w_o(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 13. Transformer 位置编码 (Positional Encoding)\n",
    "\n",
    "### 为什么需要位置编码？\n",
    "- Transformer没有循环或卷积，无法捕捉序列顺序信息\n",
    "- 需要显式地编码位置信息\n",
    "\n",
    "### 两种位置编码方式对比\n",
    "\n",
    "#### 1. 绝对位置编码 (APE - Additive Positional Encoding)\n",
    "- **方式**: PE = PE + word_embedding\n",
    "- **公式**:\n",
    "  - PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "  - PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "- **特点**: \n",
    "  - 静态的位置编码\n",
    "  - 加法融合\n",
    "  - 每个位置都有一个固定的向量\n",
    "- **应用**: 原始Transformer, BERT\n",
    "\n",
    "#### 2. 旋转位置编码 (RoPE - Rotary Position Embedding)\n",
    "- **方式**: 通过旋转矩阵作用于Q和K\n",
    "- **优势**:\n",
    "  - 相对位置信息编码\n",
    "  - 距离衰减自然出现\n",
    "  - 长序列外推能力强\n",
    "- **应用**: GPT-3.5, LLaMA等最新模型\n",
    "\n",
    "### 公式解释\n",
    "- θ_i = 10000^(-2i/d_model): 不同维度的基频率\n",
    "- m * θ_i: 位置m与基频率的乘积\n",
    "- 使用sin/cos对进行编码：易于捕捉周期性模式\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TransformerPositionalEncodings:\n",
    "    \"\"\"位置编码类：对比APE和RoPE\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_position=1024):\n",
    "        \"\"\"\n",
    "        初始化位置编码\n",
    "        \n",
    "        Args:\n",
    "            d_model: 模型维度（必须是偶数）\n",
    "            max_position: 最大序列长度\n",
    "        \"\"\"\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(\"d_model必须是偶数\")\n",
    "        self.d_model = d_model\n",
    "        self.max_position = max_position\n",
    "        \n",
    "        # 预计算频率 (APE和RoPE的共同基础)\n",
    "        self.frequencies = self._precompute_frequencies()\n",
    "        self.freqs_cos = np.cos(self.frequencies)\n",
    "        self.freqs_sin = np.sin(self.frequencies)\n",
    "\n",
    "    def _precompute_frequencies(self):\n",
    "        \"\"\"\n",
    "        预计算核心频率矩阵: m * θ_i\n",
    "        \n",
    "        Returns:\n",
    "            np.array: shape=[max_position, d_model/2]\n",
    "        \"\"\"\n",
    "        # 计算 θ_i = 1 / (10000^(2i/d_model))\n",
    "        theta = 1.0 / (10000 ** (np.arange(0, self.d_model, 2, dtype=np.float32) / self.d_model))\n",
    "        \n",
    "        # 位置索引 m = [0, 1, ..., max_position-1]\n",
    "        position = np.arange(self.max_position, dtype=np.float32)\n",
    "        \n",
    "        # 外积: m * θ_i -> [max_position, d_model/2]\n",
    "        freqs_matrix = np.outer(position, theta)\n",
    "        return freqs_matrix\n",
    "\n",
    "    def get_absolute_encoding(self):\n",
    "        \"\"\"\n",
    "        ### APE (加性位置编码)\n",
    "        \n",
    "        生成静态位置编码矩阵\n",
    "        - 偶数维度: sin(m*θ_i)\n",
    "        - 奇数维度: cos(m*θ_i)\n",
    "        \n",
    "        Returns:\n",
    "            np.array: shape=[1, max_position, d_model]\n",
    "        \"\"\"\n",
    "        pe = np.zeros((self.max_position, self.d_model))\n",
    "        \n",
    "        # 偶数维度应用sin\n",
    "        pe[:, 0::2] = self.freqs_sin\n",
    "        \n",
    "        # 奇数维度应用cos\n",
    "        pe[:, 1::2] = self.freqs_cos\n",
    "        \n",
    "        return pe[np.newaxis, :, :]\n",
    "\n",
    "    def apply_rotary_encoding(self, x):\n",
    "        \"\"\"\n",
    "        ### RoPE (旋转位置编码)\n",
    "        \n",
    "        将旋转矩阵应用于Q和K张量\n",
    "        \n",
    "        原理:\n",
    "        1. 将向量 x = [x_0, x_1, x_2, x_3, ...] 分组为 (x_0,x_1), (x_2,x_3), ...\n",
    "        2. 对每对应用2D旋转矩阵:\n",
    "           [cos θ  -sin θ] [x_0]\n",
    "           [sin θ   cos θ] [x_1]\n",
    "        3. 等价于: x * cos θ + (-x_1, x_0) * sin θ\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            x with rotary encoding applied\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # 重复cos/sin使其覆盖所有维度 (2i, 2i+1共用)\n",
    "        cos_emb = np.repeat(self.freqs_cos, 2, axis=-1)\n",
    "        sin_emb = np.repeat(self.freqs_sin, 2, axis=-1)\n",
    "        \n",
    "        # 截取序列长度内的编码\n",
    "        cos = cos_emb[np.newaxis, :seq_len, :]\n",
    "        sin = sin_emb[np.newaxis, :seq_len, :]\n",
    "        \n",
    "        # 旋转变换: 构造 (-x_1, x_0, -x_3, x_2, ...)\n",
    "        x_reshaped = x.reshape(x.shape[:-1] + (-1, 2))\n",
    "        x_rotated = np.stack([-x_reshaped[..., 1], x_reshaped[..., 0]], axis=-1)\n",
    "        x_rotated = x_rotated.reshape(x.shape)\n",
    "        \n",
    "        # 应用旋转公式: x*cos + rotated*sin\n",
    "        return x * cos + x_rotated * sin\n",
    "\n",
    "# 对比演示\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 128\n",
    "    batch_size = 2\n",
    "    seq_len = 50\n",
    "    \n",
    "    pos_encoder = TransformerPositionalEncodings(d_model, max_position=1024)\n",
    "    \n",
    "    # APE演示\n",
    "    print(\"=== 绝对位置编码 (APE) ===\")\n",
    "    ape_matrix = pos_encoder.get_absolute_encoding()\n",
    "    word_embeddings = np.random.rand(batch_size, seq_len, d_model)\n",
    "    final_input_ape = word_embeddings + ape_matrix[:, :seq_len, :]\n",
    "    print(f\"APE矩阵形状: {ape_matrix.shape}\")\n",
    "    print(f\"融合后形状: {final_input_ape.shape}\")\n",
    "    \n",
    "    # RoPE演示\n",
    "    print(\"\\\\n=== 旋转位置编码 (RoPE) ===\")\n",
    "    query = np.random.rand(batch_size, seq_len, d_model)\n",
    "    key = np.random.rand(batch_size, seq_len, d_model)\n",
    "    query_with_rope = pos_encoder.apply_rotary_encoding(query)\n",
    "    key_with_rope = pos_encoder.apply_rotary_encoding(key)\n",
    "    print(f\"RoPE后Query形状: {query_with_rope.shape}\")\n",
    "    print(f\"RoPE后Key形状: {key_with_rope.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 9. 字节对编码 (Byte Pair Encoding, BPE)\n",
    "\n",
    "### 概述\n",
    "BPE是一种数据压缩技术，经常用于NLP的分词。它通过迭代合并最频繁的连续字节对来构建词汇表。\n",
    "\n",
    "### 算法步骤\n",
    "1. **初始化**: 词汇表包含所有单个字符\n",
    "2. **统计**: 计算所有相邻字符对的频率\n",
    "3. **合并**: 选择频率最高的字符对进行合并\n",
    "4. **重复**: 重复步骤2-3直到达到目标词汇表大小\n",
    "\n",
    "### 关键概念\n",
    "- **字符对**: 文本中相邻的两个符号\n",
    "- **合并规则**: 记录所有的合并操作，用于编码新文本\n",
    "- **贪心策略**: 总是选择当前频率最高的字符对\n",
    "\n",
    "### 应用\n",
    "- GPT-2, GPT-3 使用BPE分词\n",
    "- 处理OOV（词表外）问题\n",
    "- 平衡词汇量和处理效率\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "def get_vocabulary(corpus):\n",
    "    \"\"\"\n",
    "    从语料库生成初始词汇表\n",
    "    \n",
    "    Args:\n",
    "        corpus: 可迭代的字符串\n",
    "        \n",
    "    Returns:\n",
    "        dict: {token_sequence: frequency}\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    for line in corpus:\n",
    "        for word in line.strip().split():\n",
    "            # 将单词拆分为字符，并加上结束符'</w>'\n",
    "            tokens = list(word) + ['</w>']\n",
    "            vocab[' '.join(tokens)] += 1\n",
    "    return dict(vocab)\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"统计词汇表中所有相邻符号对的频率\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for token_seq, freq in vocab.items():\n",
    "        symbols = token_seq.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(pair, v_in):\n",
    "    \"\"\"\n",
    "    在词汇表中合并指定的字符对\n",
    "    \n",
    "    Args:\n",
    "        pair: (symbol1, symbol2) 要合并的对\n",
    "        v_in: 输入词汇表\n",
    "        \n",
    "    Returns:\n",
    "        dict: 合并后的词汇表\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?<!\\S)')\n",
    "    for token_seq, freq in v_in.items():\n",
    "        new_token = pattern.sub(''.join(pair), token_seq)\n",
    "        v_out[new_token] = freq\n",
    "    return v_out\n",
    "\n",
    "def learn_bpe(corpus, num_merges):\n",
    "    \"\"\"\n",
    "    学习BPE合并规则\n",
    "    \n",
    "    Args:\n",
    "        corpus: 文本语料库\n",
    "        num_merges: 合并操作次数\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (最终词汇表, 合并规则列表)\n",
    "    \"\"\"\n",
    "    vocab = get_vocabulary(corpus)\n",
    "    merges = []\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # 选择频率最高的字符对\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        merges.append(best)\n",
    "        vocab = merge_pair(best, vocab)\n",
    "        print(f\"Merge {i+1}: {best[0]} + {best[1]} -> {''.join(best)}\")\n",
    "    \n",
    "    return vocab, merges\n",
    "\n",
    "def encode_word(word, merges):\n",
    "    \"\"\"\n",
    "    使用学到的合并规则对单词进行编码\n",
    "    \n",
    "    Args:\n",
    "        word: 要编码的单词\n",
    "        merges: 合并规则列表\n",
    "        \n",
    "    Returns:\n",
    "        list: 编码后的token列表\n",
    "    \"\"\"\n",
    "    symbols = list(word) + ['</w>']\n",
    "    token_seq = ' '.join(symbols)\n",
    "    \n",
    "    for a, b in merges:\n",
    "        pair = a + ' ' + b\n",
    "        token_seq = token_seq.replace(pair, a + b)\n",
    "    \n",
    "    return token_seq.split()\n",
    "\n",
    "# 测试示例\n",
    "if __name__ == '__main__':\n",
    "    corpus = ['low lower lowest', 'newer wider']\n",
    "    vocab, merges = learn_bpe(corpus, num_merges=10)\n",
    "    \n",
    "    # 编码测试\n",
    "    for w in ['low', 'lower', 'newest']:\n",
    "        enc = encode_word(w, merges)\n",
    "        print(f\"{w} -> {enc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743429aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 10. WordPiece 分词算法\n",
    "\n",
    "### 概述\n",
    "WordPiece 是 Google 开发的分词技术，用于 BERT 等预训练模型。\n",
    "与 BPE 不同，WordPiece 使用**概率分数**而非频率选择合并对。\n",
    "\n",
    "### 关键区别（vs BPE）\n",
    "- **合并评分**: \n",
    "  - BPE: 按频率 count(pair) 排序\n",
    "  - WordPiece: 按 count(pair) / (count(token1) * count(token2)) 排序\n",
    "  \n",
    "- **编码方式**:\n",
    "  - BPE: 顺序应用所有合并规则\n",
    "  - WordPiece: 贪心最长匹配（从左到右，选择最长的有效token）\n",
    "\n",
    "### 算法步骤\n",
    "1. 初始化词汇表为所有字符\n",
    "2. 计算所有token对的概率分数\n",
    "3. 选择分数最高的对进行合并\n",
    "4. 重复直到达到目标词汇表大小\n",
    "\n",
    "### 应用\n",
    "- BERT, ALBERT等模型\n",
    "- 处理未知词的标准方法\n",
    "- 非起始位置token加##前缀（如##ing）\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "\n",
    "def get_stats(word_freqs):\n",
    "    \"\"\"统计单个token和相邻token对的频率\"\"\"\n",
    "    token_counts = collections.defaultdict(int)\n",
    "    pair_counts = collections.defaultdict(int)\n",
    "    \n",
    "    for word_tokens, freq in word_freqs.items():\n",
    "        for token in word_tokens:\n",
    "            token_counts[token] += freq\n",
    "        for i in range(len(word_tokens) - 1):\n",
    "            pair_counts[(word_tokens[i], word_tokens[i+1])] += freq\n",
    "    \n",
    "    return token_counts, pair_counts\n",
    "\n",
    "def merge_vocab(best_pair, word_freqs):\n",
    "    \"\"\"在词汇表中执行合并操作\"\"\"\n",
    "    new_word_freqs = collections.defaultdict(int)\n",
    "    new_token = \"\".join(best_pair)\n",
    "    \n",
    "    for word_tokens, freq in word_freqs.items():\n",
    "        new_word_tokens = []\n",
    "        i = 0\n",
    "        while i < len(word_tokens):\n",
    "            try:\n",
    "                if word_tokens[i] == best_pair[0] and word_tokens[i+1] == best_pair[1]:\n",
    "                    new_word_tokens.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word_tokens.append(word_tokens[i])\n",
    "                    i += 1\n",
    "            except IndexError:\n",
    "                new_word_tokens.append(word_tokens[i])\n",
    "                i += 1\n",
    "        \n",
    "        new_word_freqs[tuple(new_word_tokens)] = freq\n",
    "    \n",
    "    return new_word_freqs\n",
    "\n",
    "def train_wordpiece(corpus, vocab_size):\n",
    "    \"\"\"\n",
    "    训练 WordPiece 模型\n",
    "    \n",
    "    Args:\n",
    "        corpus: {word: frequency} 格式的语料库\n",
    "        vocab_size: 目标词汇表大小\n",
    "        \n",
    "    Returns:\n",
    "        set: 最终词汇表\n",
    "    \"\"\"\n",
    "    # 初始化词汇表\n",
    "    vocab = {'[UNK]'}\n",
    "    for word in corpus.keys():\n",
    "        vocab.update(list(word))\n",
    "    \n",
    "    # 预分词\n",
    "    word_freqs = collections.defaultdict(int)\n",
    "    for word, freq in corpus.items():\n",
    "        word_tokens = tuple(word) + ('</w>',)\n",
    "        word_freqs[word_tokens] = freq\n",
    "    \n",
    "    # 迭代合并\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    for i in range(num_merges):\n",
    "        token_counts, pair_counts = get_stats(word_freqs)\n",
    "        \n",
    "        if not pair_counts:\n",
    "            break\n",
    "        \n",
    "        # 计算概率分数：count(pair) / (count(token1) * count(token2))\n",
    "        best_pair = None\n",
    "        max_score = -1\n",
    "        for pair, count in pair_counts.items():\n",
    "            if token_counts[pair[0]] > 0 and token_counts[pair[1]] > 0:\n",
    "                score = count / (token_counts[pair[0]] * token_counts[pair[1]])\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_pair = pair\n",
    "        \n",
    "        if best_pair is None:\n",
    "            break\n",
    "        \n",
    "        # 执行合并\n",
    "        word_freqs = merge_vocab(best_pair, word_freqs)\n",
    "        new_token = \"\".join(best_pair)\n",
    "        vocab.add(new_token)\n",
    "        \n",
    "        print(f\"Merge {i+1}/{num_merges}: {best_pair} -> {new_token} (Score: {max_score:.4f})\")\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def encode_wordpiece(text, vocab):\n",
    "    \"\"\"\n",
    "    使用WordPiece进行编码（贪心最长匹配）\n",
    "    \"\"\"\n",
    "    if text == \"\":\n",
    "        return []\n",
    "    \n",
    "    tokens = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        # 从长到短寻找词汇表中的最长子词\n",
    "        match = \"\"\n",
    "        for end in range(len(text), start, -1):\n",
    "            subword = text[start:end]\n",
    "            if subword in vocab:\n",
    "                match = subword\n",
    "                break\n",
    "        \n",
    "        if not match:\n",
    "            tokens.append('[UNK]')\n",
    "            start += 1\n",
    "            continue\n",
    "        \n",
    "        # 非起始位置的token加##前缀\n",
    "        if start > 0:\n",
    "            tokens.append('##' + match)\n",
    "        else:\n",
    "            tokens.append(match)\n",
    "        \n",
    "        start += len(match)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# 测试\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = {'low': 5, 'lower': 2, 'newest': 6, 'widest': 3}\n",
    "    final_vocab = train_wordpiece(corpus, vocab_size=30)\n",
    "    \n",
    "    clean_vocab = {token.replace('</w>', '') for token in final_vocab}\n",
    "    clean_vocab.add('[UNK]')\n",
    "    \n",
    "    test_words = [\"lowest\", \"newer\"]\n",
    "    for word in test_words:\n",
    "        encoded = encode_wordpiece(word, clean_vocab)\n",
    "        print(f\"{word} -> {encoded}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
