{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 8. 强化学习：价值迭代 (Value Iteration)\n",
    "\n",
    "### 问题：网格世界寻路\n",
    "- 3×4网格，含有墙壁和陷阱\n",
    "- 目标：找到最优策略最大化累积奖励\n",
    "\n",
    "### 贝尔曼最优方程\n",
    "V*(s) = max_a [R(s,a) + γ * V*(s')]\n",
    "\n",
    "### 价值迭代算法\n",
    "1. 初始化所有状态的价值为0\n",
    "2. 对每个状态，计算所有动作的Q值\n",
    "3. 取Q值最大的作为新的状态价值\n",
    "4. 重复直到收敛（价值变化<阈值）\n",
    "\n",
    "### 最优策略提取\n",
    "π*(s) = argmax_a [R(s,a) + γ * V*(s')]\n",
    "\n",
    "### 应用场景\n",
    "- 机器人路径规划\n",
    "- 游戏AI\n",
    "- 资源分配等\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        # 定义网格世界的形状\n",
    "        self.shape = (3, 4)\n",
    "        # 定义终止状态及其奖励\n",
    "        self.terminal_states = {(0, 3): 1, (1, 3): -1}\n",
    "        # 定义墙壁位置\n",
    "        self.wall_states = [(1, 1)]\n",
    "        # 定义所有状态（不包括墙壁）\n",
    "        self.states = []\n",
    "        for r in range(self.shape[0]):\n",
    "            for c in range(self.shape[1]):\n",
    "                if (r, c) not in self.wall_states:\n",
    "                    self.states.append((r, c))\n",
    "        # 定义动作\n",
    "        self.actions = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "        # 定义移动的固定成本\n",
    "        self.step_reward = -0.04\n",
    "        # 折扣因子\n",
    "        self.gamma = 1.0\n",
    "\n",
    "    def get_next_state_and_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        确定性环境：给定状态和动作，返回下一状态和奖励\n",
    "        \"\"\"\n",
    "        if state in self.terminal_states:\n",
    "            return state, 0\n",
    "\n",
    "        move = self.actions[action]\n",
    "        next_state = (state[0] + move[0], state[1] + move[1])\n",
    "        \n",
    "        # 边界和墙壁检查\n",
    "        r, c = next_state\n",
    "        if r < 0 or r >= self.shape[0] or c < 0 or c >= self.shape[1] or next_state in self.wall_states:\n",
    "            next_state = state\n",
    "        \n",
    "        # 奖励\n",
    "        if next_state in self.terminal_states:\n",
    "            reward = self.terminal_states[next_state]\n",
    "        else:\n",
    "            reward = self.step_reward\n",
    "            \n",
    "        return next_state, reward\n",
    "\n",
    "def value_iteration(env, theta=1e-4):\n",
    "    \"\"\"\n",
    "    执行价值迭代算法\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld环境\n",
    "        theta: 收敛阈值\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (最优价值函数, 最优策略)\n",
    "    \"\"\"\n",
    "    # 初始化价值函数\n",
    "    V = {s: 0 for s in env.states}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        delta = 0\n",
    "        \n",
    "        # 对所有状态更新价值\n",
    "        for s in env.states:\n",
    "            if s in env.terminal_states:\n",
    "                continue\n",
    "            \n",
    "            v_old = V[s]\n",
    "            action_values = []\n",
    "            \n",
    "            # 计算所有动作的Q值\n",
    "            for action in env.actions:\n",
    "                next_state, reward = env.get_next_state_and_reward(s, action)\n",
    "                q_value = reward + env.gamma * V[next_state]\n",
    "                action_values.append(q_value)\n",
    "            \n",
    "            # 贝尔曼最优方程：取最大Q值\n",
    "            V[s] = max(action_values)\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "        \n",
    "        print(f\"Iteration {iteration}, Delta: {delta:.6f}\")\n",
    "\n",
    "        # 收敛检查\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # 提取最优策略\n",
    "    pi_star = {}\n",
    "    for s in env.states:\n",
    "        if s in env.terminal_states:\n",
    "            continue\n",
    "        \n",
    "        best_action = None\n",
    "        max_q_value = -float('inf')\n",
    "        \n",
    "        for action in env.actions:\n",
    "            next_state, reward = env.get_next_state_and_reward(s, action)\n",
    "            q_value = reward + env.gamma * V[next_state]\n",
    "            if q_value > max_q_value:\n",
    "                max_q_value = q_value\n",
    "                best_action = action\n",
    "        \n",
    "        pi_star[s] = best_action\n",
    "    \n",
    "    return V, pi_star\n",
    "\n",
    "# 测试（需在外部调用）\n",
    "# if __name__ == \"__main__\":\n",
    "#     env = GridWorld()\n",
    "#     V, pi = value_iteration(env)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
