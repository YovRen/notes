{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f9d938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Tensor(sizes) / torch.empty(sizes) ---\n",
      "torch.empty(2, 3):\n",
      "tensor([[4.2973e+09, 8.8702e-43, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "\n",
      "--- 2. tensor(data) ---\n",
      "torch.tensor([[1, 2], [3, 4]]):\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "torch.tensor(np.array([5, 6, 7])):\n",
      "tensor([5, 6, 7], dtype=torch.int32)\n",
      "\n",
      "--- 3. ones(sizes) ---\n",
      "torch.ones(2, 3):\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "--- 4. zeros(sizes) ---\n",
      "torch.zeros(3, 2):\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "--- 5. eye(sizes) ---\n",
      "torch.eye(3):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "torch.eye(4, 2):\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "--- 6. arange(s, e, step) ---\n",
      "torch.arange(0, 10, 2):\n",
      "tensor([0, 2, 4, 6, 8])\n",
      "\n",
      "--- 7. linspace(s, e, steps) ---\n",
      "torch.linspace(0, 1, 5):\n",
      "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "\n",
      "--- 8. rand/randn(sizes) ---\n",
      "torch.rand(2, 2):\n",
      "tensor([[0.6339, 0.9639],\n",
      "        [0.2309, 0.9678]])\n",
      "\n",
      "torch.randn(2, 2):\n",
      "tensor([[-0.2483,  0.3796],\n",
      "        [-1.0685,  1.3851]])\n",
      "\n",
      "--- 9. normal(mean, std) ---\n",
      "torch.normal(mean=torch.tensor([0.0, 1.0]), std=torch.tensor([1.0, 0.5])):\n",
      "tensor([ 1.2500, -0.1945])\n",
      "\n",
      "torch.normal(mean=0.0, std=1.0, size=(2, 2)):\n",
      "tensor([[-0.1843,  0.0299],\n",
      "        [-0.3297,  0.3638]])\n",
      "\n",
      "--- 10. randperm(m) ---\n",
      "torch.randperm(5):\n",
      "tensor([3, 4, 2, 1, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- 1. Tensor(sizes) / torch.empty(sizes) ---\")\n",
    "# Tensor(sizes) 是 torch.empty(sizes) 的别名，用于创建一个指定形状但未初始化的张量。\n",
    "# 它的值是内存中当前存在的任意值。\n",
    "# 注意：PyTorch 默认创建的 Tensor 是 float32 类型 [4]。\n",
    "t_empty = torch.empty(2, 3)\n",
    "print(f\"torch.empty(2, 3):\\n{t_empty}\\n\") # cite: 1, 4\n",
    "\n",
    "print(\"--- 2. tensor(data) ---\")\n",
    "# 从 Python 列表、元组或 NumPy 数组创建张量，类似于 NumPy 的 np.array [1, 5, 11]。\n",
    "# 数据类型会自动推断 [11]。\n",
    "data = [[1, 2], [3, 4]]\n",
    "t_from_list = torch.tensor(data)\n",
    "print(f\"torch.tensor([[1, 2], [3, 4]]):\\n{t_from_list}\\n\") # cite: 1, 11\n",
    "\n",
    "numpy_array = np.array([5, 6, 7])\n",
    "t_from_numpy = torch.tensor(numpy_array) # 或者 torch.from_numpy(numpy_array) [1]\n",
    "print(f\"torch.tensor(np.array([5, 6, 7])):\\n{t_from_numpy}\\n\") # cite: 1\n",
    "\n",
    "print(\"--- 3. ones(sizes) ---\")\n",
    "# 创建一个指定形状的张量，所有元素都为 1 [1, 9, 30]。\n",
    "t_ones = torch.ones(2, 3)\n",
    "print(f\"torch.ones(2, 3):\\n{t_ones}\\n\") # cite: 1, 9\n",
    "\n",
    "print(\"--- 4. zeros(sizes) ---\")\n",
    "# 创建一个指定形状的张量，所有元素都为 0 [1, 6, 8]。\n",
    "t_zeros = torch.zeros(3, 2)\n",
    "print(f\"torch.zeros(3, 2):\\n{t_zeros}\\n\") # cite: 1, 6\n",
    "\n",
    "print(\"--- 5. eye(sizes) ---\")\n",
    "# 创建一个 2D 张量（单位矩阵），对角线为 1，其余为 0 [25, 27]。\n",
    "# 参数 n 为行数，m 为列数。如果只提供 n，则创建一个 n x n 的方阵 [26]。\n",
    "t_eye_square = torch.eye(3)\n",
    "print(f\"torch.eye(3):\\n{t_eye_square}\\n\") # cite: 25\n",
    "\n",
    "t_eye_rect = torch.eye(4, 2)\n",
    "print(f\"torch.eye(4, 2):\\n{t_eye_rect}\\n\") # cite: 26\n",
    "\n",
    "print(\"--- 6. arange(s, e, step) ---\")\n",
    "# 创建一个 1D 张量，包含从 s (start) 到 e (end) 的值，步长为 step (不包含 e) [2, 3]。\n",
    "# 注意：torch.range() 已被弃用，应使用 torch.arange() [2]。\n",
    "t_arange = torch.arange(0, 10, 2)\n",
    "print(f\"torch.arange(0, 10, 2):\\n{t_arange}\\n\") # cite: 2\n",
    "\n",
    "print(\"--- 7. linspace(s, e, steps) ---\")\n",
    "# 创建一个 1D 张量，包含从 s (start) 到 e (end) 之间均匀分布的 steps 个点 (包含 s 和 e) [2, 16, 29]。\n",
    "t_linspace = torch.linspace(0, 1, 5)\n",
    "print(f\"torch.linspace(0, 1, 5):\\n{t_linspace}\\n\") # cite: 16\n",
    "\n",
    "print(\"--- 8. rand/randn(sizes) ---\")\n",
    "# rand(sizes): 创建一个指定形状的张量，元素是来自 [0, 1) 区间均匀分布的随机数 [1, 22]。\n",
    "t_rand = torch.rand(2, 2)\n",
    "print(f\"torch.rand(2, 2):\\n{t_rand}\\n\") # cite: 1\n",
    "\n",
    "# randn(sizes): 创建一个指定形状的张量，元素是来自标准正态分布 (均值 0，方差 1) 的随机数 [14, 17, 24]。\n",
    "t_randn = torch.randn(2, 2)\n",
    "print(f\"torch.randn(2, 2):\\n{t_randn}\\n\") # cite: 14\n",
    "\n",
    "print(\"--- 9. normal(mean, std) ---\")\n",
    "# 创建一个张量，其中的随机数来自单独的正态分布，其均值和标准差由 mean 和 std 参数指定 [7, 10, 12]。\n",
    "# mean 和 std 可以是标量或形状匹配的张量。\n",
    "mean_tensor = torch.tensor([0.0, 1.0])\n",
    "std_tensor = torch.tensor([1.0, 0.5])\n",
    "t_normal = torch.normal(mean=mean_tensor, std=std_tensor)\n",
    "print(f\"torch.normal(mean=torch.tensor([0.0, 1.0]), std=torch.tensor([1.0, 0.5])):\\n{t_normal}\\n\") # cite: 10\n",
    "\n",
    "# 也可以用标量均值和标准差\n",
    "t_normal_scalar = torch.normal(mean=0.0, std=1.0, size=(2, 2))\n",
    "print(f\"torch.normal(mean=0.0, std=1.0, size=(2, 2)):\\n{t_normal_scalar}\\n\") # cite: 10\n",
    "\n",
    "print(\"--- 10. randperm(m) ---\")\n",
    "# 生成一个从 0 到 n-1 (不包含 n) 的整数随机排列的 1D 张量 [19, 23, 36]。\n",
    "# 常用语洗牌数据集或生成随机序列 [19, 36]。\n",
    "t_randperm = torch.randperm(5)\n",
    "print(f\"torch.randperm(5):\\n{t_randperm}\\n\") # cite: 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb38152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 加法操作 (Addition) ---\n",
      "a + b:\n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "\n",
      "torch.add(a, b):\n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "\n",
      "a.add_(b) (修改后的 a):\n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "\n",
      "--- 2. 索引 (Indexing) ---\n",
      "tensor[0, 0]: 10\n",
      "\n",
      "tensor[1, :]: tensor([13, 14, 15])\n",
      "\n",
      "tensor[1]: tensor([13, 14, 15])\n",
      "\n",
      "tensor[:, 2]: tensor([12, 15, 18])\n",
      "\n",
      "tensor[0:2, 0:2]:\n",
      "tensor([[10, 11],\n",
      "        [13, 14]])\n",
      "\n",
      "tensor[indices_rows, indices_cols]: tensor([10, 17])\n",
      "\n",
      "--- 3. 维度变换 (Reshaping / View) ---\n",
      "原始张量 x (4x4):\n",
      "tensor([[ 0.7731,  0.7889, -0.3229,  2.1550],\n",
      "        [-0.9092, -1.4473, -0.8163, -0.2732],\n",
      "        [ 2.1582,  1.0211, -2.0695, -2.7672],\n",
      "        [ 1.2168, -1.3652, -1.2796,  1.1501]])\n",
      "\n",
      "x.view(16):\n",
      "tensor([ 0.7731,  0.7889, -0.3229,  2.1550, -0.9092, -1.4473, -0.8163, -0.2732,\n",
      "         2.1582,  1.0211, -2.0695, -2.7672,  1.2168, -1.3652, -1.2796,  1.1501])\n",
      "\n",
      "x.view(-1, 8):\n",
      "tensor([[ 0.7731,  0.7889, -0.3229,  2.1550, -0.9092, -1.4473, -0.8163, -0.2732],\n",
      "        [ 2.1582,  1.0211, -2.0695, -2.7672,  1.2168, -1.3652, -1.2796,  1.1501]])\n",
      "\n",
      "x.reshape(2, 8):\n",
      "tensor([[ 0.7731,  0.7889, -0.3229,  2.1550, -0.9092, -1.4473, -0.8163, -0.2732],\n",
      "        [ 2.1582,  1.0211, -2.0695, -2.7672,  1.2168, -1.3652, -1.2796,  1.1501]])\n",
      "\n",
      "--- 4. 拼接 (Concatenation) ---\n",
      "torch.cat((tensor1, tensor2), dim=0):\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "\n",
      "torch.cat((tensor1, tensor2), dim=1):\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n",
      "\n",
      "--- 5. 堆叠 (Stacking) ---\n",
      "torch.stack((tensor1, tensor2), dim=0):\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "\n",
      "torch.stack((tensor1, tensor2), dim=1):\n",
      "tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n",
      "\n",
      "--- 6. 展开 (Squeeze) 和 扩充 (Unsqueeze) ---\n",
      "原始张量 x_squeeze (1x2x1x3):\n",
      "torch.Size([1, 2, 1, 3])\n",
      "\n",
      "x_squeeze.squeeze(): torch.Size([2, 3])\n",
      "\n",
      "x_squeeze.squeeze(0): torch.Size([2, 1, 3])\n",
      "\n",
      "原始张量 a_unsqueeze (形状 (3,)):\n",
      "torch.Size([3])\n",
      "\n",
      "a_unsqueeze.unsqueeze(0): torch.Size([1, 3])\n",
      "\n",
      "a_unsqueeze.unsqueeze(1): torch.Size([3, 1])\n",
      "\n",
      "--- 7. 转置 (Transpose) ---\n",
      "原始矩阵 (2x3):\n",
      "<built-in method view of Tensor object at 0x000001799BCAA540>\n",
      "\n",
      "矩阵转置 (.T):\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "\n",
      "原始高维张量 (2x3x4):\n",
      "torch.Size([2, 3, 4])\n",
      "\n",
      "高维张量转置 (交换 dim 0 和 dim 2): torch.Size([4, 3, 2])\n",
      "\n",
      "--- 8. 元素数量 (Number of elements) ---\n",
      "张量 tensor 的元素数量: 9\n",
      "\n",
      "--- 9. 类型转换 (Type Casting) ---\n",
      "原始浮点张量 (dtype: torch.float32):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "转换为 int64 的张量 (dtype: torch.int64):\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "转换为 float64 的张量 (dtype: torch.float64):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n",
      "\n",
      "--- 10. 克隆 (Clone) ---\n",
      "原始张量 (修改克隆后): tensor([1, 2, 3])\n",
      "\n",
      "克隆张量 (被修改): tensor([99,  2,  3])\n",
      "\n",
      "--- 11. item() ---\n",
      "单元素张量: 42, Python 数值: 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"--- 1. 加法操作 (Addition) ---\")\n",
    "# 逐元素加法 [1]。\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# 方法一：使用运算符 '+' [1]\n",
    "c_add_op = a + b\n",
    "print(f\"a + b:\\n{c_add_op}\\n\")\n",
    "\n",
    "# 方法二：使用 torch.add() 函数 [1]\n",
    "c_add_func = torch.add(a, b)\n",
    "print(f\"torch.add(a, b):\\n{c_add_func}\\n\")\n",
    "\n",
    "# 方法三：in-place 加法 (就地操作)，会修改原 Tensor [1]\n",
    "# 注意：PyTorch 中的就地操作通常以 '_' 结尾 [1, 2]。\n",
    "a.add_(b)\n",
    "print(f\"a.add_(b) (修改后的 a):\\n{a}\\n\")\n",
    "\n",
    "print(\"--- 2. 索引 (Indexing) ---\")\n",
    "# PyTorch 的索引与 NumPy 类似 [1]。\n",
    "tensor = torch.tensor([[10, 11, 12],\n",
    "                       [13, 14, 15],\n",
    "                       [16, 17, 18]])\n",
    "\n",
    "# 获取单个元素\n",
    "print(f\"tensor[0, 0]: {tensor[0, 0]}\\n\") # 获取第一行第一列的元素 [1]\n",
    "\n",
    "# 获取一行\n",
    "print(f\"tensor[1, :]: {tensor[1, :]}\\n\") # 获取第二行所有元素 [1]\n",
    "print(f\"tensor[1]: {tensor[1]}\\n\")      # 简写形式 [1]\n",
    "\n",
    "# 获取一列\n",
    "print(f\"tensor[:, 2]: {tensor[:, 2]}\\n\") # 获取第三列所有元素 [1]\n",
    "\n",
    "# 获取子区域\n",
    "print(f\"tensor[0:2, 0:2]:\\n{tensor[0:2, 0:2]}\\n\") # 获取左上角 2x2 子矩阵 [1]\n",
    "\n",
    "# 使用列表或张量进行高级索引 (Fancy Indexing)\n",
    "indices_rows = torch.tensor([0, 2])\n",
    "indices_cols = torch.tensor([0, 1])\n",
    "print(f\"tensor[indices_rows, indices_cols]: {tensor[indices_rows, indices_cols]}\\n\") # 获取 (0,0) 和 (2,1) 处的元素 [1]\n",
    "\n",
    "print(\"--- 3. 维度变换 (Reshaping / View) ---\")\n",
    "# .view() 或 .reshape() 用于改变 Tensor 的形状 [1, 3]。\n",
    "# .view() 共享底层数据，而 .reshape() 可能复制数据 [1]。\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "print(f\"原始张量 x (4x4):\\n{x}\\n\")\n",
    "\n",
    "# .view() 改变形状，共享数据 [1, 3]。\n",
    "y = x.view(16) # 将 4x4 变为 1x16 [1]\n",
    "print(f\"x.view(16):\\n{y}\\n\")\n",
    "\n",
    "z = x.view(-1, 8) # -1 会自动推断维度大小 [1, 3]。这里是 2x8\n",
    "print(f\"x.view(-1, 8):\\n{z}\\n\")\n",
    "\n",
    "# .reshape() 也是改变形状，如果内存不连续可能会复制数据 [1]。\n",
    "a_reshape = x.reshape(2, 8)\n",
    "print(f\"x.reshape(2, 8):\\n{a_reshape}\\n\")\n",
    "\n",
    "print(\"--- 4. 拼接 (Concatenation) ---\")\n",
    "# torch.cat() 用于在给定维度上拼接多个张量 [1]。\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# 沿维度 0 拼接 (按行堆叠) [1]\n",
    "cat_dim0 = torch.cat((tensor1, tensor2), dim=0)\n",
    "print(f\"torch.cat((tensor1, tensor2), dim=0):\\n{cat_dim0}\\n\")\n",
    "\n",
    "# 沿维度 1 拼接 (按列堆叠) [1]\n",
    "cat_dim1 = torch.cat((tensor1, tensor2), dim=1)\n",
    "print(f\"torch.cat((tensor1, tensor2), dim=1):\\n{cat_dim1}\\n\")\n",
    "\n",
    "print(\"--- 5. 堆叠 (Stacking) ---\")\n",
    "# torch.stack() 用于沿着一个新的维度堆叠多个张量 [1, 13]。\n",
    "# 类似于将多个同形状的张量“垒起来” [13]。\n",
    "stack_dim0 = torch.stack((tensor1, tensor2), dim=0)\n",
    "print(f\"torch.stack((tensor1, tensor2), dim=0):\\n{stack_dim0}\\n\")\n",
    "# 结果形状是 (2, 2, 2)\n",
    "\n",
    "stack_dim1 = torch.stack((tensor1, tensor2), dim=1)\n",
    "print(f\"torch.stack((tensor1, tensor2), dim=1):\\n{stack_dim1}\\n\")\n",
    "# 结果形状是 (2, 2, 2)\n",
    "\n",
    "print(\"--- 6. 展开 (Squeeze) 和 扩充 (Unsqueeze) ---\")\n",
    "# squeeze() 移除所有大小为 1 的维度 [1, 28]。\n",
    "# unsqueeze() 在指定维度添加一个大小为 1 的维度 [1, 21]。\n",
    "\n",
    "x_squeeze = torch.zeros(1, 2, 1, 3)\n",
    "print(f\"原始张量 x_squeeze (1x2x1x3):\\n{x_squeeze.shape}\\n\")\n",
    "\n",
    "y_squeezed = x_squeeze.squeeze() # 移除所有大小为 1 的维度\n",
    "print(f\"x_squeeze.squeeze(): {y_squeezed.shape}\\n\") # 形状变为 (2, 3) [1, 28]\n",
    "\n",
    "z_squeezed_dim = x_squeeze.squeeze(0) # 只移除指定维度 (dim=0) 的大小为 1 的维度 [1]\n",
    "print(f\"x_squeeze.squeeze(0): {z_squeezed_dim.shape}\\n\") # 形状变为 (2, 1, 3)\n",
    "\n",
    "a_unsqueeze = torch.tensor([1, 2, 3]) # 形状是 (3,)\n",
    "print(f\"原始张量 a_unsqueeze (形状 (3,)):\\n{a_unsqueeze.shape}\\n\")\n",
    "\n",
    "b_unsqueezed = a_unsqueeze.unsqueeze(0) # 在 dim=0 处添加一个维度\n",
    "print(f\"a_unsqueeze.unsqueeze(0): {b_unsqueezed.shape}\\n\") # 形状变为 (1, 3) [1, 21]\n",
    "\n",
    "c_unsqueezed = a_unsqueeze.unsqueeze(1) # 在 dim=1 处添加一个维度\n",
    "print(f\"a_unsqueeze.unsqueeze(1): {c_unsqueezed.shape}\\n\") # 形状变为 (3, 1) [1, 21]\n",
    "\n",
    "print(\"--- 7. 转置 (Transpose) ---\")\n",
    "# .T 属性或 .transpose() 方法用于交换两个维度 [1]。\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "print(f\"原始矩阵 (2x3):\\n{matrix.view}\\n\")\n",
    "\n",
    "matrix_t = matrix.T # .T 属性是 .transpose(0, 1) 的简写 [1]\n",
    "print(f\"矩阵转置 (.T):\\n{matrix_t}\\n\") # 形状变为 3x2\n",
    "\n",
    "# 对于更高维度的张量，可以使用 transpose(dim0, dim1)\n",
    "high_dim_tensor = torch.randn(2, 3, 4)\n",
    "print(f\"原始高维张量 (2x3x4):\\n{high_dim_tensor.shape}\\n\")\n",
    "transposed_tensor = high_dim_tensor.transpose(0, 2) # 交换维度 0 和维度 2 [1]\n",
    "print(f\"高维张量转置 (交换 dim 0 和 dim 2): {transposed_tensor.shape}\\n\") # 形状变为 4x3x2\n",
    "\n",
    "print(\"--- 8. 元素数量 (Number of elements) ---\")\n",
    "# .numel() 返回张量中元素的总数量 [1].\n",
    "# .size() 或 .shape 返回张量的形状 [1].\n",
    "elements_count = tensor.numel()\n",
    "print(f\"张量 tensor 的元素数量: {elements_count}\\n\") # cite: 1\n",
    "\n",
    "print(\"--- 9. 类型转换 (Type Casting) ---\")\n",
    "# 使用 .to() 或 .type() 方法转换数据类型 [1].\n",
    "float_tensor = torch.ones(2, 2, dtype=torch.float32)\n",
    "print(f\"原始浮点张量 (dtype: {float_tensor.dtype}):\\n{float_tensor}\\n\")\n",
    "\n",
    "int_tensor = float_tensor.to(torch.int64) # 转换为 long (int64) 类型 [1]\n",
    "print(f\"转换为 int64 的张量 (dtype: {int_tensor.dtype}):\\n{int_tensor}\\n\")\n",
    "\n",
    "double_tensor = float_tensor.type(torch.float64) # 另一种类型转换方式 [1]\n",
    "print(f\"转换为 float64 的张量 (dtype: {double_tensor.dtype}):\\n{double_tensor}\\n\")\n",
    "\n",
    "print(\"--- 10. 克隆 (Clone) ---\")\n",
    "# .clone() 创建张量的一个副本 [1].\n",
    "original_tensor = torch.tensor([1, 2, 3])\n",
    "cloned_tensor = original_tensor.clone()\n",
    "cloned_tensor[0] = 99\n",
    "print(f\"原始张量 (修改克隆后): {original_tensor}\\n\")\n",
    "print(f\"克隆张量 (被修改): {cloned_tensor}\\n\")\n",
    "\n",
    "print(\"--- 11. item() ---\")\n",
    "# 对于只包含一个元素的张量，可以使用 .item() 方法获取其 Python 数值 [1].\n",
    "single_element_tensor = torch.tensor(42)\n",
    "python_number = single_element_tensor.item()\n",
    "print(f\"单元素张量: {single_element_tensor}, Python 数值: {python_number}\\n\") # cite: 1\n",
    "# 如果张量包含多个元素，使用 .item() 会报错。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
