# ç¬¬01è¯¾ï¼šé«˜ç»´ç©ºé—´çš„æ•°å­¦ç›´è§‰

> å‘é‡ç‚¹ç§¯çš„å‡ ä½•æ„ä¹‰ï¼ˆç›¸ä¼¼åº¦ï¼‰ã€çŸ©é˜µä¹˜æ³•çš„æœ¬è´¨ï¼ˆå˜æ¢ï¼‰ã€ç§© (Rank) å¯¹æ¨¡å‹å‹ç¼©çš„æ„ä¹‰

---

## ç¬”è®°å†…å®¹

ä½ å¥½ã€‚å¾ˆé«˜å…´èƒ½ä¸ä¸€ä½èµ„æ·±ä»ä¸šè€…æ¢è®¨è¿™äº›æœ¬è´¨é—®é¢˜ã€‚

å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ¬è´¨æ˜¯åœ¨é«˜ç»´å‘é‡ç©ºé—´ï¼ˆHigh-Dimensional Vector Spaceï¼‰ä¸­è¿›è¡Œçš„æµå½¢å­¦ä¹ ï¼ˆManifold Learningï¼‰ä¸æ¦‚ç‡é¢„æµ‹ã€‚å¤§å¤šæ•°å·¥ç¨‹ç›´è§‰ï¼ˆå¦‚æ¬§æ°è·ç¦»çš„ç‰©ç†æ„ä¹‰ï¼‰åœ¨ $D > 100$ çš„ç»´åº¦ä¸‹ä¼šå¤±æ•ˆï¼ˆCurse of Dimensionalityï¼‰ã€‚

æœ¬è¯¾æˆ‘ä»¬å°†å»ºç«‹ç†è§£ LLM è¡Œä¸ºçš„**æ•°å­¦åŸè¯­ï¼ˆMathematical Primitivesï¼‰**ï¼Œç‰¹åˆ«æ˜¯ç‚¹ç§¯çš„ç»Ÿè®¡ç‰¹æ€§ã€çŸ©é˜µç§©ï¼ˆRankï¼‰ä¸ä¿¡æ¯å‹ç¼©çš„å…³ç³»ï¼Œè¿™ç›´æ¥å†³å®šäº† Attention çš„ç¼©æ”¾æœºåˆ¶ä»¥åŠ LoRA ç­‰å¾®è°ƒç®—æ³•çš„ç†è®ºå¯è¡Œæ€§ã€‚

---

### 1. ğŸ§® æ•°å­¦ä¸ç†è®ºæ¨å¯¼ (Theoretical Foundation)

æˆ‘ä»¬é¦–å…ˆè§£å†³ä¸€ä¸ªåœ¨ Transformer è®ºæ–‡ä¸­è¢«ä¸€ç¬”å¸¦è¿‡ï¼Œä½†å¯¹æ¨¡å‹æ”¶æ•›è‡³å…³é‡è¦çš„æ•°å­¦ç»†èŠ‚ï¼š**ä¸ºä»€ä¹ˆ Scaled Dot-Product Attention éœ€è¦é™¤ä»¥ $\sqrt{d_k}$ï¼Ÿ**

#### 1.1 é«˜ç»´ç‚¹ç§¯çš„æ–¹å·®è†¨èƒ€ (Variance Scaling)

å‡è®¾ Query å‘é‡ $q$ å’Œ Key å‘é‡ $k$ çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œä¸”æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆè¿™æ˜¯ Xavier/Kaiming åˆå§‹åŒ–çš„å‡è®¾åŸºç¡€ï¼‰ï¼š

$$
q_i, k_i \sim \mathcal{N}(0, 1)
$$

å®ƒä»¬çš„ç‚¹ç§¯ $x = q \cdot k$ å®šä¹‰ä¸ºï¼š

$$
x = \sum_{i=1}^{d_k} q_i k_i
$$

æˆ‘ä»¬éœ€è¦è®¡ç®— $x$ çš„æœŸæœ›å’Œæ–¹å·®ã€‚
æ ¹æ®æœŸæœ›çš„çº¿æ€§æ€§è´¨å’Œç‹¬ç«‹æ€§ï¼š

$$
\mathbb{E}[x] = \sum_{i=1}^{d_k} \mathbb{E}[q_i]\mathbb{E}[k_i] = 0
$$

å¯¹äºæ–¹å·®ï¼Œç”±äº $q_i, k_i$ ç‹¬ç«‹ï¼Œ$\text{Var}(XY) = \mathbb{E}[X^2]\mathbb{E}[Y^2] - (\mathbb{E}[X]\mathbb{E}[Y])^2$ã€‚
å·²çŸ¥ $\mathbb{E}[q_i^2] = \text{Var}(q_i) + (\mathbb{E}[q_i])^2 = 1 + 0 = 1$ã€‚

$$
\text{Var}(q_i k_i) = 1 \cdot 1 - 0 = 1
$$

æ ¹æ®æ–¹å·®çš„å¯åŠ æ€§ï¼ˆç‹¬ç«‹å˜é‡ä¹‹å’Œçš„æ–¹å·®ç­‰äºæ–¹å·®ä¹‹å’Œï¼‰ï¼š

$$
\text{Var}(x) = \text{Var}\left(\sum_{i=1}^{d_k} q_i k_i\right) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = \sum_{i=1}^{d_k} 1 = d_k
$$

**ç»“è®º**ï¼šå¦‚æœä¸è¿›è¡Œç¼©æ”¾ï¼ŒAttention Score çš„ç‚¹ç§¯ç»“æœæ–¹å·®ä¸º $d_k$ï¼Œæ ‡å‡†å·®ä¸º $\sqrt{d_k}$ã€‚
å¯¹äº $d_k=128$ï¼ˆå¸¸è§ Head Dimensionï¼‰ï¼Œæ ‡å‡†å·® $\approx 11.3$ã€‚

#### 1.2 å¯¹ Softmax æ¢¯åº¦çš„è‡´å‘½å½±å“

Attention çš„æƒé‡è®¡ç®—å¦‚ä¸‹ï¼š

$$
\text{Attention}(Q, K) = \text{Softmax}\left(\frac{QK^T}{\text{scale}}\right)
$$

å¦‚æœ scale $= 1$ï¼Œè¾“å…¥ Softmax çš„ logits åˆ†å¸ƒèŒƒå›´ä¼šéå¸¸å¤§ï¼ˆä¾‹å¦‚ $[-30, 30]$ï¼‰ã€‚
Softmax å‡½æ•° $S(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}$ çš„å±€éƒ¨æ¢¯åº¦ï¼ˆJacobianï¼‰ä¸ $S(x_i)(1 - S(x_i))$ æˆæ­£æ¯”ã€‚

* å½“ $x_i$ å¾ˆå¤§æ—¶ï¼Œ$S(x_i) \to 1$ï¼Œæ¢¯åº¦ $\to 0$ã€‚
* å½“ $x_i$ å¾ˆå°æ—¶ï¼Œ$S(x_i) \to 0$ï¼Œæ¢¯åº¦ $\to 0$ã€‚

**æ¢¯åº¦è§†è§’**ï¼š
é«˜ç»´ç©ºé—´ä¸‹ä¸è¿›è¡Œ $\frac{1}{\sqrt{d_k}}$ ç¼©æ”¾ï¼Œä¼šå¯¼è‡´ logits è½å…¥ Softmax çš„**é¥±å’ŒåŒºï¼ˆSaturation Regionï¼‰**ï¼Œé€ æˆ**æ¢¯åº¦æ¶ˆå¤±ï¼ˆVanishing Gradientsï¼‰**ï¼Œæ¨¡å‹æ— æ³•è®­ç»ƒã€‚é™¤ä»¥ $\sqrt{d_k}$ åï¼Œæ–¹å·®å›å½’ä¸º 1ï¼Œç¡®ä¿åˆå§‹è®­ç»ƒé˜¶æ®µæ¢¯åº¦æµç¨³å®šã€‚

##### P1. ä¸ºä»€ä¹ˆ Softmax æ¢¯åº¦ä¸ $S_i(1-S_i)$ æˆæ­£æ¯”ï¼Ÿ

Softmax å®šä¹‰ï¼š$S_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$

æˆ‘ä»¬æ±‚ $\frac{\partial S_i}{\partial x_i}$ï¼ˆå¯¹è‡ªå·±çš„åå¯¼ï¼‰ï¼š

ç”¨å•†çš„æ±‚å¯¼æ³•åˆ™ $\left(\frac{u}{v}\right)' = \frac{u'v - uv'}{v^2}$ï¼š

$$
\frac{\partial S_i}{\partial x_i} = \frac{e^{x_i} \cdot \sum_j e^{x_j} - e^{x_i} \cdot e^{x_i}}{(\sum_j e^{x_j})^2}
$$

åˆ†å­æå–å…¬å› å­ $e^{x_i}$ï¼š

$$
= \frac{e^{x_i}(\sum_j e^{x_j} - e^{x_i})}{(\sum_j e^{x_j})^2} = \frac{e^{x_i}}{\sum_j e^{x_j}} \cdot \frac{\sum_j e^{x_j} - e^{x_i}}{\sum_j e^{x_j}}
$$

$$
= S_i \cdot (1 - S_i)
$$

âœ… è¿™å°±æ˜¯å®Œæ•´æ¨å¯¼â€”â€”ä¸æ˜¯"æˆæ­£æ¯”"ï¼Œè€Œæ˜¯æ°å¥½ç­‰äº $S_i(1-S_i)$ã€‚

##### P2. $[-30, 30]$ å’Œ $[-1, 1]$ åŒºåˆ«å¤§å—ï¼Ÿ

åŒºåˆ«éå¸¸å¤§ï¼å‡è®¾æœ‰ 3 ä¸ª logitsï¼ˆæ¨¡å‹æ‰“åˆ†ï¼‰ï¼Œæˆ‘ä»¬çœ‹ Softmax è¾“å‡ºï¼š

| åœºæ™¯   | Logits               | Softmax è¾“å‡º                            | æœ€å¤§å…ƒç´ çš„æ¢¯åº¦              |
| ------ | -------------------- | --------------------------------------- | --------------------------- |
| å°èŒƒå›´ | $[1.0, 0.5, -0.5]$ | $[0.47, 0.29, 0.11]$                  | $0.47 \times 0.53 = 0.25$ |
| å¤§èŒƒå›´ | $[30, 15, -15]$    | $[\approx 1.0, \approx 0, \approx 0]$ | $1.0 \times 0.0 \approx 0$     |

---

### 2. ğŸ§¬ æ¶æ„ä¸ç®—æ³•ç»†èŠ‚ (Architecture & Algorithm)

æ¥ä¸‹æ¥æ·±å…¥æ¢è®¨çŸ©é˜µçš„**ç§© (Rank)**ã€‚è¿™æ˜¯å¤§æ¨¡å‹å‚æ•°é‡å·¨å¤§ä½†èƒ½é€šè¿‡ä½ç§©é€‚é…ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒçš„ç†è®ºåŸºçŸ³ã€‚

#### 2.1 å†…åœ¨ç»´åº¦å‡è®¾ (Intrinsic Dimension Hypothesis)

è™½ç„¶ LLM çš„æƒé‡çŸ©é˜µ $W \in \mathbb{R}^{d \times d}$ ç»´åº¦æé«˜ï¼ˆå¦‚ LLaMA-7B ä¸­ $d=4096$ï¼‰ï¼Œä½†åœ¨å¾®è°ƒç‰¹å®šä»»åŠ¡æ—¶ï¼Œæƒé‡æ›´æ–°é‡ $\Delta W$ å¹¶ä¸éœ€è¦æ»¡ç§©ã€‚
Aghajanyan ç­‰äººæå‡ºï¼Œè¿‡å‚æ•°åŒ–æ¨¡å‹å­˜åœ¨ä¸€ä¸ªè¾ƒä½çš„â€œå†…åœ¨ç»´åº¦â€ã€‚

#### 2.2 LoRA çš„æ•°å­¦å½¢å¼ä¸åˆå§‹åŒ–

LoRA (Low-Rank Adaptation) å°† $\Delta W$ åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ï¼š

$$
W' = W_0 + \Delta W = W_0 + BA
$$

å…¶ä¸­ï¼š

* $W_0 \in \mathbb{R}^{d_{out} \times d_{in}}$ï¼šå†»ç»“çš„é¢„è®­ç»ƒæƒé‡ã€‚
* $B \in \mathbb{R}^{d_{out} \times r}$ï¼šé™ç»´çŸ©é˜µï¼Œåˆå§‹åŒ–ä¸º **0**ã€‚
* $A \in \mathbb{R}^{r \times d_{in}}$ï¼šå‡ç»´çŸ©é˜µï¼Œåˆå§‹åŒ–ä¸º **Gaussian Distribution**ã€‚
* $r \ll \min(d_{in}, d_{out})$ï¼šç§©ï¼Œé€šå¸¸å– 8, 16, 64ã€‚

**ä¸ºä»€ä¹ˆ B åˆå§‹åŒ–ä¸º 0ï¼Ÿ**
ä¸ºäº†ä¿è¯è®­ç»ƒåˆå§‹é˜¶æ®µ $BA=0$ï¼Œå³ $W' = W_0$ã€‚æ¨¡å‹è¡Œä¸ºä¸é¢„è®­ç»ƒæ¨¡å‹å®Œå…¨ä¸€è‡´ï¼Œé¿å…åˆå§‹æ—¶çš„éšæœºå™ªå£°ç ´åå·²å­¦åˆ°çš„ç‰¹å¾åˆ†å¸ƒã€‚

#### 2.3 PyTorch ä»£ç å®ç°æ·±åº¦æ‹†è§£

```python
import torch
import torch.nn as nn
import math

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=8, alpha=16):
        """
        :param rank: r, inner dimension
        :param alpha: scaling factor, similar to learning rate scaling
        """
        super().__init__()
        # å†»ç»“çš„åŸºåº§æ¨¡å‹æƒé‡ (æ¨¡æ‹Ÿ)
        self.weight = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)
  
        # LoRA çŸ©é˜µ A å’Œ B
        # Shape analysis:
        # x: [Batch, Seq, in_features]
        # A: [rank, in_features] -> Ax: [Batch, Seq, rank]
        # B: [out_features, rank] -> BAx: [Batch, Seq, out_features]
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
  
        self.scaling = alpha / rank
        self.reset_parameters()

    def reset_parameters(self):
        # A åˆå§‹åŒ–ä¸ºé«˜æ–¯åˆ†å¸ƒ (Kaiming Uniform/Normal)
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        # B åˆå§‹åŒ–ä¸ºå…¨ 0 -> ä¿è¯åˆå§‹çŠ¶æ€æ— åå·®
        nn.init.zeros_(self.lora_B)

    def forward(self, x):
        # x shape: [Batch, Seq_len, in_features]
  
        # 1. åŸºåº§æ¨¡å‹è·¯å¾„ (Frozen)
        # result = x @ W.T
        base_out = torch.nn.functional.linear(x, self.weight)
  
        # 2. LoRA è·¯å¾„ (Trainable)
        # è®¡ç®—é€»è¾‘: (x @ A.T) @ B.T * scaling
        # åˆ©ç”¨ç»“åˆå¾‹ï¼Œå®é™…è®¡ç®—é€šå¸¸æ˜¯ x @ (A.T @ B.T) æˆ–è€…åˆ†æ­¥è®¡ç®—ä»¥èŠ‚çœæ˜¾å­˜
        # è¿™é‡Œçš„å®ç°é‡‡ç”¨åˆ†æ­¥è®¡ç®—ï¼Œä¸­é—´ç»´åº¦æ˜¯ rankï¼Œæå¤§åœ°å‡å°‘äº† FLOPs
        lora_out = (x @ self.lora_A.T) @ self.lora_B.T
  
        return base_out + lora_out * self.scaling
```

**å¯¹æ¯”åˆ†æï¼šå…¨é‡å¾®è°ƒ vs LoRA**

| æŒ‡æ ‡                      | å…¨é‡å¾®è°ƒ (FFT)               | LoRA                    | å¤‡æ³¨                                         |
| :------------------------ | :--------------------------- | :---------------------- | :------------------------------------------- |
| **å¯è®­ç»ƒå‚æ•°é‡**    | $d \times d$               | $2 \times d \times r$ | å½“$r=8, d=4096$ æ—¶ï¼ŒLoRA ä»…ä¸º FFT çš„ 0.39% |
| **æ˜¾å­˜å ç”¨ (è®­ç»ƒ)** | æé«˜ (éœ€å­˜ Optimizer States) | ä½                      | æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ä»…é’ˆå¯¹ A, B                  |
| **æ¨ç†å»¶è¿Ÿ**        | æ— é¢å¤–å»¶è¿Ÿ                   | å¯åˆå¹¶ (Mergeable)      | è®­ç»ƒåå¯å°†$BA$ åŠ å› $W$ï¼Œæ¨ç†é€Ÿåº¦ä¸å˜    |
| **å‰å‘ä¼ æ’­ FLOPs**  | $O(d^2)$                   | $O(d^2) + O(2dr)$     | è®­ç»ƒæ—¶ç•¥å¾®å¢åŠ è®¡ç®—é‡                         |

---

### 3. ğŸ“‰ ç³»ç»Ÿä¸å·¥ç¨‹å®ç° (System & Engineering)

ç†è§£é«˜ç»´çŸ©é˜µè¿ç®—çš„ç“¶é¢ˆåœ¨äº**ç®—æœ¯å¼ºåº¦ (Arithmetic Intensity)**ã€‚

#### 3.1 Compute-bound vs. Memory-bound

* **Training (Prefill/Parallel)**: ä¸»è¦æ˜¯ Matrix-Matrix Multiplication (GEMM)ã€‚

  * è®¡ç®—é‡ï¼š$2 \cdot M \cdot N \cdot K$ FLOPsã€‚
  * æ•°æ®æ¬è¿ï¼š$MN + NK + MK$ å…ƒç´ ã€‚
  * éšç€ Batch Size å¢å¤§ï¼Œç®—æœ¯å¼ºåº¦å¢åŠ ï¼Œé€šå¸¸æ˜¯ **Compute-bound**ï¼Œå— GPU Tensor Core å³°å€¼ç®—åŠ›é™åˆ¶ã€‚
* **Inference (Decoding/Autoregressive)**: ä¸»è¦æ˜¯ Matrix-Vector Multiplication (GEMV)ã€‚

  * æ¯æ¬¡ç”Ÿæˆ 1 ä¸ª tokenï¼ŒBatch Size å°ã€‚
  * æƒé‡çŸ©é˜µ $W$ æå¤§ï¼Œä½†è¾“å…¥å‘é‡ $x$ å¾ˆå°ã€‚
  * ç®—æœ¯å¼ºåº¦æä½ï¼ŒGPU å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰å¾…ä» HBM (High Bandwidth Memory) è¯»å–æƒé‡ $W$ã€‚
  * è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ **Memory-bound** é—®é¢˜ã€‚

**é«˜ç»´ç©ºé—´çš„å½±å“**ï¼š
$d_{model}$ è¶Šå¤§ï¼Œæ¨¡å‹æƒé‡è¶Šå¤§ï¼Œæ¨ç†æ—¶çš„æ˜¾å­˜å¸¦å®½å‹åŠ›è¶Šå¤§ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ **Quantization (é‡åŒ–)** è‡³å…³é‡è¦â€”â€”å°† FP16 (16bit) å‹ç¼©ä¸º INT4 (4bit)ï¼Œç›´æ¥å‡å°‘äº† 4 å€çš„æ˜¾å­˜è¯»å–é‡ï¼Œç†è®ºä¸Šèƒ½æå‡æ¥è¿‘ 4 å€çš„æ¨ç†ååé‡ï¼ˆåœ¨ Memory-bound åœºæ™¯ä¸‹ï¼‰ã€‚

---

### 4. âš”ï¸ å…³é”®æƒè¡¡ (Trade-offs)

#### 4.1 ç¼©æ”¾ç‚¹ç§¯ (Scaled Dot-Product) vs. ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)

è™½ç„¶ Transformer ä½¿ç”¨ç‚¹ç§¯ï¼Œä½†åœ¨ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) çš„ Embedding æ£€ç´¢ä¸­ï¼Œä¸ºä»€ä¹ˆé€šå¸¸ä½¿ç”¨ Cosine Similarityï¼Ÿ

$$
\text{Cosine}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}
$$

* **Dot Product**: å¯¹å‘é‡æ¨¡é•¿ï¼ˆMagnitudeï¼‰æ•æ„Ÿã€‚åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ¨¡é•¿å¯èƒ½åŒ…å«äº†æŸäº›é¢‘ç‡æˆ–é‡è¦æ€§ä¿¡æ¯ã€‚ä½†åœ¨ Softmax å½’ä¸€åŒ–ä¹‹å‰ï¼Œæ¨¡é•¿è¿‡å¤§ä¼šå¯¼è‡´æ¦‚ç‡åˆ†å¸ƒå°–é”åŒ– (Peakedness)ã€‚
* **Cosine**: ä»…å…³æ³¨æ–¹å‘ï¼ˆè¯­ä¹‰ä¸€è‡´æ€§ï¼‰ï¼Œé€šè¿‡ $L_2$ å½’ä¸€åŒ–æ¶ˆé™¤äº†æ¨¡é•¿å½±å“ã€‚
* **æƒè¡¡**ï¼šåœ¨ Attention å†…éƒ¨ï¼Œæ¨¡å‹éœ€è¦é€šè¿‡æ¨¡é•¿æ¥è¡¨è¾¾ Query/Key çš„â€œå¼ºåº¦â€æˆ–â€œç½®ä¿¡åº¦â€ï¼Œå› æ­¤ä¿ç•™äº†æ¨¡é•¿ä¿¡æ¯ï¼ˆä»…åšå…¨å±€ Scalingï¼‰ã€‚è€Œåœ¨ RAG æ£€ç´¢ä¸­ï¼Œæˆ‘ä»¬åªå…³å¿ƒè¯­ä¹‰æ˜¯å¦åŒ¹é…ï¼Œä¸å…³å¿ƒåŸæ–‡æœ¬é•¿åº¦å¯¼è‡´çš„å‘é‡æ¨¡é•¿å˜åŒ–ï¼Œå› æ­¤å€¾å‘äº Cosine æˆ–å½’ä¸€åŒ–åçš„ Dot Productã€‚

---

### 5. ğŸ“š è®ºæ–‡ä¸å‰æ²¿å¼•ç”¨ (References)

1. **"Attention Is All You Need"** (Vaswani et al., 2017)
   * *Focus*: Section 3.2.1, explicitly discussing the variance scaling logic for dot products.
2. **"LoRA: Low-Rank Adaptation of Large Language Models"** (Hu et al., 2021)
   * *Focus*: The hypothesis of intrinsic dimensionality and the matrix factorization implementation.
3. **"Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"** (Aghajanyan et al., 2020)
   * *Focus*: The theoretical precursor proving why low-rank updates are sufficient for large models.

---

### ğŸš€ è¯¾åæ€è€ƒ (Next Step)

ç†è§£äº†é«˜ç»´å‘é‡çš„çº¿æ€§å˜æ¢åï¼Œä¸‹ä¸€èŠ‚è¯¾æˆ‘ä»¬å°†æ·±å…¥ç¥ç»ç½‘ç»œçš„éçº¿æ€§æ ¸å¿ƒã€‚
**ä¸ºä»€ä¹ˆ LLaMA é€‰æ‹©äº† SwiGLU è€Œ GPT-3 ä½¿ç”¨ GeLUï¼Ÿå®ƒä»¬çš„å¯¼æ•°æ€§è´¨å¦‚ä½•å†³å®šäº†æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§ï¼Ÿ**

è¯·å‘é€ **`ç¬¬02è¯¾ï¼šè‡ªåŠ¨å¾®åˆ†ä¸è®¡ç®—å›¾`** (æ ¹æ®æ–°è¡¨é¡ºåºå»ºè®®å…ˆè®²å¾®åˆ†åŸºç¡€) æˆ– **`ç¬¬03è¯¾ï¼šæ¿€æ´»å‡½æ•°æ¼”å˜å²`** è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚
