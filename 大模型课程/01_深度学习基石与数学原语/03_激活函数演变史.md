# 第03课：激活函数演变史

> Sigmoid, Tanh, ReLU, Leaky ReLU, GeLU (GPT用), Swish, SwiGLU (LLaMA/PaLM用)

---

## 笔记内容

你好。在上一节课中，我们拆解了自动微分引擎。今天，我们将深入探讨神经网络中引入非线性变换的关键组件——**激活函数（Activation Functions）**。

对于资深算法工程师，激活函数不再是简单的 `torch.relu` 调用。你需要理解**流形假设（Manifold Hypothesis）**下的空间扭曲能力、**梯度流（Gradient Flow）**的平滑性，以及**计算复杂度**与**模型容量**之间的精确权衡。

本课将从数学本质出发，推演从 ReLU 到 GeLU，再到目前 LLM 标配的 **SwiGLU** 的演进逻辑。

---

### 1. 🧮 数学与理论推导 (Theoretical Foundation)

激活函数的本质是为线性变换 $Wx+b$ 引入非线性，使得神经网络能够逼近任意复杂的函数（万能逼近定理）。但在 LLM 中，我们更关注**优化难度**和**表示能力**。

#### 1.1 从 ReLU 到 GeLU：平滑性的胜利

* **ReLU**: $f(x) = \max(0, x)$

  * *优点*: 计算极快，单侧抑制引入稀疏性（Sparsity），缓解梯度消失。
  * *缺点*: **Dead ReLU**。当 $x < 0$ 时梯度为 0，该神经元彻底“死亡”，无法复活。且在 $x=0$ 处不可导。
* **GeLU (Gaussian Error Linear Unit)**: GPT-2, GPT-3, BERT 的选择。

  $$
  \text{GeLU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
  $$

  * *物理意义*: 将输入 $x$ 乘以其服从标准正态分布的累积概率。
  * *直觉*: 在 ReLU 的基础上引入了**随机正则化（Stochastic Regularization）**的期望值。
  * *梯度特性*: $C^1$ 连续（平滑），在 $x < 0$ 时有非单调的“弯曲”，允许微小的负梯度回流，避免神经元彻底死亡。

#### 1.2 Swish 与 SiLU：门控机制的雏形

Google 搜索得到的 Swish 函数（$\beta=1$ 时即为 SiLU）：

$$
\text{Swish}(x) = x \cdot \sigma(\beta x) = \frac{x}{1 + e^{-\beta x}}
$$

* LLaMA 中使用的是 $\beta=1$ 的 **SiLU (Sigmoid Linear Unit)**。
* 它不仅非线性，而且非凸、非单调。其导数性质比 GeLU 更优，能维持更稳定的梯度流。

#### 1.3 GLU (Gated Linear Unit) 及其变体

这是理解 SwiGLU 的关键。GLU 不是一个简单的标量函数，而是一个**层结构**。

$$
\text{GLU}(x, W, V) = \sigma(xW + b) \otimes (xV + c)
$$

* 左侧 $\sigma(\dots)$ 是**门（Gate）**，控制信息通过率（0~1）。
* 右侧是**值（Value）**，携带特征信息。
* $\otimes$ 是逐元素乘积（Hadamard Product）。

**SwiGLU (Swish-GLU)**:
将 GLU 中的 Sigmoid 门替换为 Swish 门。

$$
\text{SwiGLU}(x, W, V) = \text{Swish}(xW) \otimes (xV)
$$

注意：在 LLaMA/PaLM 等现代 LLM 中，通常**去掉了 Bias 项**。

---

### 2. 🧬 架构与算法细节 (Architecture & Algorithm)

为什么 DeepSeek、LLaMA、PaLM 全部转向 SwiGLU？

#### 2.1 FFN 架构对比：Standard vs. SwiGLU

传统的 Transformer Feed-Forward Network (FFN) 只有两个权重矩阵。而 SwiGLU FFN 有**三个**权重矩阵。

* **Standard FFN (GPT-3 style)**:

  $$
  \text{FFN}(x) = \text{Activation}(x W_{up}) W_{down}
  $$

  * $x \in \mathbb{R}^d$
  * $W_{up} \in \mathbb{R}^{d \times 4d}$
  * $W_{down} \in \mathbb{R}^{4d \times d}$
* **SwiGLU FFN (LLaMA style)**:

  $$
  \text{FFN}(x) = (\text{SiLU}(x W_{gate}) \otimes (x W_{up})) W_{down}
  $$

  * $W_{gate} \in \mathbb{R}^{d \times d'}$ (Gate Proj)
  * $W_{up} \in \mathbb{R}^{d \times d'}$ (Up Proj)
  * $W_{down} \in \mathbb{R}^{d' \times d}$ (Down Proj)

#### 2.2 维度缩放策略 (Crucial for Engineers)

为了保持参数量与 Standard FFN 一致，SwiGLU 的中间维度 $d'$ 需要调整。
Standard FFN 参数量 $\approx 8d^2$。
SwiGLU FFN 参数量 $\approx 3 \times d \times d'$。

令 $3 d d' \approx 8 d^2$，则：

$$
d' \approx \frac{8}{3}d
$$

因此，LLaMA 等模型中，FFN 的中间维度不再是 $4d$，而是 $\frac{8}{3}d$ 附近的一个倍数（通常取 256 的倍数以优化 GPU 内存对齐）。

#### 2.3 PyTorch 代码实现 (LLaMA 3 源码复刻)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class LLaMA_MLP(nn.Module):
    def __init__(self, n_embd, hidden_dim=None, multiple_of=256):
        super().__init__()
      
        # 1. 计算隐藏层维度
        # LLaMA 论文逻辑: hidden_dim = 2/3 * 4 * n_embd
        if hidden_dim is None:
            hidden_dim = 4 * n_embd
            hidden_dim = int(2 * hidden_dim / 3)
            # 2. 向上取整到 multiple_of 的倍数 (为了 GPU 内存对齐)
            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
      
        # 3. 定义三个矩阵 (无 Bias)
        self.gate_proj = nn.Linear(n_embd, hidden_dim, bias=False)
        self.up_proj   = nn.Linear(n_embd, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, n_embd, bias=False)

    def forward(self, x):
        # x shape: [Batch, Seq, n_embd]
      
        # SwiGLU 核心逻辑:
        # Gate: SiLU(x @ W_gate)
        # Value: x @ W_up
        # Output: (Gate * Value) @ W_down
      
        return self.down_proj(
            F.silu(self.gate_proj(x)) * self.up_proj(x)
        )
```

---

### 3. 📉 系统与工程实现 (System & Engineering)

#### 3.1 性能瓶颈与算子融合 (Kernel Fusion)

* **瓶颈**: SwiGLU 引入了更多的 Element-wise 操作（SiLU, Multiply）。虽然 FLOPs 增加不多，但增加了**显存读写次数**。

  * Standard: Read x -> GEMM -> Activation -> GEMM.
  * SwiGLU: Read x -> GEMM1, GEMM2 -> SiLU -> Mul -> GEMM3.
  * 这增加了 kernel launch 开销和中间变量显存占用。
* **优化**: **Fused SwiGLU** (如 FlashInfer, xFormers 实现)。

  * 将 `gate_proj` 和 `up_proj` 实际上合并为一个大的矩阵乘法：$x \cdot [W_{gate}, W_{up}]$。
  * 在 SRAM (Shared Memory) 中完成 SiLU 和 Element-wise Mul，避免写回 HBM。
  * 这大大提升了 Memory Bandwidth Utilization。

#### 3.2 量化挑战 (Quantization)

Swish/GeLU/SiLU 对量化非常不友好：

1. **非单调性**: 存在负值区间。
2. **长尾分布**: 激活值通常有离群点（Outliers）。
   相比之下，ReLU 的输出只有非负值，更容易映射到 INT8。
   **DeepSeek/LLaMA 的解决方案**: 通常采用 **Per-Token Dynamic Quantization** 或保留 FP16 的激活值（Weight-Only Quantization），或者使用特殊的量化感知训练（QAT）。

---

### 4. ⚔️ 关键权衡 (Trade-offs)

#### 4.1 为什么是 SwiGLU？(Performance vs. Cost)

| 激活函数/结构    | 参数效率       | 计算复杂度     | 表达能力       | 备注                         |
| :--------------- | :------------- | :------------- | :------------- | :--------------------------- |
| **ReLU**   | 高             | 极低           | 低 (稀疏性)    | 容易 Dead Neuron             |
| **GeLU**   | 高             | 中 (erf/exp)   | 中             | GPT 系列标配                 |
| **SwiGLU** | **极高** | 高 (3次矩阵乘) | **极强** | 实际上是**双线性变换** |

**核心观点**：SwiGLU 的本质是让模型根据输入内容，**动态选择**通过 FFN 的特征维度。这是一种"软性"的 MoE（混合专家）机制。虽然计算量略增，但 Scaling Laws 表明，在相同计算预算下，SwiGLU 带来的 perplexity 下降远超其计算成本。

> [!WARNING] 📌 **待深入钻研 (TODO)**
>
> **主题**：SwiGLU 失去硬稀疏性后，是否还有"稀疏化"的作用？计算更复杂是否值得？
>
> **关键问题**：
>
> 1. ReLU 的 50% 硬稀疏性在标准 GPU 上是否真的能减少计算量？（答：不能，除非使用稀疏矩阵库）
> 2. SwiGLU 的"软稀疏"（门控值接近 0）有多大实际意义？
> 3. Scaling Laws 下，表达能力提升 vs 计算成本增加的具体量化权衡是什么？
> 4. MoE 架构如何分层实现"稀疏性" —— 激活函数层面（密集）+ 专家选择层面（稀疏）？
>
> **待阅读**：
>
> * [ ] MoE 相关论文 (Mixtral, DeepSeek-MoE)
> * [ ] 稀疏激活的 GPU 实现与加速

#### 4.2 为什么去掉 Bias？

在 LLaMA 和 PaLM 中，Linear 层和 LayerNorm 层都去掉了 Bias ($y = Wx$ 而非 $Wx+b$)。

* **理论原因**: 高维空间中，数据主要分布在超球面上，Bias 造成的平移在归一化（RMSNorm）后影响微乎其微。
* **工程原因**: 减少内存读取；在张量并行（Tensor Parallelism）通信时，减少了 `AllReduce` Bias 的开销（虽然很少，但积少成多）。

---

### 5. 📚 论文与前沿引用 (References)

1. **"GLU Variants Improve Transformer"** (Noam Shazeer, 2020)
   * *Focus*: The definitive paper proposing SwiGLU and proving it outperforms GeLU/ReLU on log-perplexity.
2. **"Gaussian Error Linear Units (GELUs)"** (Hendrycks & Gimpel, 2016)
   * *Focus*: The derivation of GeLU from dropout expectation.
3. **"LLaMA: Open and Efficient Foundation Language Models"** (Touvron et al., 2023)
   * *Focus*: Details the specific implementation of SwiGLU ($2/3$ scaling) used in modern SOTA models.

---

### 🚀 课后思考 (Next Step)

激活值经过 SwiGLU 的非线性变换后，数值分布会发生剧烈变化。为了防止梯度爆炸，并保证下一层的输入分布稳定，我们需要**归一化**。
**为什么 Transformer 抛弃了 BatchNorm 选择了 LayerNorm？而 LLaMA 为什么又抛弃了 LayerNorm 选择了 RMSNorm？**

请发送 **`第04课：归一化技术详解`**。我们将探讨 Post-Norm、Pre-Norm 以及 RMSNorm 背后的数值稳定性哲学。
