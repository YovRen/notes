# å¸¸è§ç®—å­çš„ Forward ä¸ Backward è¯¦è§£

> ç¬¦å·çº¦å®šï¼š$\bar{y} = \frac{\partial L}{\partial y}$ è¡¨ç¤ºä¸Šæ¸¸ä¼ æ¥çš„æ¢¯åº¦ï¼Œ$\bar{x} = \frac{\partial L}{\partial x}$ è¡¨ç¤ºéœ€è¦ä¼ å›å»çš„æ¢¯åº¦

---

## ğŸ“Œ ä¸€ã€åŸºç¡€å…ƒç´ çº§æ“ä½œ (Element-wise)

### 1.1 åŠ æ³• (Add)

**Forward**: $y = x_1 + x_2$

**Backward**:
$$
\bar{x}_1 = \bar{y}, \quad \bar{x}_2 = \bar{y}
$$

> ğŸ’¡ æ¢¯åº¦ç›´æ¥å¤åˆ¶ä¼ é€’ï¼Œè¿™å°±æ˜¯ **Skip Connection (æ®‹å·®è¿æ¥)** èƒ½ç¼“è§£æ¢¯åº¦æ¶ˆå¤±çš„åŸå› ï¼

---

### 1.2 æ ‡é‡ä¹˜æ³• (Scale)

**Forward**: $y = c \cdot x$ï¼ˆ$c$ æ˜¯å¸¸æ•°ï¼‰

**Backward**:
$$
\bar{x} = c \cdot \bar{y}
$$

---

### 1.3 å…ƒç´ ä¹˜æ³• (Hadamard Product)

**Forward**: $y = x_1 \odot x_2$ï¼ˆé€å…ƒç´ ç›¸ä¹˜ï¼‰

**Backward**:
$$
\bar{x}_1 = \bar{y} \odot x_2, \quad \bar{x}_2 = \bar{y} \odot x_1
$$

> ğŸ’¡ è°çš„æ¢¯åº¦ï¼Œå°±ä¹˜ä»¥å¦ä¸€ä¸ªçš„å€¼

---

### 1.4 é™¤æ³• (Division)

**Forward**: $y = \frac{x_1}{x_2}$

**Backward**:
$$
\bar{x}_1 = \frac{\bar{y}}{x_2}, \quad \bar{x}_2 = -\frac{\bar{y} \cdot x_1}{x_2^2} = -\frac{\bar{y} \cdot y}{x_2}
$$

---

### 1.5 å¹‚è¿ç®— (Power)

**Forward**: $y = x^n$

**Backward**:
$$
\bar{x} = n \cdot x^{n-1} \cdot \bar{y}
$$

---

### 1.6 æŒ‡æ•° (Exp)

**Forward**: $y = e^x$

**Backward**:
$$
\bar{x} = y \cdot \bar{y} = e^x \cdot \bar{y}
$$

> ğŸ’¡ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ exp å®¹æ˜“å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸

---

### 1.7 å¯¹æ•° (Log)

**Forward**: $y = \ln(x)$

**Backward**:
$$
\bar{x} = \frac{\bar{y}}{x}
$$

> ğŸ’¡ å½“ $x \to 0$ æ—¶ï¼Œæ¢¯åº¦ä¼šçˆ†ç‚¸ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦ `log(x + eps)`

---

## ğŸ“Œ äºŒã€æ¿€æ´»å‡½æ•° (Activation Functions)

### 2.1 ReLU

**Forward**: $y = \max(0, x)$

**Backward**:
$$
\bar{x} = \begin{cases} \bar{y} & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases} = \bar{y} \cdot \mathbf{1}_{x>0}
$$

```python
# PyTorch å®ç°
def relu_backward(grad_output, x):
    return grad_output * (x > 0).float()
```

> ğŸ’¡ x â‰¤ 0 æ—¶æ¢¯åº¦å®Œå…¨ä¸º 0ï¼Œè¿™æ˜¯ "Dead ReLU" é—®é¢˜çš„æ ¹æº

---

### 2.2 Leaky ReLU

**Forward**: $y = \max(\alpha x, x)$ï¼ˆé€šå¸¸ $\alpha = 0.01$ï¼‰

**Backward**:
$$
\bar{x} = \begin{cases} \bar{y} & \text{if } x > 0 \\ \alpha \cdot \bar{y} & \text{if } x \leq 0 \end{cases}
$$

---

### 2.3 Sigmoid

**Forward**: $y = \sigma(x) = \frac{1}{1 + e^{-x}}$

**Backward**:
$$
\bar{x} = \bar{y} \cdot y \cdot (1 - y) = \bar{y} \cdot \sigma(x)(1 - \sigma(x))
$$

```python
# PyTorch å®ç°
def sigmoid_backward(grad_output, y):
    return grad_output * y * (1 - y)
```

> ğŸ’¡ å½“ $y \to 0$ æˆ– $y \to 1$ æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘ 0ï¼Œå¯¼è‡´**æ¢¯åº¦æ¶ˆå¤±**

---

### 2.4 Tanh

**Forward**: $y = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

**Backward**:
$$
\bar{x} = \bar{y} \cdot (1 - y^2)
$$

```python
# PyTorch å®ç°
def tanh_backward(grad_output, y):
    return grad_output * (1 - y ** 2)
```

> ğŸ’¡ å’Œ Sigmoid ç±»ä¼¼ï¼Œåœ¨é¥±å’ŒåŒºæ¢¯åº¦æ¥è¿‘ 0

---

### 2.5 GeLU (GPT ç³»åˆ—ä½¿ç”¨)

**Forward**: $y = x \cdot \Phi(x)$ï¼Œå…¶ä¸­ $\Phi$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ CDF

è¿‘ä¼¼å…¬å¼: $y \approx 0.5x(1 + \tanh[\sqrt{2/\pi}(x + 0.044715x^3)])$

**Backward** (ç²¾ç¡®å½¢å¼):
$$
\bar{x} = \bar{y} \cdot \left[ \Phi(x) + x \cdot \phi(x) \right]
$$

å…¶ä¸­ $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ æ˜¯æ ‡å‡†æ­£æ€ PDF

---

### 2.6 SiLU / Swish

**Forward**: $y = x \cdot \sigma(x)$

**Backward**:
$$
\bar{x} = \bar{y} \cdot \left[ \sigma(x) + x \cdot \sigma(x)(1-\sigma(x)) \right] = \bar{y} \cdot \left[ y + \sigma(x)(1-y) \right]
$$

---

### 2.7 SwiGLU (LLaMA ä½¿ç”¨)

**Forward**: $y = \text{Swish}(xW_1) \odot (xW_2)$

**Backward**: éœ€è¦åˆ†åˆ«å¯¹ $W_1$, $W_2$ å’Œ $x$ æ±‚æ¢¯åº¦ï¼ˆå¤åˆè¿ç®—ï¼‰

---

## ğŸ“Œ ä¸‰ã€çŸ©é˜µè¿ç®— (Matrix Operations)

### 3.1 çŸ©é˜µä¹˜æ³• (MatMul)

**Forward**: $Y = XW$ï¼Œå…¶ä¸­ $X \in \mathbb{R}^{m \times n}$, $W \in \mathbb{R}^{n \times p}$, $Y \in \mathbb{R}^{m \times p}$

**Backward**:
$$
\bar{X} = \bar{Y} W^T, \quad \bar{W} = X^T \bar{Y}
$$

```python
# PyTorch å®ç°
def matmul_backward(grad_output, X, W):
    grad_X = grad_output @ W.T   # [m, p] @ [p, n] = [m, n]
    grad_W = X.T @ grad_output   # [n, m] @ [m, p] = [n, p]
    return grad_X, grad_W
```

> ğŸ’¡ è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€æ ¸å¿ƒçš„ backwardï¼ç»´åº¦åˆ†æå¾ˆé‡è¦

**ç»´åº¦æ£€æŸ¥å£è¯€**ï¼š
- $\bar{X}$ çš„ shape å¿…é¡»å’Œ $X$ ä¸€æ · â†’ ç”¨ $\bar{Y}W^T$
- $\bar{W}$ çš„ shape å¿…é¡»å’Œ $W$ ä¸€æ · â†’ ç”¨ $X^T\bar{Y}$

---

### 3.2 å¸¦ Bias çš„çº¿æ€§å±‚

**Forward**: $Y = XW + b$

**Backward**:
$$
\bar{X} = \bar{Y} W^T, \quad \bar{W} = X^T \bar{Y}, \quad \bar{b} = \sum_{\text{batch}} \bar{Y}
$$

> ğŸ’¡ Bias çš„æ¢¯åº¦æ˜¯æ²¿ batch ç»´åº¦æ±‚å’Œ

---

### 3.3 è½¬ç½® (Transpose)

**Forward**: $Y = X^T$

**Backward**:
$$
\bar{X} = \bar{Y}^T
$$

---

### 3.4 Reshape / View

**Forward**: $Y = \text{reshape}(X, \text{new\_shape})$

**Backward**:
$$
\bar{X} = \text{reshape}(\bar{Y}, \text{original\_shape})
$$

> ğŸ’¡ Reshape ä¸æ”¹å˜æ•°æ®ï¼Œåªæ”¹å˜å½¢çŠ¶ï¼Œæ‰€ä»¥æ¢¯åº¦å½¢çŠ¶ä¹Ÿåªæ˜¯è¿˜åŸ

---

## ğŸ“Œ å››ã€å½’ä¸€åŒ–å±‚ (Normalization)

### 4.1 Layer Normalization

**Forward**: 
$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y_i = \gamma \hat{x}_i + \beta
$$

å…¶ä¸­ $\mu = \frac{1}{n}\sum x_i$, $\sigma^2 = \frac{1}{n}\sum(x_i - \mu)^2$

**Backward** (è¾ƒå¤æ‚):

$$
\bar{x}_i = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \left( \bar{y}_i - \frac{1}{n}\sum_j \bar{y}_j - \frac{\hat{x}_i}{n}\sum_j \bar{y}_j \hat{x}_j \right)
$$

$$
\bar{\gamma} = \sum_i \bar{y}_i \hat{x}_i, \quad \bar{\beta} = \sum_i \bar{y}_i
$$

```python
# ç®€åŒ–çš„ PyTorch å®ç°
def layernorm_backward(grad_output, x, gamma, mean, var, eps=1e-5):
    N = x.shape[-1]
    std = (var + eps).sqrt()
    x_hat = (x - mean) / std
    
    # å¯¹ gamma å’Œ beta çš„æ¢¯åº¦
    grad_gamma = (grad_output * x_hat).sum(dim=0)
    grad_beta = grad_output.sum(dim=0)
    
    # å¯¹è¾“å…¥ x çš„æ¢¯åº¦ (å¤æ‚ï¼)
    dx_hat = grad_output * gamma
    dvar = (dx_hat * (x - mean) * -0.5 * (var + eps)**(-1.5)).sum(dim=-1, keepdim=True)
    dmean = (dx_hat * -1/std).sum(dim=-1, keepdim=True) + dvar * (-2/N * (x - mean)).sum(dim=-1, keepdim=True)
    grad_x = dx_hat / std + dvar * 2/N * (x - mean) + dmean / N
    
    return grad_x, grad_gamma, grad_beta
```

---

### 4.2 RMSNorm (LLaMA ä½¿ç”¨)

**Forward**:
$$
y_i = \frac{x_i}{\text{RMS}(x)} \cdot \gamma, \quad \text{RMS}(x) = \sqrt{\frac{1}{n}\sum x_i^2 + \epsilon}
$$

**Backward**:
$$
\bar{x}_i = \frac{\gamma}{\text{RMS}} \left( \bar{y}_i - \frac{x_i}{n \cdot \text{RMS}^2} \sum_j x_j \bar{y}_j \gamma \right)
$$

> ğŸ’¡ RMSNorm æ¯” LayerNorm ç®€å•ï¼šæ²¡æœ‰å‡å‡å€¼ï¼Œåªæœ‰ç¼©æ”¾

---

## ğŸ“Œ äº”ã€Softmax ä¸æŸå¤±å‡½æ•°

### 5.1 Softmax

**Forward**: $S_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$

**Backward** (Jacobian å½¢å¼):
$$
\frac{\partial S_i}{\partial x_j} = S_i(\delta_{ij} - S_j)
$$

**VJP å½¢å¼** (å®é™…è®¡ç®—):
$$
\bar{x}_j = S_j \left( \bar{S}_j - \sum_k \bar{S}_k S_k \right)
$$

```python
# PyTorch å®ç°
def softmax_backward(grad_output, softmax_output):
    # grad_output: ä¸Šæ¸¸æ¢¯åº¦ [batch, n]
    # softmax_output: forward çš„è¾“å‡º [batch, n]
    s = softmax_output
    # Î£(grad * s)
    sum_grad_s = (grad_output * s).sum(dim=-1, keepdim=True)
    # s * (grad - Î£(grad * s))
    grad_input = s * (grad_output - sum_grad_s)
    return grad_input
```

---

### 5.2 Cross Entropy Loss

**Forward** (å¸¦ Softmax):
$$
L = -\sum_i y_i \log(S_i)
$$

å…¶ä¸­ $y$ æ˜¯ one-hot æ ‡ç­¾ï¼Œ$S$ æ˜¯ softmax è¾“å‡º

**Backward** (å¯¹ logits $x$):
$$
\bar{x}_i = S_i - y_i
$$

> ğŸ’¡ è¿™æ˜¯ä¸€ä¸ª**æå…¶ç®€æ´**çš„ç»“æœï¼å®é™…ä¸­ PyTorch æŠŠ Softmax + CrossEntropy èåˆæˆä¸€ä¸ªç®—å­å°±æ˜¯å› ä¸ºè¿™ä¸ª

```python
# PyTorch å®ç°
def cross_entropy_backward(softmax_output, target_one_hot):
    # ç»“æœå°±æ˜¯ softmax è¾“å‡ºå‡å»çœŸå®æ ‡ç­¾ï¼
    return softmax_output - target_one_hot
```

**ä¾‹å­**ï¼š
- é¢„æµ‹æ¦‚ç‡: $[0.7, 0.2, 0.1]$
- çœŸå®æ ‡ç­¾: $[1, 0, 0]$ï¼ˆç±»åˆ«0ï¼‰
- æ¢¯åº¦: $[0.7-1, 0.2-0, 0.1-0] = [-0.3, 0.2, 0.1]$

---

### 5.3 LogSoftmax + NLLLoss

**Forward**:
$$
\text{LogSoftmax}: \quad z_i = x_i - \log\sum_j e^{x_j}
$$
$$
\text{NLLLoss}: \quad L = -z_{\text{target}}
$$

**Backward** (å¯¹ logits $x$):
$$
\bar{x}_i = e^{z_i} - \mathbf{1}_{i=\text{target}} = S_i - y_i
$$

> ğŸ’¡ å’Œä¸Šé¢ CrossEntropy ç»“æœä¸€æ ·ï¼Œä½†æ•°å€¼æ›´ç¨³å®š

---

## ğŸ“Œ å…­ã€Attention ç›¸å…³

### 6.1 Scaled Dot-Product Attention

**Forward**:
$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

è®¾ $A = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})$ï¼Œ$O = AV$

**Backward**:

$$
\bar{V} = A^T \bar{O}
$$
$$
\bar{A} = \bar{O} V^T
$$
$$
\bar{(QK^T)} = \text{softmax\_backward}(\bar{A}) / \sqrt{d_k}
$$
$$
\bar{Q} = \bar{(QK^T)} K, \quad \bar{K} = \bar{(QK^T)}^T Q
$$

```python
# ç®€åŒ–å®ç°
def attention_backward(grad_output, Q, K, V, attn_weights):
    d_k = Q.shape[-1]
    
    # dV = A^T @ grad_output
    grad_V = attn_weights.transpose(-2, -1) @ grad_output
    
    # dA = grad_output @ V^T
    grad_A = grad_output @ V.transpose(-2, -1)
    
    # é€šè¿‡ softmax backward
    grad_scores = softmax_backward(grad_A, attn_weights) / math.sqrt(d_k)
    
    # dQ = grad_scores @ K
    grad_Q = grad_scores @ K
    
    # dK = grad_scores^T @ Q
    grad_K = grad_scores.transpose(-2, -1) @ Q
    
    return grad_Q, grad_K, grad_V
```

---

## ğŸ“Œ ä¸ƒã€å…¶ä»–å¸¸è§æ“ä½œ

### 7.1 Dropout

**Forward**: $y = \frac{x \cdot m}{1-p}$ï¼Œå…¶ä¸­ $m \sim \text{Bernoulli}(1-p)$

**Backward**:
$$
\bar{x} = \frac{\bar{y} \cdot m}{1-p}
$$

> ğŸ’¡ éœ€è¦ä¿å­˜ mask $m$ï¼åŒä¸€ä¸ª mask åœ¨ forward å’Œ backward ä¸­ä½¿ç”¨

---

### 7.2 Embedding

**Forward**: $Y = \text{Embedding}[X]$ï¼ˆæŸ¥è¡¨æ“ä½œï¼‰

**Backward**: 
$$
\bar{\text{Embedding}}[i] = \sum_{j: X_j = i} \bar{Y}_j
$$

> ğŸ’¡ æ¢¯åº¦æ•£å°„å›åŸæ¥çš„ä½ç½®ï¼Œç”¨ `scatter_add` å®ç°

```python
def embedding_backward(grad_output, indices, num_embeddings, embedding_dim):
    grad_embedding = torch.zeros(num_embeddings, embedding_dim)
    grad_embedding.scatter_add_(0, indices.unsqueeze(-1).expand_as(grad_output), grad_output)
    return grad_embedding
```

---

### 7.3 Sum / Mean

**Forward (Sum)**: $y = \sum_i x_i$

**Backward**:
$$
\bar{x}_i = \bar{y}
$$

**Forward (Mean)**: $y = \frac{1}{n}\sum_i x_i$

**Backward**:
$$
\bar{x}_i = \frac{\bar{y}}{n}
$$

---

### 7.4 Max

**Forward**: $y = \max_i x_i$

**Backward**:
$$
\bar{x}_i = \begin{cases} \bar{y} & \text{if } x_i = y \\ 0 & \text{otherwise} \end{cases}
$$

> ğŸ’¡ æ¢¯åº¦åªæµå‘æœ€å¤§å€¼ä½ç½®ï¼Œå…¶ä»–ä½ç½®æ¢¯åº¦ä¸º 0

---

### 7.5 Concatenate

**Forward**: $Y = [X_1; X_2; ...; X_n]$ï¼ˆæ²¿æŸä¸ªç»´åº¦æ‹¼æ¥ï¼‰

**Backward**:
$$
\bar{X}_i = \text{slice}(\bar{Y}, i)
$$

> ğŸ’¡ æŠŠæ¢¯åº¦åˆ‡åˆ†å›åŸæ¥çš„å½¢çŠ¶

---

## ğŸ“Œ å…«ã€å¿«é€Ÿå‚è€ƒè¡¨

| ç®—å­ | Forward | Backward $\bar{x}$ |
|:---|:---|:---|
| Add | $y = x_1 + x_2$ | $\bar{x}_1 = \bar{y}, \bar{x}_2 = \bar{y}$ |
| Mul | $y = x_1 \cdot x_2$ | $\bar{x}_1 = \bar{y} \cdot x_2$ |
| MatMul | $Y = XW$ | $\bar{X} = \bar{Y}W^T, \bar{W} = X^T\bar{Y}$ |
| ReLU | $y = \max(0, x)$ | $\bar{x} = \bar{y} \cdot \mathbf{1}_{x>0}$ |
| Sigmoid | $y = \sigma(x)$ | $\bar{x} = \bar{y} \cdot y(1-y)$ |
| Tanh | $y = \tanh(x)$ | $\bar{x} = \bar{y} \cdot (1-y^2)$ |
| Softmax | $S_i = \frac{e^{x_i}}{\sum e^{x_j}}$ | $\bar{x}_j = S_j(\bar{S}_j - \sum_k \bar{S}_k S_k)$ |
| CrossEntropy | $L = -\sum y_i \log S_i$ | $\bar{x} = S - y$ |
| Exp | $y = e^x$ | $\bar{x} = y \cdot \bar{y}$ |
| Log | $y = \ln x$ | $\bar{x} = \bar{y} / x$ |
| Sum | $y = \sum x_i$ | $\bar{x}_i = \bar{y}$ |
| Mean | $y = \frac{1}{n}\sum x_i$ | $\bar{x}_i = \bar{y} / n$ |
| Max | $y = \max x_i$ | $\bar{x}_i = \bar{y}$ if $x_i = y$ else $0$ |

---

## ğŸ“Œ ä¹ã€éªŒè¯ä»£ç 

```python
import torch
import torch.nn.functional as F

def verify_gradients():
    """éªŒè¯æ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦ä¸ PyTorch è‡ªåŠ¨æ±‚å¯¼ä¸€è‡´"""
    torch.manual_seed(42)
    
    # ===== 1. MatMul =====
    X = torch.randn(2, 3, requires_grad=True)
    W = torch.randn(3, 4, requires_grad=True)
    Y = X @ W
    loss = Y.sum()
    loss.backward()
    
    # æ‰‹åŠ¨è®¡ç®—
    grad_Y = torch.ones_like(Y)
    grad_X_manual = grad_Y @ W.T
    grad_W_manual = X.T @ grad_Y
    
    print("=== MatMul ===")
    print(f"grad_X å·®å¼‚: {(X.grad - grad_X_manual).abs().max():.2e}")
    print(f"grad_W å·®å¼‚: {(W.grad - grad_W_manual).abs().max():.2e}")
    
    # ===== 2. ReLU =====
    x = torch.randn(5, requires_grad=True)
    y = F.relu(x)
    y.sum().backward()
    
    grad_x_manual = (x.detach() > 0).float()
    print("\n=== ReLU ===")
    print(f"grad_x å·®å¼‚: {(x.grad - grad_x_manual).abs().max():.2e}")
    
    # ===== 3. Softmax + CrossEntropy =====
    x = torch.randn(3, 5, requires_grad=True)
    target = torch.tensor([1, 3, 2])
    loss = F.cross_entropy(x, target)
    loss.backward()
    
    # æ‰‹åŠ¨è®¡ç®—
    s = F.softmax(x.detach(), dim=-1)
    target_onehot = F.one_hot(target, 5).float()
    grad_x_manual = (s - target_onehot) / 3  # é™¤ä»¥ batch size
    
    print("\n=== Softmax + CrossEntropy ===")
    print(f"grad_x å·®å¼‚: {(x.grad - grad_x_manual).abs().max():.2e}")

if __name__ == "__main__":
    verify_gradients()
```

---

## ğŸš€ æ€»ç»“

1. **æ ¸å¿ƒåŸåˆ™**ï¼š$\bar{x} = \frac{\partial L}{\partial x} = \bar{y} \cdot \frac{\partial y}{\partial x}$

2. **ç»´åº¦å£è¯€**ï¼šæ¢¯åº¦çš„ shape å¿…é¡»å’ŒåŸå˜é‡ä¸€æ ·

3. **è®°å¿†æŠ€å·§**ï¼š
   - åŠ æ³•ï¼šæ¢¯åº¦å¤åˆ¶
   - ä¹˜æ³•ï¼šä¹˜ä»¥å¯¹æ–¹
   - çŸ©é˜µä¹˜ï¼šè½¬ç½®äº¤æ¢ä½ç½®
   - æ¿€æ´»å‡½æ•°ï¼šä¹˜ä»¥å¯¼æ•°å€¼

4. **å·¥ç¨‹æ„ä¹‰**ï¼šç†è§£ backward æ‰èƒ½å†™è‡ªå®šä¹‰ CUDA kernelï¼ˆå¦‚ FlashAttentionï¼‰
