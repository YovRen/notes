# ç¬¬02è¯¾ï¼šè‡ªåŠ¨å¾®åˆ†ä¸è®¡ç®—å›¾

> PyTorch/TensorFlow å†…éƒ¨æ˜¯å¦‚ä½•æ„å»º DAG (æœ‰å‘æ— ç¯å›¾) çš„ï¼Ÿæ‰‹æ¨é“¾å¼æ³•åˆ™

---

## ç¬”è®°å†…å®¹

ä½ å¥½ã€‚åœ¨ä¸Šä¸€è¯¾ä¸­ï¼Œæˆ‘ä»¬ç¡®ç«‹äº†é«˜ç»´ç©ºé—´ä¸­â€œå‘é‡â€ä½œä¸ºä¿¡æ¯è½½ä½“çš„æ•°å­¦ç›´è§‰ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†æ·±å…¥ LLM è®­ç»ƒå¼•æ“çš„å¿ƒè„â€”â€”**è‡ªåŠ¨å¾®åˆ†ï¼ˆAutomatic Differentiation, ADï¼‰**ã€‚

å¯¹äºèµ„æ·±å·¥ç¨‹å¸ˆè€Œè¨€ï¼Œç†è§£ AutoDiff ä¸ä»…ä»…æ˜¯è°ƒç”¨ `loss.backward()`ã€‚åœ¨è®­ç»ƒåƒäº¿å‚æ•°æ¨¡å‹æ—¶ï¼Œè®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ç›´æ¥å†³å®šäº†æ˜¾å­˜çš„**å³°å€¼å ç”¨ï¼ˆPeak Memoryï¼‰**å’Œ**é€šä¿¡å¼€é”€**ã€‚æˆ‘ä»¬å°†ä» Jacobian çŸ©é˜µåˆ‡å…¥ï¼Œå‰–æ PyTorch çš„åŠ¨æ€å›¾æœºåˆ¶ï¼Œå¹¶é‡ç‚¹è®²è§£ LLM è®­ç»ƒä¸­å¿…ä¸å¯å°‘çš„**æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆActivation/Gradient Checkpointingï¼‰**æŠ€æœ¯ã€‚

---

### 1. ğŸ§® æ•°å­¦ä¸ç†è®ºæ¨å¯¼ (Theoretical Foundation)

æ·±åº¦å­¦ä¹ ä¸­çš„åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰æœ¬è´¨ä¸Šæ˜¯**åå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†ï¼ˆReverse Mode ADï¼‰**ã€‚å…¶æ ¸å¿ƒæ•°å­¦æ“ä½œæ˜¯ **Vector-Jacobian Product (VJP)**ã€‚

#### 1.1 é›…å¯æ¯”çŸ©é˜µä¸é“¾å¼æ³•åˆ™

å‡è®¾æ¨¡å‹ä¸ºä¸€ä¸ªå¤åˆå‡½æ•° $F(x) = f_L(f_{L-1}(\dots f_1(x)))$ï¼ŒæŸå¤±å‡½æ•°ä¸ºæ ‡é‡ $L \in \mathbb{R}$ã€‚
å¯¹äºä»»æ„å±‚ $l$ï¼Œè¾“å…¥ä¸ºå‘é‡ $h_{l-1} \in \mathbb{R}^{n}$ï¼Œè¾“å‡ºä¸º $h_l \in \mathbb{R}^{m}$ã€‚
è¯¥å±‚çš„å±€éƒ¨æ¢¯åº¦ç”±**é›…å¯æ¯”çŸ©é˜µ (Jacobian Matrix)** $J_l$ å®šä¹‰ï¼š

$$
J_l = \frac{\partial h_l}{\partial h_{l-1}} \in \mathbb{R}^{m \times n}
$$

æ ¹æ®é“¾å¼æ³•åˆ™ï¼ŒæŸå¤± $L$ å¯¹å‚æ•° $W$ çš„æ¢¯åº¦ä¸ºï¼š

$$
\nabla_W L = \left( \frac{\partial h_l}{\partial W} \right)^T \cdot \underbrace{\left( \frac{\partial h_L}{\partial h_{L-1}} \cdot \dots \cdot \frac{\partial h_{l+1}}{\partial h_l} \right)^T \cdot \nabla_{h_L} L}_{\text{Backpropagated Gradient Signal}}
$$

åœ¨åå‘æ¨¡å¼ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸æ˜¾å¼è®¡ç®—å·¨å¤§çš„ Jacobian çŸ©é˜µï¼ˆé‚£æ˜¯ $O(n^2)$ çš„å­˜å‚¨ï¼‰ï¼Œè€Œæ˜¯è®¡ç®—**å‘é‡-é›…å¯æ¯”ç§¯ (VJP)**ï¼š
è®¾ $\bar{h}_l = \nabla_{h_l} L$ ä¸ºâ€œä¸Šæ¸¸â€ä¼ æ¥çš„æ¢¯åº¦å‘é‡ï¼Œåˆ™å½“å‰å±‚çš„æ¢¯åº¦è®¡ç®—ä¸ºï¼š

$$
\bar{h}_{l-1} = \bar{h}_l^T J_l
$$

#### 1.2 Softmax çš„æ¢¯åº¦æ¨å¯¼ (Attention æ ¸å¿ƒ)

Transformer ä¸­çš„ Attention åŒ…å« Softmax æ“ä½œã€‚ç†è§£å…¶ Jacobian æ˜¯ä¼˜åŒ– FlashAttention ç­‰ç®—å­çš„åŸºç¡€ã€‚

$$
S_i = \frac{e^{z_i}}{\sum_{k} e^{z_k}}
$$

æˆ‘ä»¬éœ€è¦è®¡ç®— $\frac{\partial S_i}{\partial z_j}$ã€‚
å½“ $i = j$ æ—¶ï¼š

$$
\frac{\partial S_i}{\partial z_i} = \frac{e^{z_i}(\sum e^{z_k}) - e^{z_i}e^{z_i}}{(\sum e^{z_k})^2} = S_i(1 - S_i)
$$

å½“ $i \neq j$ æ—¶ï¼š

$$
\frac{\partial S_i}{\partial z_j} = \frac{0 - e^{z_i}e^{z_j}}{(\sum e^{z_k})^2} = -S_i S_j
$$

ç»¼ä¸Šï¼ŒSoftmax çš„ Jacobian çŸ©é˜µæ˜¯å¯¹ç§°ä¸”åŠè´Ÿå®šçš„ï¼ˆé™åˆ¶åœ¨é›¶å’Œå­ç©ºé—´ï¼‰ï¼š

$$
J_{ij} = S_i (\delta_{ij} - S_j)
$$

**æ¢¯åº¦æµè§†è§’**ï¼š
åå‘ä¼ æ’­æ—¶ï¼Œ$\delta z_j = \sum_i \delta S_i \frac{\partial S_i}{\partial z_j} = S_j (\delta S_j - \sum_k \delta S_k S_k)$ã€‚
è¿™æ„å‘³ç€ Softmax çš„æ¢¯åº¦è®¡ç®—é«˜åº¦ä¾èµ–äºè¾“å‡ºåˆ†å¸ƒ $S$ æœ¬èº«ã€‚**æ•°å€¼ç¨³å®šæ€§**åœ¨è¿™é‡Œè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨ä½ç²¾åº¦ï¼ˆFP16/BF16ï¼‰è®­ç»ƒæ—¶ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ LogSoftmax é€šå¸¸æ¯” Softmax æ›´ç¨³å®šçš„æ•°å­¦åŸå› ã€‚

---

### 2. ğŸ§¬ æ¶æ„ä¸ç®—æ³•ç»†èŠ‚ (Architecture & Algorithm)

PyTorch é‡‡ç”¨**åŠ¨æ€è®¡ç®—å›¾ (Dynamic Computational Graph / Define-by-Run)**ã€‚è¿™ä¸ TensorFlow 1.x çš„é™æ€å›¾æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèµ‹äºˆäº† LLM å¤„ç†å˜é•¿åºåˆ—ï¼ˆVariable Sequence Lengthï¼‰çš„çµæ´»æ€§ã€‚

#### 2.1 èŠ‚ç‚¹ (Node) ä¸ è¾¹ (Edge)

åœ¨ PyTorch å†…éƒ¨ï¼ˆC++ å±‚ï¼‰ï¼š

* **Tensor**: æ•°æ®è½½ä½“ï¼ŒåŒ…å« `grad_fn` æŒ‡é’ˆã€‚
* **Node (grad_fn)**: ä»£è¡¨ä¸€ä¸ªç®—å­ï¼ˆå¦‚ `AddBackward`, `MmBackward`ï¼‰ã€‚å®ƒå­˜å‚¨äº† `next_functions`ï¼ˆæŒ‡å‘è¾“å…¥çš„ `grad_fn`ï¼‰ï¼Œä»è€Œå½¢æˆ DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰ã€‚

#### 2.2 è‡ªå®šä¹‰ Autograd Function (æ·±åº¦å®šåˆ¶è§†è§’)

ä¸ºäº†ä¼˜åŒ– LLM çš„æ˜¾å­˜ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦æ‰‹å†™ `autograd.Function` æ¥æ§åˆ¶ä¸­é—´æ¿€æ´»å€¼çš„å­˜å‚¨ï¼ˆä¾‹å¦‚ï¼šFlashAttention ä¸ºäº†é¿å…å­˜å‚¨ $N \times N$ çš„ Attention Matrixï¼Œé‡å†™äº† backward passï¼‰ã€‚

```python
import torch

class EfficientAttention(torch.autograd.Function):
    @staticmethod
    def forward(ctx, Q, K, V):
        # Q, K, V shape: [Batch, Head, Seq, Dim]
        # å‰å‘è®¡ç®—ï¼šæ ‡å‡† Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))
        attn_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, V)
      
        # å…³é”®ç‚¹ï¼šctx.save_for_backward
        # é»˜è®¤ Autograd ä¼šä¿å­˜æ‰€æœ‰å‚ä¸è®¡ç®—çš„ Tensorã€‚
        # åœ¨ FlashAttention ä¸­ï¼Œæˆ‘ä»¬ä¸ä¿å­˜ attn_weights (N^2 ç©ºé—´)ï¼Œ
        # è€Œæ˜¯åªä¿å­˜ Q, K, Vï¼Œåœ¨ backward æ—¶é‡ç®— attn_weightsã€‚
        ctx.save_for_backward(Q, K, V, output) 
      
        return output

    @staticmethod
    def backward(ctx, grad_output):
        # å–å‡ºä¿å­˜çš„ Tensor
        Q, K, V, output = ctx.saved_tensors
      
        # recomputation (é‡è®¡ç®—) é€»è¾‘ï¼š
        # è¿™é‡Œéœ€è¦é‡æ–°æ‰§è¡Œéƒ¨åˆ† forward é€»è¾‘æ¥æ¢å¤ attn_weights
        # è¿™å°±æ˜¯ "æ—¶é—´æ¢ç©ºé—´" çš„æœ¬è´¨
        with torch.enable_grad():
             # ... å¤æ‚çš„æ¢¯åº¦è®¡ç®—é€»è¾‘ (Triton/CUDA Kernel å®ç°) ...
             pass
      
        return dQ, dK, dV
```

#### 2.3 æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)

å¯¹äº Transformerï¼Œå±‚æ•° $L$ ææ·±ã€‚
å¦‚æœä¸ä½¿ç”¨ Checkpointingï¼Œæ˜¾å­˜å¤æ‚åº¦ä¸º $O(L \cdot N \cdot d)$ã€‚
ä½¿ç”¨ Checkpointingï¼Œæˆ‘ä»¬å°†è®¡ç®—å›¾åˆ‡åˆ†ä¸ºå¤šä¸ª Segmentã€‚å‰å‘ä¼ æ’­æ—¶ï¼Œ**åªä¿ç•™ Segment è¾¹ç•Œçš„ Tensor**ï¼Œä¸¢å¼ƒä¸­é—´æ¿€æ´»å€¼ã€‚åå‘ä¼ æ’­ç»è¿‡è¯¥ Segment æ—¶ï¼Œ**é‡æ–°è¿›è¡Œä¸€æ¬¡å‰å‘è®¡ç®—**ä»¥æ¢å¤æ¿€æ´»å€¼ã€‚

| ç‰¹æ€§                 | æ ‡å‡† Backprop                                | Gradient Checkpointing                                       |
| :------------------- | :------------------------------------------- | :----------------------------------------------------------- |
| **æ˜¾å­˜å¤æ‚åº¦** | $O(N)$ (å­˜å‚¨æ‰€æœ‰å±‚æ¿€æ´»)                    | $O(\sqrt{N})$ (ä»…å­˜å‚¨è¾¹ç•Œ)                                 |
| **è®¡ç®—å¤æ‚åº¦** | $1 \times$ Forward + $1 \times$ Backward | $(1 + \frac{1}{k}) \times$ Forward + $1 \times$ Backward |
| **LLM å®è·µ**   | å‡ ä¹æ— æ³•è®­ç»ƒ 30B+ æ¨¡å‹                       | **è®­ç»ƒå¤§æ¨¡å‹çš„æ ‡é…**                                   |

---

### 3. ğŸ“‰ ç³»ç»Ÿä¸å·¥ç¨‹å®ç° (System & Engineering)

#### 3.1 æ˜¾å­˜ç“¶é¢ˆåˆ†æ (Memory-Bound Analysis)

è®­ç»ƒ LLM çš„æ˜¾å­˜å ç”¨ä¸»è¦ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š

1. **Model Parameters**: $\Phi$ (FP32/BF16)ã€‚
2. **Optimizer States**: Momentum, Variance (AdamW éœ€è¦ $2\Phi$ çš„ FP32 çŠ¶æ€)ã€‚
3. **Activations**: å‰å‘ä¼ æ’­ä¸­äº§ç”Ÿçš„ä¸­é—´å˜é‡ã€‚

å¯¹äºé•¿ä¸Šä¸‹æ–‡ï¼ˆLong Contextï¼‰è®­ç»ƒï¼Œ**Activations** æ˜¯ä¸»è¦ç“¶é¢ˆã€‚
ä¸€ä¸ª `[Batch=1, Seq=4096, Hidden=4096]` çš„ Tensor å ç”¨ï¼š
$1 \times 4096 \times 4096 \times 2 \text{ bytes (BF16)} \approx 32 \text{ MB}$ã€‚
LLaMA-7B æœ‰ 32 å±‚ï¼Œæ¯å±‚åŒ…å«å¤šä¸ªè¿™æ ·çš„ä¸­é—´å˜é‡ï¼ˆSelf-Attn, MLPï¼‰ï¼Œä¸ä¼˜åŒ–çš„è¯ç¬é—´ OOMã€‚

**Checkpointing çš„å·¥ç¨‹å®ç°**ï¼š
åœ¨ `transformers` åº“ä¸­ï¼Œé€šå¸¸é€šè¿‡ `model.gradient_checkpointing_enable()` å¼€å¯ã€‚
åº•å±‚è°ƒç”¨ `torch.utils.checkpoint.checkpoint(function, *args)`ã€‚è¿™æ„å‘³ç€ PyTorch å¼•æ“åœ¨ Graph Execution æ—¶ï¼Œé‡åˆ° Checkpoint èŠ‚ç‚¹ä¸ä¼šè®°å½•ä¸­é—´ op çš„ `grad_fn`ï¼Œç›´åˆ° Backward ä¿¡å·åˆ°è¾¾è¯¥èŠ‚ç‚¹ï¼Œæ‰ä¸´æ—¶æ„å»ºå±€éƒ¨å›¾ã€‚

#### 3.2 ç®—å­èåˆ (Operator Fusion)

AutoDiff çš„ä¸€ä¸ªå‰¯ä½œç”¨æ˜¯äº§ç”Ÿå¤§é‡çš„ kernel launch å¼€é”€å’Œæ˜¾å­˜è¯»å†™ï¼ˆMemory Accessï¼‰ã€‚
ä¾‹å¦‚ï¼š`x = x * scale + bias`

* **æœªèåˆ**ï¼šRead x -> Mul -> Write tmp; Read tmp -> Add -> Write xã€‚
* **èåˆ (Fusion)**ï¼šRead x -> Mul & Add -> Write xã€‚

åœ¨ GPU ä¸Šï¼Œè®¡ç®—é€šå¸¸å¾ˆå¿«ï¼Œ**HBM å¸¦å®½æ˜¯ç“¶é¢ˆ**ã€‚PyTorch 2.0 (`torch.compile`) å’Œ NVIDIA Apex/Triton è‡´åŠ›äºå°† Element-wise æ“ä½œï¼ˆå¦‚ ReLU, Dropout, Add, Normï¼‰èåˆè¿›ä¸€ä¸ª Kernelï¼Œè¿™ç›´æ¥æ”¹å˜äº†è®¡ç®—å›¾çš„æ‰§è¡Œæ–¹å¼ã€‚

---

### 4. âš”ï¸ å…³é”®æƒè¡¡ (Trade-offs)

#### 4.1 é™æ€å›¾ (Static) vs. åŠ¨æ€å›¾ (Dynamic)

* **TensorFlow 1.x / XLA (Static)**:
  * *ä¼˜åŠ¿*: ç¼–è¯‘å™¨å¯ä»¥å…¨å±€çœ‹åˆ°æ•´ä¸ªå›¾ï¼Œè¿›è¡Œæ¿€è¿›çš„å†…å­˜ä¼˜åŒ–ï¼ˆBuffer Reuseï¼‰å’Œç®—å­èåˆã€‚
  * *åŠ£åŠ¿*: debug æå…¶ç—›è‹¦ï¼Œéš¾ä»¥å®ç°åŠ¨æ€æ§åˆ¶æµï¼ˆå¦‚ MoE ä¸­çš„ Token Routingï¼Œæ¯ä¸ª batch æ¿€æ´»çš„ä¸“å®¶ä¸åŒï¼‰ã€‚
* **PyTorch (Dynamic)**:
  * *ä¼˜åŠ¿*: æå…¶çµæ´»ï¼Œç¬¦åˆ Python ç¼–ç¨‹ç›´è§‰ï¼Œå®Œç¾é€‚é… NLP å˜é•¿æ•°æ®ã€‚
  * *åŠ£åŠ¿*: è§£é‡Šå™¨å¼€é”€ï¼Œä¼˜åŒ–çš„æœºä¼šçª—å£å°ã€‚
  * *ç°çŠ¶*: **PyTorch 2.0** è¯•å›¾ä¸¤å…¨å…¶ç¾ã€‚é€šè¿‡ `torch.compile` æ•è·å›¾ï¼ˆGraph Captureï¼‰ï¼Œå°†åŠ¨æ€å›¾è½¬åŒ–ä¸ºé™æ€ IR (Inductor)ï¼Œç„¶åç¼–è¯‘æˆ Triton Kernelã€‚

#### 4.2 é‡è®¡ç®— (Recomputation) vs. æ˜¾å­˜

* **Trade-off**: å¢åŠ  33% çš„è®¡ç®—æ—¶é—´ï¼ˆå¤šä¸€æ¬¡å‰å‘ï¼‰ï¼Œæ¢å– 4-5 å€çš„æ˜¾å­˜èŠ‚çœã€‚
* **å†³ç­–**: åœ¨æ˜¾å­˜å¸¦å®½å—é™ï¼ˆMemory-boundï¼‰çš„åœºæ™¯ä¸‹ï¼Œé‡è®¡ç®—åè€Œå¯èƒ½å˜å¿«ï¼å› ä¸ºå‡å°‘äº†ä» HBM å†™å…¥å’Œè¯»å–å·¨å¤§ Activation Tensor çš„æ—¶é—´ï¼Œè€Œç°ä»£ GPU çš„è®¡ç®—å•å…ƒï¼ˆTensor Coresï¼‰å¾€å¾€æ˜¯ç©ºé—²çš„ã€‚

---

### 5. ğŸ“š è®ºæ–‡ä¸å‰æ²¿å¼•ç”¨ (References)

1. **"Training Deep Nets with Sublinear Memory Cost"** (Chen et al., 2016)
   * *Focus*: The foundational paper for Gradient Checkpointing ($O(\sqrt{N})$ memory strategy).
2. **"PyTorch: An Imperative Style, High-Performance Deep Learning Library"** (Paszke et al., NeurIPS 2019)
   * *Focus*: The architecture design of dynamic graphs and autograd engine.
3. **"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"** (Dao et al., 2022)
   * *Focus*: How custom backward passes (manual differentiation) can bypass HBM bottlenecks.

---

### ğŸš€ è¯¾åæ€è€ƒ (Next Step)

æ—¢ç„¶æˆ‘ä»¬æŒæ¡äº†æ¢¯åº¦æ˜¯å¦‚ä½•æµåŠ¨çš„ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯æ¢è®¨**å¦‚ä½•è®©æ¢¯åº¦æµåŠ¨å¾—æ›´é¡ºç•…**ï¼Œæˆ–è€…è¯´ï¼Œ**å¦‚ä½•é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸**ã€‚
è¿™ç›´æ¥å¼•å‡ºäº† Transformer ä¸­è‡³å…³é‡è¦çš„ç»„ä»¶ï¼š**æ¿€æ´»å‡½æ•°**ä¸**å½’ä¸€åŒ–**ã€‚

**ä¸ºä»€ä¹ˆ LLaMA ç”šè‡³å»æ‰äº† Bias é¡¹ï¼Ÿä¸ºä»€ä¹ˆ SwiGLU æ¯” GeLU æ•ˆæœæ›´å¥½ï¼Ÿ**

è¯·å‘é€ **`ç¬¬03è¯¾ï¼šæ¿€æ´»å‡½æ•°æ¼”å˜å² (å…¨é›†)`** ç»§ç»­æ·±å…¥ã€‚
