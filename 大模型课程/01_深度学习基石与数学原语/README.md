# 🔴 第一阶段：深度学习基石与数学原语

> *不仅是基础，更是为了读懂最新论文中的公式推导*

---

## 📋 阶段概览

| 课时 | 主题                                          | 核心内容                          |
| :--: | :-------------------------------------------- | :-------------------------------- |
|  01  | [高维空间的数学直觉](./01_高维空间的数学直觉.md) | 向量、矩阵、秩与模型压缩          |
|  02  | [自动微分与计算图](./02_自动微分与计算图.md)     | DAG、链式法则、反向传播           |
|  03  | [激活函数演变史](./03_激活函数演变史.md)         | ReLU → GeLU → SwiGLU            |
|  04  | [归一化技术详解](./04_归一化技术详解.md)         | BatchNorm → LayerNorm → RMSNorm |
|  05  | [损失函数与概率分布](./05_损失函数与概率分布.md) | Cross-Entropy、KL散度、Perplexity |
|  06  | [优化算法的深层逻辑](./06_优化算法的深层逻辑.md) | SGD → Adam → AdamW → Lion      |

---

## 🎯 学习目标

完成本阶段后，你将能够：

- [X] 理解高维空间中向量运算的几何意义
- [X] 从零推导神经网络的反向传播算法
- [X] 解释为什么现代 LLM 使用 GeLU/SwiGLU 而非 ReLU
- [ ] 说明 Pre-Norm vs Post-Norm 的区别
- [ ] 从信息论角度理解交叉熵损失
- [ ] 解释 AdamW 中权重衰减解耦的意义

---

## 🔗 前置知识

- 线性代数基础（向量、矩阵乘法）
- 微积分基础（导数、偏导数）
- Python 编程基础
- PyTorch 基本使用

---

## 📚 推荐资源

|  类型  | 资源                           | 说明         |
| :-----: | :----------------------------- | :----------- |
| 📖 书籍 | *Deep Learning* - Goodfellow | 深度学习圣经 |
| 📖 书籍 | *线性代数应该这样学*         | 直觉化理解   |
| 🎥 视频 | 3Blue1Brown 线性代数系列       | 可视化理解   |
| 📝 博客 | Lil'Log                        | 技术博客精品 |

---

[⬅️ 返回主目录](../README.md) | [➡️ 下一阶段](../02_Transformer架构微观拆解/README.md)
