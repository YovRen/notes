"""
éªŒè¯å‡è®¾ï¼šè¯å‘é‡çš„æ¨¡é•¿ä¸è¯é¢‘ç›¸å…³ï¼Œæ–¹å‘ä¸è¯­ä¹‰ç›¸å…³

å®éªŒè®¾è®¡ï¼š
1. åŠ è½½é¢„è®­ç»ƒè¯å‘é‡ (Word2Vec / GloVe)
2. åˆ†æï¼šæ¨¡é•¿ vs è¯é¢‘ çš„ç›¸å…³æ€§
3. t-SNE å¯è§†åŒ–ï¼šéªŒè¯è¯­ä¹‰ç›¸ä¼¼çš„è¯æ–¹å‘æ˜¯å¦æ¥è¿‘
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from collections import defaultdict
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================
# æ–¹æ³•1: ä½¿ç”¨ Gensim åŠ è½½ Word2Vec (æ¨èï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹)
# ============================================================


def load_word2vec_gensim():
    """
    ä½¿ç”¨ gensim åŠ è½½ Google é¢„è®­ç»ƒçš„ Word2Vec
    é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ (~1.5GB)
    """
    import gensim.downloader as api
    print("æ­£åœ¨åŠ è½½ Word2Vec æ¨¡å‹ (é¦–æ¬¡éœ€ä¸‹è½½ ~1.5GB)...")
    model = api.load("word2vec-google-news-300")
    return model

# ============================================================
# æ–¹æ³•2: ä½¿ç”¨ GloVe (æ›´å°ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•)
# ============================================================


def download_glove():
    """ä¸‹è½½ GloVe è¯å‘é‡ (50ç»´ï¼Œå°æ–‡ä»¶ï¼Œé€‚åˆæµ‹è¯•)"""
    import urllib.request
    import zipfile

    url = "http://nlp.stanford.edu/data/glove.6B.zip"
    zip_path = "glove.6B.zip"

    if not os.path.exists("glove.6B.50d.txt"):
        print("æ­£åœ¨ä¸‹è½½ GloVe (862MB)ï¼Œè¯·ç¨å€™...")
        urllib.request.urlretrieve(url, zip_path)

        print("è§£å‹ä¸­...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extract("glove.6B.50d.txt")
        os.remove(zip_path)

    return "glove.6B.50d.txt"


def load_glove(filepath, max_words=50000):
    """åŠ è½½ GloVe è¯å‘é‡"""
    embeddings = {}
    word_rank = {}  # è¯åœ¨æ–‡ä»¶ä¸­çš„æ’åï¼ˆè¿‘ä¼¼è¯é¢‘æ’åï¼‰

    print(f"åŠ è½½ GloVe è¯å‘é‡ (å‰ {max_words} ä¸ªè¯)...")
    with open(filepath, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f):
            if idx >= max_words:
                break
            values = line.strip().split()
            word = values[0]
            vector = np.array(values[1:], dtype=np.float32)
            embeddings[word] = vector
            word_rank[word] = idx + 1  # æ’åä»1å¼€å§‹

    print(f"åŠ è½½å®Œæˆï¼Œå…± {len(embeddings)} ä¸ªè¯")
    return embeddings, word_rank

# ============================================================
# æ–¹æ³•3: è‡ªå·±è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ (æœ€è½»é‡ï¼Œæ— éœ€ä¸‹è½½)
# ============================================================


def train_simple_word2vec():
    """
    ä½¿ç”¨ gensim åœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒ Word2Vec
    è¿™æ ·å¯ä»¥ç›´æ¥è·å–çœŸå®è¯é¢‘
    """
    from gensim.models import Word2Vec
    from collections import Counter

    # ä½¿ç”¨ä¸€äº›ç¤ºä¾‹å¥å­ï¼ˆå®é™…åº”ç”¨ä¸­ç”¨æ›´å¤§çš„è¯­æ–™åº“ï¼‰
    sentences = [
        # çš‡å®¤ç›¸å…³
        ["king", "queen", "prince", "princess", "royal", "crown", "throne", "palace"],
        ["king", "rules", "the", "kingdom", "with", "queen"],
        ["prince", "will", "become", "king", "someday"],
        ["queen", "wears", "a", "beautiful", "crown"],

        # åŠ¨ç‰©ç›¸å…³
        ["cat", "dog", "pet", "animal", "cute", "furry"],
        ["cat", "sleeps", "on", "the", "sofa"],
        ["dog", "runs", "in", "the", "park"],
        ["my", "pet", "cat", "is", "cute"],

        # é£Ÿç‰©ç›¸å…³
        ["apple", "banana", "orange", "fruit", "sweet", "healthy"],
        ["eat", "apple", "every", "day"],
        ["banana", "is", "yellow", "fruit"],
        ["orange", "juice", "is", "sweet"],

        # ç§‘æŠ€ç›¸å…³
        ["computer", "phone", "laptop", "technology", "digital", "software"],
        ["use", "computer", "for", "work"],
        ["phone", "is", "a", "communication", "device"],

        # é«˜é¢‘è¯ï¼ˆæ•…æ„å¤šå‡ºç°ï¼‰
        ["the", "is", "a", "of", "and", "to", "in"],
        ["the", "the", "the", "is", "is", "a", "a"],
        ["of", "the", "and", "to", "in", "the", "is"],
    ] * 100  # é‡å¤ä»¥å¢åŠ è®­ç»ƒæ•°æ®

    # ç»Ÿè®¡çœŸå®è¯é¢‘
    word_freq = Counter()
    for sent in sentences:
        word_freq.update(sent)

    print("è®­ç»ƒ Word2Vec æ¨¡å‹...")
    model = Word2Vec(
        sentences=sentences,
        vector_size=50,
        window=5,
        min_count=1,
        workers=4,
        epochs=100
    )

    return model, dict(word_freq)

# ============================================================
# åˆ†æå‡½æ•°
# ============================================================


def analyze_magnitude_vs_frequency(embeddings, word_freq_or_rank, is_rank=False):
    """
    åˆ†æï¼šæ¨¡é•¿ vs è¯é¢‘/æ’å çš„ç›¸å…³æ€§

    Args:
        embeddings: dict, word -> vector
        word_freq_or_rank: dict, word -> frequency æˆ– rank
        is_rank: bool, True è¡¨ç¤ºæ˜¯æ’åï¼ˆè¶Šå°è¶Šé«˜é¢‘ï¼‰ï¼ŒFalse è¡¨ç¤ºæ˜¯é¢‘ç‡
    """
    words = []
    magnitudes = []
    frequencies = []

    for word, vec in embeddings.items():
        if word in word_freq_or_rank:
            words.append(word)
            magnitudes.append(np.linalg.norm(vec))
            freq = word_freq_or_rank[word]
            # å¦‚æœæ˜¯æ’åï¼Œè½¬æ¢ä¸º"ä¼ªé¢‘ç‡"ï¼ˆæ’åè¶Šå°ï¼Œé¢‘ç‡è¶Šé«˜ï¼‰
            if is_rank:
                freq = 1.0 / freq  # æ’åçš„å€’æ•°ä½œä¸ºé¢‘ç‡ä»£ç†
            frequencies.append(freq)

    magnitudes = np.array(magnitudes)
    frequencies = np.array(frequencies)

    # è®¡ç®— Pearson ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(magnitudes, frequencies)[0, 1]

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # å›¾1: æ•£ç‚¹å›¾
    ax1 = axes[0]
    ax1.scatter(frequencies, magnitudes, alpha=0.5, s=10)
    ax1.set_xlabel('è¯é¢‘ (æˆ– 1/æ’å)' if is_rank else 'è¯é¢‘', fontsize=12)
    ax1.set_ylabel('å‘é‡æ¨¡é•¿', fontsize=12)
    ax1.set_title(f'æ¨¡é•¿ vs è¯é¢‘\nPearson ç›¸å…³ç³»æ•° r = {correlation:.4f}', fontsize=14)

    if is_rank:
        ax1.set_xscale('log')
    ax1.set_yscale('linear')

    # æ ‡æ³¨ä¸€äº›å…¸å‹è¯
    # æ‰¾é«˜é¢‘ä½æ¨¡é•¿å’Œä½é¢‘é«˜æ¨¡é•¿çš„è¯
    sorted_indices = np.argsort(magnitudes)
    for idx in list(sorted_indices[:5]) + list(sorted_indices[-5:]):
        ax1.annotate(words[idx], (frequencies[idx], magnitudes[idx]), fontsize=8)

    # å›¾2: æŒ‰æ¨¡é•¿æ’åºçš„è¯
    ax2 = axes[1]
    sorted_by_mag = sorted(zip(words, magnitudes, frequencies), key=lambda x: x[1], reverse=True)

    top_mag = sorted_by_mag[:15]
    bottom_mag = sorted_by_mag[-15:]

    display_words = [w for w, m, f in top_mag + bottom_mag]
    display_mags = [m for w, m, f in top_mag + bottom_mag]
    colors = ['red'] * 15 + ['blue'] * 15

    y_pos = np.arange(len(display_words))
    ax2.barh(y_pos, display_mags, color=colors, alpha=0.7)
    ax2.set_yticks(y_pos)
    ax2.set_yticklabels(display_words)
    ax2.set_xlabel('å‘é‡æ¨¡é•¿', fontsize=12)
    ax2.set_title('æ¨¡é•¿æœ€å¤§ (çº¢) vs æœ€å° (è“) çš„è¯', fontsize=14)
    ax2.invert_yaxis()

    plt.tight_layout()
    plt.savefig('magnitude_vs_frequency.png', dpi=150, bbox_inches='tight')
    plt.show()

    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"   Pearson ç›¸å…³ç³»æ•°: r = {correlation:.4f}")
    if correlation > 0.3:
        print(f"   âœ… æ­£ç›¸å…³: æ¨¡é•¿ä¸è¯é¢‘æœ‰è¾ƒå¼ºæ­£ç›¸å…³ï¼Œæ”¯æŒ'æ¨¡é•¿ç¼–ç è¯é¢‘'å‡è®¾")
    elif correlation < -0.3:
        print(f"   âš ï¸ è´Ÿç›¸å…³: æ¨¡é•¿ä¸è¯é¢‘è´Ÿç›¸å…³")
    else:
        print(f"   â“ å¼±ç›¸å…³: ç›¸å…³æ€§ä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®")

    return correlation


def visualize_semantic_clusters(embeddings, word_groups, title="t-SNE è¯­ä¹‰èšç±»å¯è§†åŒ–"):
    """
    ç”¨ t-SNE å¯è§†åŒ–è¯­ä¹‰èšç±»

    Args:
        embeddings: dict, word -> vector
        word_groups: dict, group_name -> list of words
    """
    all_words = []
    all_vectors = []
    all_labels = []
    all_colors = []

    color_map = plt.cm.get_cmap('tab10')

    for group_idx, (group_name, words) in enumerate(word_groups.items()):
        for word in words:
            if word in embeddings:
                all_words.append(word)
                all_vectors.append(embeddings[word])
                all_labels.append(group_name)
                all_colors.append(color_map(group_idx))

    if len(all_vectors) < 5:
        print("è¯æ±‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œ t-SNE å¯è§†åŒ–")
        return

    all_vectors = np.array(all_vectors)

    # å¯¹æ¯”ï¼šåŸå§‹å‘é‡ vs L2 å½’ä¸€åŒ–åçš„å‘é‡
    normalized_vectors = all_vectors / np.linalg.norm(all_vectors, axis=1, keepdims=True)

    fig, axes = plt.subplots(1, 2, figsize=(16, 7))

    for ax_idx, (vectors, subtitle) in enumerate([
        (all_vectors, "åŸå§‹å‘é‡ (ä¿ç•™æ¨¡é•¿)"),
        (normalized_vectors, "L2 å½’ä¸€åŒ–å (åªä¿ç•™æ–¹å‘)")
    ]):
        print(f"æ­£åœ¨è®¡ç®— t-SNE ({subtitle})...")

        perplexity = min(30, len(vectors) - 1)
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        vectors_2d = tsne.fit_transform(vectors)

        ax = axes[ax_idx]

        # æŒ‰ç»„ç»˜åˆ¶
        for group_idx, (group_name, _) in enumerate(word_groups.items()):
            mask = [l == group_name for l in all_labels]
            group_points = vectors_2d[mask]
            ax.scatter(group_points[:, 0], group_points[:, 1],
                       c=[color_map(group_idx)], label=group_name, s=100, alpha=0.7)

        # æ ‡æ³¨è¯
        for i, word in enumerate(all_words):
            ax.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),
                        fontsize=9, alpha=0.8)

        ax.set_title(subtitle, fontsize=14)
        ax.legend(loc='best')
        ax.set_xlabel('t-SNE ç»´åº¦ 1')
        ax.set_ylabel('t-SNE ç»´åº¦ 2')

    plt.suptitle(title, fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('tsne_semantic_clusters.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("\nğŸ“Š t-SNE å¯è§†åŒ–è¯´æ˜:")
    print("   - å·¦å›¾: åŸå§‹å‘é‡ï¼ŒåŒæ—¶åŒ…å«æ–¹å‘å’Œæ¨¡é•¿ä¿¡æ¯")
    print("   - å³å›¾: å½’ä¸€åŒ–åï¼Œåªä¿ç•™æ–¹å‘ä¿¡æ¯")
    print("   - å¦‚æœä¸¤å›¾èšç±»æ•ˆæœç›¸ä¼¼ï¼Œè¯´æ˜è¯­ä¹‰ä¸»è¦ç¼–ç åœ¨æ–¹å‘ä¸Š")


def compute_similarity_comparison(embeddings, word_pairs):
    """
    å¯¹æ¯”ç‚¹ç§¯ç›¸ä¼¼åº¦å’Œä½™å¼¦ç›¸ä¼¼åº¦
    """
    print("\nğŸ“Š ç›¸ä¼¼åº¦å¯¹æ¯” (ç‚¹ç§¯ vs ä½™å¼¦):")
    print("-" * 70)
    print(f"{'è¯å¯¹':<25} {'ç‚¹ç§¯':<12} {'ä½™å¼¦':<12} {'æ¨¡é•¿1':<10} {'æ¨¡é•¿2':<10}")
    print("-" * 70)

    for w1, w2 in word_pairs:
        if w1 in embeddings and w2 in embeddings:
            v1, v2 = embeddings[w1], embeddings[w2]
            dot = np.dot(v1, v2)
            cos = dot / (np.linalg.norm(v1) * np.linalg.norm(v2))
            mag1, mag2 = np.linalg.norm(v1), np.linalg.norm(v2)
            print(f"{w1 + ' - ' + w2:<25} {dot:<12.4f} {cos:<12.4f} {mag1:<10.4f} {mag2:<10.4f}")
    print("-" * 70)

# ============================================================
# ä¸»ç¨‹åº
# ============================================================


def main(choice="1"):
    print("=" * 60)
    print("  éªŒè¯å®éªŒï¼šè¯å‘é‡çš„æ¨¡é•¿ vs æ–¹å‘")
    print("=" * 60)

    # é€‰æ‹©æ•°æ®æº
    print("\næ•°æ®æºé€‰é¡¹:")
    print("1. è‡ªå·±è®­ç»ƒå°æ¨¡å‹ (æ— éœ€ä¸‹è½½)")
    print("2. ä¸‹è½½ GloVe é¢„è®­ç»ƒè¯å‘é‡ (862MB)")
    print("3. ä¸‹è½½ Google Word2Vec (1.5GB)")
    print(f"\nå½“å‰ä½¿ç”¨é€‰é¡¹: {choice}")

    if choice == "2":
        # GloVe
        glove_path = download_glove()
        embeddings, word_rank = load_glove(glove_path, max_words=30000)

        # åˆ†ææ¨¡é•¿ vs æ’å
        print("\n" + "=" * 60)
        print("  å®éªŒ1: æ¨¡é•¿ä¸è¯é¢‘(æ’å)çš„ç›¸å…³æ€§")
        print("=" * 60)
        analyze_magnitude_vs_frequency(embeddings, word_rank, is_rank=True)

        # è¯­ä¹‰èšç±»å¯è§†åŒ–
        word_groups = {
            "çš‡å®¤": ["king", "queen", "prince", "princess", "royal", "crown", "throne"],
            "åŠ¨ç‰©": ["cat", "dog", "lion", "tiger", "elephant", "bird", "fish"],
            "æ°´æœ": ["apple", "banana", "orange", "grape", "mango", "peach"],
            "å›½å®¶": ["china", "japan", "america", "france", "germany", "russia"],
            "é¢œè‰²": ["red", "blue", "green", "yellow", "black", "white", "purple"],
        }

    elif choice == "3":
        # Word2Vec
        model = load_word2vec_gensim()
        embeddings = {word: model[word] for word in model.key_to_index}
        word_rank = {word: idx + 1 for idx, word in enumerate(model.key_to_index)}

        analyze_magnitude_vs_frequency(embeddings, word_rank, is_rank=True)

        word_groups = {
            "çš‡å®¤": ["king", "queen", "prince", "princess", "royal", "crown", "throne"],
            "åŠ¨ç‰©": ["cat", "dog", "lion", "tiger", "elephant", "bird", "fish"],
            "æ°´æœ": ["apple", "banana", "orange", "grape", "mango", "peach"],
            "å›½å®¶": ["China", "Japan", "America", "France", "Germany", "Russia"],
            "ç§‘æŠ€": ["computer", "phone", "laptop", "software", "internet", "technology"],
        }

    else:
        # è‡ªå·±è®­ç»ƒ
        model, word_freq = train_simple_word2vec()
        embeddings = {word: model.wv[word] for word in model.wv.key_to_index}

        print("\n" + "=" * 60)
        print("  å®éªŒ1: æ¨¡é•¿ä¸è¯é¢‘çš„ç›¸å…³æ€§")
        print("=" * 60)
        analyze_magnitude_vs_frequency(embeddings, word_freq, is_rank=False)

        word_groups = {
            "çš‡å®¤": ["king", "queen", "prince", "princess", "royal", "crown"],
            "åŠ¨ç‰©": ["cat", "dog", "pet", "animal"],
            "æ°´æœ": ["apple", "banana", "orange", "fruit"],
            "é«˜é¢‘è¯": ["the", "is", "a", "of", "and", "to"],
        }

    # è¯­ä¹‰èšç±»å¯è§†åŒ–
    print("\n" + "=" * 60)
    print("  å®éªŒ2: t-SNE è¯­ä¹‰èšç±»å¯è§†åŒ–")
    print("=" * 60)
    visualize_semantic_clusters(embeddings, word_groups)

    # ç›¸ä¼¼åº¦å¯¹æ¯”
    print("\n" + "=" * 60)
    print("  å®éªŒ3: ç‚¹ç§¯ vs ä½™å¼¦ç›¸ä¼¼åº¦å¯¹æ¯”")
    print("=" * 60)

    word_pairs = [
        ("king", "queen"),
        ("king", "prince"),
        ("cat", "dog"),
        ("apple", "banana"),
        ("king", "apple"),
        ("cat", "banana"),
    ]

    # è¿‡æ»¤å­˜åœ¨çš„è¯å¯¹
    valid_pairs = [(w1, w2) for w1, w2 in word_pairs
                   if w1 in embeddings and w2 in embeddings]

    if valid_pairs:
        compute_similarity_comparison(embeddings, valid_pairs)

    print("\nâœ… å®éªŒå®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾ç‰‡:")
    print("   - magnitude_vs_frequency.png (æ¨¡é•¿ vs è¯é¢‘)")
    print("   - tsne_semantic_clusters.png (è¯­ä¹‰èšç±»)")


if __name__ == "__main__":
    import sys
    choice = sys.argv[1] if len(sys.argv) > 1 else "1"
    main(choice)
