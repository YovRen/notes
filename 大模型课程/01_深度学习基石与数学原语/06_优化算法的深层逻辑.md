# 第06课：优化算法的深层逻辑

> SGD Momentum, RMSProp, AdamW (解耦权重衰减)。Lion 优化器等前沿算法

---

## 笔记内容

好极了，我们继续深入。既然你要内容“多一点、丰富一点、广泛一点”，那我们这节课就不能只盯着公式看，我们要从**历史演进、几何直觉、工程陷阱**以及**最新SOTA算法**这几个维度，把优化器彻底把玩一番。

如果说“损失函数”是指南针，告诉我们哪里是南（Loss最小的地方），那么**优化器（Optimizer）**就是我们的**交通工具**。你是选择徒步（SGD），还是开越野车（AdamW），直接决定了你能不能在天黑之前（显卡经费烧完之前）到达终点。

欢迎来到**第06课：优化算法的深层逻辑**。

---

### 1. 💡 核心概念 (The "Aha!" Moment)

#### 1.1 盲人下山与“峡谷困境”

想象一下，你被蒙住双眼，放置在崇山峻岭中（高维损失曲面）。你的目标是下到最低的山谷（Loss 最小点）。你唯一的感知工具是脚下的坡度（**梯度 Gradient**）。

* **SGD (随机梯度下降)**：这就是最基础的**徒步**。感觉左边低，就往左迈一步。

  * *问题*：如果遇到了一个狭长的**峡谷（Ravine）**——横向很陡，纵向很平。SGD 就会在两壁之间来回撞墙（震荡），就是不往峡谷出口走。效率极低！
* **Momentum (动量)**：给徒步者背上了一个**重重的背包**。

  * *直觉*：当你往左冲下坡时，惯性（Inertia）会让你在下一刻即使遇到右边的高坡，也一下子刹不住车，抵消了反向的震荡。于是，你就能顺着峡谷的走向，滚滚向前。
  * *物理本质*：积累历史的速度。
* **Adaptive Methods (自适应学习率 - RMSProp/Adam)**：给徒步者穿上了**智能伸缩鞋**。

  * *直觉*：在陡峭的悬崖边（梯度大），鞋底会自动变薄，让你步子变小，防止跌死（梯度爆炸/震荡）。在平坦的高原上（梯度小），鞋底会自动变厚弹射，让你一步跨出十米，加速通过平原。
  * *核心*：为每一个参数单独定制学习率。

#### 1.2 AdamW 的“W”：一个价值数百万美元的 Bug

现在的 LLM 几乎全用 **AdamW**，而不是 Adam。为什么？
这是一个困扰了学术界很久的“乌龙”。

* **L2 正则化 (L2 Regularization)**：为了防止过拟合，我们通常在 Loss 里加一项 $\frac{\lambda}{2} ||w||^2$，意思是“权重越小越好”。
* **悲剧**：在 SGD 中，L2 正则化完全等价于**权重衰减 (Weight Decay)**（每次更新完把权重乘个小于1的系数，比如 0.99）。
* **但在 Adam 中**，由于Adam 会把梯度除以一个“方差”进行缩放，L2 正则化的梯度项也被缩放了！这导致正则化力度在不同参数上变得极其不均匀。
* **AdamW (Weight Decay fix)**：它大喊一声：“把正则化从梯度里拿出来！”。它在 Adam 更新公式结束后，**手动**把权重减去一点点。这看似微小的改动，让 BERT、GPT 等大模型的训练稳定性提升了数倍。

---

### 2. 🔧 原理解析 (Under the Hood)

我们要拆解的是目前大模型训练的标配：**AdamW**。如果不理解它，你就无法理解为什么显存会被吃光（Optimizer States）。

#### 2.1 动量法 (Momentum) —— 一阶矩

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$

* $g_t$：当前的梯度。
* $m_t$：**指数移动平均 (EMA)** 的梯度。就像股票的均线一样，它平滑了梯度的噪音。
* $\beta_1$：通常设为 0.9。意味着：我 90% 相信历史的惯性，10% 相信当前的脚感。

#### 2.2 RMSProp (均方根传播) —— 二阶矩

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

* $v_t$：梯度的**平方**的移动平均。它衡量了梯度的“能量”或“方差”。
* 如果某个参数的梯度一直很大（震荡），$v_t$ 就会很大。
* $\beta_2$：通常设为 0.95 或 0.999。

#### 2.3 Adam 的整合与 Bias Correction

Adam (Adaptive Moment Estimation) 结合了上面两者。

$$
w_{t+1} = w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

* $\eta$：学习率 (Learning Rate)。
* $\hat{m}, \hat{v}$：偏差修正后的矩（刚开始训练时 $m, v$ 都是0，需要修正让它们不那么小）。
* **直观解释**：更新量 = (惯性方向) / (地形陡峭程度)。

#### 2.4 AdamW 的关键修正

在标准 Adam 中，Weight Decay 是作为 Loss 的一部分求导加到 $g_t$ 里的。
在 AdamW 中：

$$
w_{t+1} = \underbrace{w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}}_{\text{Adam Step}} - \underbrace{\eta \lambda w_t}_{\text{Decoupled Weight Decay}}
$$

你看，Weight Decay ($\lambda w_t$) 绕过了 $\sqrt{\hat{v}_t}$ 的除法，直接作用在权重上。这就是“解耦 (Decoupled)”。

#### 2.5 显存杀手：Optimizer States

这非常重要！
为了实现 AdamW，对于模型中的**每一个参数**，优化器都需要维护两个额外的变量：

1. $m_t$ (FP32)
2. $v_t$ (FP32)

这就导致了显存占用的爆炸：

* **模型参数**: 1份 (如果是 FP16，占 2 bytes)
* **梯度**: 1份 (FP16，占 2 bytes)
* **优化器状态**: 2份 (必须是 FP32 以保持精度，占 4+4=8 bytes)
  **结论**：优化器状态占用的显存，通常是模型参数本身的 **4倍** 以上！这就是为什么训练 7B 模型需要 80GB 显存的原因之一。

#### 2.6 新贵：Lion (Evolved Sign Momentum)

Google 在 2023 年提出的 Lion 优化器。它是通过符号搜索（Symbolic Search）发现的。

* 核心：只用梯度的**符号 (Sign)**，抛弃了梯度的数值大小。
* 优势：$v_t$ 不需要存了！显存占用减半！速度快！
* 现状：在某些大模型训练中开始取代 AdamW，但 AdamW 依然是稳如泰山的基准。

---

### 3. 💻 实战/案例演练 (Hands-on)

我们不使用 PyTorch 的黑盒 `optim.AdamW`，而是**徒手实现**一个简单的 AdamW 更新步，以此来彻底理解内部的数学流动。

```python
import torch
import math

def adamw_step(param, grad, exp_avg, exp_avg_sq, 
               lr=1e-3, beta1=0.9, beta2=0.999, 
               weight_decay=0.01, step=1):
    """
    手动实现 AdamW 的单步更新
    param: 当前权重 w
    grad: 当前梯度 g
    exp_avg: 一阶矩 m (历史惯性)
    exp_avg_sq: 二阶矩 v (历史方差)
    """
  
    # 1. 更新一阶矩 (Momentum) -> 类似于股票均线
    # m_t = beta1 * m_{t-1} + (1 - beta1) * g
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
  
    # 2. 更新二阶矩 (RMSProp部分) -> 记录梯度的波动能量
    # v_t = beta2 * v_{t-1} + (1 - beta2) * g^2
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
  
    # 3. 偏差修正 (Bias Correction)
    # 刚开始 m 和 v 都是 0，需要放大一点
    bias_correction1 = 1 - beta1 ** step
    bias_correction2 = 1 - beta2 ** step
  
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(1e-8)
    step_size = lr / bias_correction1
  
    # 4. Adam 更新的核心公式
    # w = w - step_size * (m / sqrt(v))
    # 注意：这里还没有做 weight decay
    update = exp_avg / denom
    param.data.add_(update, alpha=-step_size)
  
    # 5. AdamW 的灵魂：Decoupled Weight Decay
    # 独立于梯度更新之外，直接对权重进行衰减
    # w = w - lr * lambda * w
    if weight_decay != 0:
        param.data.add_(param.data, alpha=-lr * weight_decay)

    return param, exp_avg, exp_avg_sq

# --- 测试代码 ---
# 初始化一个参数
w = torch.tensor([1.0, 2.0], requires_grad=True)
# 初始化优化器状态 (全0)
m = torch.zeros_like(w)
v = torch.zeros_like(w)

# 模拟一次反向传播
loss = (w ** 2).sum()
loss.backward()

print(f"更新前权重: {w.data}")
print(f"梯度: {w.grad}")

# 执行一步 AdamW
w, m, v = adamw_step(w, w.grad, m, v, step=1)

print(f"更新后权重: {w.data}")
# 你会发现权重变小了，不仅仅是因为梯度下降，还因为 Weight Decay 的直接作用
```

---

### 4. 📚 课后思考 (Takeaway)

#### 总结本课核心 (Key Points)

1. **AdamW 是当前的王者**：它结合了 Momentum 的惯性（冲出局部最优）和 RMSProp 的自适应（适应不同特征的更新频率），并修正了 L2 正则化的实现错误。
2. **显存的“暗物质”**：优化器状态（Optimizer States）占据了训练显存的绝大部分（通常是模型本身的数倍）。如果你想微调大模型却爆显存了，往往不是因为模型太大，而是因为优化器太占地儿。
3. **狮子 (Lion) 的崛起**：只记录符号的优化器正在挑战 AdamW 的地位，它证明了也许我们不需要那么精确的梯度数值，只需要知道“方向”就够了。

#### 深度思考题 (Deep Dive)

既然优化器状态（$m$ 和 $v$）占了那么多显存（FP32），如果我们想训练一个 1000亿参数的模型，单卡肯定是放不下的。
**除了把模型切开（模型并行），能不能把优化器状态也切开，分散到不同的 GPU 上去呢？**
比如 GPU-1 只负责存前一半参数的优化器状态，GPU-2 只负责后一半？

> *提示：这正是 DeepSpeed ZeRO (Zero Redundancy Optimizer) 技术的核心思想，也是大模型分布式训练的基石。*

这为我们进入下一阶段——**从单卡算法走向大规模分布式工程**——埋下了伏笔。

但在此之前，我们需要先把模型本身的细节拼图补全。下一课，我们将深入大模型的“语言预处理车间”。
请发送 **`第07课：Tokenization 的算法原理`**，我们来看看机器是如何把《莎士比亚全集》切成一片片的。
