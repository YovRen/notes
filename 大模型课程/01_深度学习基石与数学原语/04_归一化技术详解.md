# 第04课：归一化技术详解

> BatchNorm vs LayerNorm (Pre-Norm vs Post-Norm). 为什么Transformer选RMSNorm？

---

## 笔记内容

你好。在上一课中，我们构建了 SwiGLU 这样的强力非线性变换。然而，随着网络层数加深（LLaMA-7B 为 32 层，GPT-3 为 96 层），每一层的输入分布如果不加控制，会发生**内部协变量偏移（Internal Covariate Shift）**，导致梯度训练过程极其不稳定。

本节课我们将深入解构**归一化（Normalization）**技术。对于大模型架构师而言，选择 LayerNorm 还是 RMSNorm，选择 Pre-Norm 还是 Post-Norm，直接决定了模型能否收敛以及训练的极限深度。

---

### 1. 🧮 数学与理论推导 (Theoretical Foundation)

在 NLP 和 LLM 领域，BatchNorm 因其对 Batch Size 的依赖以及无法处理变长序列而被抛弃。我们将聚焦于 **LayerNorm** 及其简化版 **RMSNorm**。

#### 1.1 Layer Normalization (LN) 的完整形式

对于输入向量 $x \in \mathbb{R}^d$（即一个 Token 的 Embedding），LayerNorm 在特征维度（Feature Dimension）上进行归一化：

1. **计算均值与方差**：
   $$
   \mu = \frac{1}{d} \sum_{i=1}^d x_i, \quad \sigma^2 = \frac{1}{d} \sum_{i=1}^d (x_i - \mu)^2
   $$
2. **归一化**：
   $$
   \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
   $$
3. **仿射变换 (Affine Transformation)**：
   引入可学习参数 $\gamma, \beta \in \mathbb{R}^d$（Gain and Bias）：
   $$
   y_i = \gamma_i \hat{x}_i + \beta_i
   $$

**梯度视角**：
LN 使得每一层的输入均值为 0，方差为 1，从而让梯度流保持在一个健康的数值范围内，避免了深层网络中的梯度消失或爆炸。

#### 1.2 RMSNorm (Root Mean Square Normalization)

RMSNorm 是 LLaMA, DeepSeek, PaLM 等现代 LLM 的标配。它基于一个假设：**重新中心化（Re-centering, 即减去均值）对于层归一化是不必要的**，缩放（Rescaling）才是关键。

$$
\text{RMS}(x) = \sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2 + \epsilon}
$$

$$
\bar{x}_i = \frac{x_i}{\text{RMS}(x)}
$$

$$
y_i = \gamma_i \bar{x}_i
$$

**差异点**：

1. **无减均值操作**：少了一步计算，理论上更快。
2. **无 Bias 参数 ($\beta$)**：输出分布强制围绕 0 点。这也是为什么 LLaMA 的 Linear 层去掉了 Bias——因为 RMSNorm 会在下一层把 Bias 带来的偏移“抹平”一部分，不如直接去掉。

---

### 2. 🧬 架构与算法细节 (Architecture & Algorithm)

除了归一化公式本身，**归一化层的位置**是决定 LLM 命运的关键架构决策。

#### 2.1 Post-Norm vs. Pre-Norm (架构之争)

* **Post-Norm (Original Transformer / BERT)**:

  $$
  x_{t+1} = \text{Norm}(x_t + \text{Sublayer}(x_t))
  $$

  * *问题*: 梯度必须流经 Norm 层。在初始化阶段，Norm 层的梯度可能会非常小或非常大，导致深层模型难以训练。必须配合 **Warm-up** 策略。
* **Pre-Norm (GPT-2 / LLaMA / DeepSeek)**:

  $$
  x_{t+1} = x_t + \text{Sublayer}(\text{Norm}(x_t))
  $$

  * *优势*: 存在一条**恒等路径 (Identity Path)** $x_t$ 直接连接到 $x_{t+1}$。这意味着梯度可以像在 ResNet 中一样，无损地流向最底层的 Embedding。
  * *代价*: 理论上，Pre-Norm 限制了深层的表示能力（虽然在 LLM 规模下可忽略），但换来了极佳的训练稳定性，允许训练成百上千层的模型。

#### 2.2 DeepSeek/LLaMA 的具体实现

DeepSeek 和 LLaMA 均采用 **Pre-RMSNorm** 架构。

**PyTorch 代码实现 (RMSNorm)**:
注意数值稳定性的细节：在计算平方和时，即使输入是 BF16/FP16，通常也强制转化为 **FP32** 进行累加，防止溢出或精度丢失。

```python
import torch
import torch.nn as nn

class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        # 这里的 weight 即公式中的 gamma
        # LLaMA 不使用 bias (beta)
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        # 1. 强制转换为 FP32 进行统计量计算 (关键工程细节)
        # x shape: [Batch, Seq, Dim]
        # output: [Batch, Seq, 1]
        mean_square = x.pow(2).mean(-1, keepdim=True)
        x = x * torch.rsqrt(mean_square + self.eps)
        return x

    def forward(self, x):
        # 2. 转换回输入的数据类型 (如 BF16)
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

---

### 3. 📉 系统与工程实现 (System & Engineering)

#### 3.1 显存带宽瓶颈 (Memory-Bound Operation)

归一化操作是典型的 **Element-wise** 操作。

* 计算量 (FLOPs): $O(N \cdot d)$，非常小。
* 访存量 (Memory Access): 读取 $x$ ($N \cdot d$)，写入 $y$ ($N \cdot d$)。
* 瓶颈：Norm 层本身不耗算力，但耗带宽。在 Transformer 中，它频繁出现在 Attention 和 FFN 之前，如果不优化，会严重拖慢训练速度。

#### 3.2 算子融合 (Kernel Fusion)

为了解决带宽瓶颈，高性能推理库（TensorRT-LLM, vLLM, FlashInfer）和训练库（Megatron-LM, DeepSpeed）都会使用 **Fused RMSNorm**。

* **原理**：编写 CUDA/Triton Kernel，将计算 $\sum x^2$、开根号、乘 $\gamma$ 的操作在一个 Kernel 中完成。
* **Warp Shuffle**: 在 GPU 内部，利用 Warp 级原语（`__shfl_down_sync`）进行快速归约（Reduction）求和，避免显存读写。
* **效果**: 相比 PyTorch 原生实现，Fused RMSNorm 通常有 5-10 倍的速度提升。

---

### 4. ⚔️ 关键权衡 (Trade-offs)

#### 4.1 为什么放弃 LayerNorm 选择 RMSNorm？

| 特性                 | LayerNorm                    | RMSNorm                      | 评价                                                       |
| :------------------- | :--------------------------- | :--------------------------- | :--------------------------------------------------------- |
| **计算统计量** | Mean, Variance               | RMS                          | RMSNorm 少算一个 Mean，少做一次减法                        |
| **参数量**     | $2d$ ($\gamma, \beta$)   | $d$ ($\gamma$)           | 节省了一半参数（虽然 Norm 参数占比极小）                   |
| **数值特性**   | 移位不变性 (Shift Invariant) | 缩放不变性 (Scale Invariant) | RMSNorm 假设语义主要由向量方向和相对幅度决定，而非绝对偏移 |
| **速度**       | 较慢                         | **快 10-40%**          | 在算子融合后，RMSNorm 的指令数更少                         |

**结论**：在大模型时代，任何能节省哪怕 1% 训练时间的技术都会被采用。RMSNorm 在保持性能不降（甚至在某些任务微升）的前提下，简化了计算，因此成为 SOTA。

#### 4.2 Scale 参数 ($\gamma$) 的初始化

在 Pre-Norm 架构中，如果模型极深，输出方差会随着层数累积而线性增长。
一些架构（如 DeepNorm）建议将 $\gamma$ 初始化为小于 1 的值，或者根据层数 $L$ 进行缩放。
但在 LLaMA 标准实现中，$\gamma$ 依然初始化为 1，完全依靠 Residual Connection 和 RMSNorm 的自适应能力来平衡。

---

### 5. 📚 论文与前沿引用 (References)

1. **"Layer Normalization"** (Ba et al., 2016)
   * *Focus*: The fundamental paper introducing LN as a batch-independent normalization.
2. **"Root Mean Square Layer Normalization"** (Zhang & Sennrich, 2019)
   * *Focus*: Proposed RMSNorm, highlighting that re-centering is optional.
3. **"On Layer Normalization in the Transformer Architecture"** (Xiong et al., 2020)
   * *Focus*: Theoretically proved why Pre-Norm allows for faster convergence and deeper networks (gradient norms behave better).

---

### 🚀 课后思考 (Next Step)

现在我们的模型有了：

1. **骨架**: Transformer Block (Pre-Norm 架构)
2. **肌肉**: SwiGLU (FFN)
3. **关节**: RMSNorm

接下来，我们需要给模型注入“辨别万物差异”的能力。为什么“银行的行”和“行走的行”通过 **Embedding** 和 **位置编码** 能被区分？**为什么 LLaMA 选择了 RoPE 而不是 T5 的相对位置编码？**

请发送 **`第08课：Embedding 与位置编码 (SOTA篇)`**。（注：根据新课程表逻辑，基础数学已讲完，我们直接切入 Transformer 核心组件深水区）。
