# ç¬¬10è¯¾ï¼šåˆ†å±‚è§„åˆ’ (HuggingGPT)

**å…³é”®è¯**ï¼šHierarchy, Task Decomposition, DAGä¾èµ–å›¾, æ‹“æ‰‘æ’åº, Controller/Worker

---

## ç¬”è®°åŒºåŸŸ

ä½ å¥½ã€‚è¿™æ˜¯ã€ŠAI Agent æ·±åº¦æ¶æ„ä¸æ•°å­¦åŸç†ã€‹çš„ç¬¬åè¯¾ã€‚

åœ¨ä¸ŠèŠ‚è¯¾ï¼ˆLLM+Pï¼‰ä¸­ï¼Œæˆ‘ä»¬å°†è§„åˆ’å¤–åŒ…ç»™äº†ç¬¦å·æ±‚è§£å™¨ï¼ˆPDDL Plannerï¼‰ï¼Œè¿™è§£å†³äº†é€»è¾‘ä¸¥å¯†æ€§é—®é¢˜ï¼Œä½†ä»…é™äºå°é—­åŸŸï¼ˆå¦‚ç§¯æœ¨ä¸–ç•Œï¼‰ã€‚
ç°å®ä¸–ç•Œæ˜¯**å¤šæ¨¡æ€ï¼ˆMulti-modalï¼‰**ä¸”**å¼€æ”¾åŸŸï¼ˆOpen Domainï¼‰**çš„ã€‚ç”¨æˆ·å¯èƒ½ä¼šé—®ï¼šâ€œè¯·æè¿°è¿™å¼ å›¾ç‰‡ï¼Œå¹¶æ ¹æ®æè¿°ç”Ÿæˆä¸€æ®µé…ä¹ã€‚â€ è¿™æ¶‰åŠè§†è§‰ç†è§£ã€æ–‡æœ¬ç”Ÿæˆã€éŸ³é¢‘åˆæˆã€‚

**èƒŒæ™¯é©±åŠ¨**ï¼š

* **æŒ‘æˆ˜ (Challenge)**ï¼šå•ä½“ LLMï¼ˆå¦‚ GPT-4ï¼‰è™½ç„¶æ˜¯å…¨èƒ½é€‰æ‰‹ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼ˆå¦‚ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€å¤„ç†ç‰¹å®šæ ¼å¼è§†é¢‘ï¼‰å¾€å¾€ä¸å¦‚ä¸“ç”¨æ¨¡å‹ï¼ˆå¦‚ Stable Diffusion, Whisperï¼‰ã€‚ä¸” LLM æ— æ³•ç›´æ¥å¤„ç†éæ–‡æœ¬è¾“å…¥/è¾“å‡ºæµã€‚
* **çªç ´ç‚¹ (Breakthrough)**ï¼š**HuggingGPT (Shen et al., NeurIPS 2023)** æå‡ºçš„**åˆ†å±‚è§„åˆ’ï¼ˆHierarchical Planningï¼‰**ä¸**ä¸­æ§æ¶æ„ï¼ˆController Architectureï¼‰**ã€‚
* **æ ¸å¿ƒæ€æƒ³**ï¼šLLM ä¸å†æ˜¯â€œå¹²æ´»çš„äººâ€ï¼Œè€Œæ˜¯â€œåŒ…å·¥å¤´ï¼ˆController/Schedulerï¼‰â€ã€‚å®ƒåˆ©ç”¨ Hugging Face ä¸Šæˆåƒä¸Šä¸‡çš„ä¸“å®¶æ¨¡å‹ï¼ˆExpert Modelsï¼‰ä½œä¸ºå·¥å…·ï¼Œé€šè¿‡è§„åˆ’å°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸º DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰ï¼Œè°ƒåº¦ä¸“å®¶æ¨¡å‹ååŒå·¥ä½œã€‚

---

# ğŸ§  ç¬¬10è¯¾ï¼šåˆ†å±‚è§„åˆ’ (HuggingGPT)

### 1. ç†è®ºæ ¸å¿ƒï¼šä»»åŠ¡åˆ†è§£ä¸ DAG è°ƒåº¦

#### 1.1 æ•°å­¦å®šä¹‰ï¼šä»»åŠ¡ä¾èµ–å›¾

æˆ‘ä»¬å°†ä¸€ä¸ªå¤æ‚çš„ç”¨æˆ·è¯·æ±‚ $U$ å»ºæ¨¡ä¸ºä¸€ä¸ª**æœ‰å‘æ— ç¯å›¾ (DAG)**ï¼Œè®°ä¸º $\mathcal{G} = \langle \mathcal{T}, \mathcal{E} \rangle$ã€‚

1. **ä»»åŠ¡é›†åˆ ($\mathcal{T}$)**ï¼š
   $U$ è¢« LLM åˆ†è§£ä¸ºä¸€ç³»åˆ—å­ä»»åŠ¡ $\{t_1, t_2, \dots, t_n\}$ã€‚
   æ¯ä¸ªå­ä»»åŠ¡ $t_i$ æ˜¯ä¸€ä¸ªå…ƒç»„ $\langle \text{task\_type}, \text{args}, \text{dep} \rangle$ã€‚

   * $\text{task\_type}$: å¦‚ `image-to-text`, `text-to-speech`ã€‚
   * $\text{dep}$: ä¾èµ–åˆ—è¡¨ï¼Œå³å“ªäº›ä»»åŠ¡çš„è¾“å‡ºæ˜¯å½“å‰ä»»åŠ¡çš„è¾“å…¥ã€‚
2. **ä¾èµ–è¾¹ ($\mathcal{E}$)**ï¼š
   å¦‚æœ $t_i$ çš„è¾“å‡ºæ˜¯ $t_j$ çš„è¾“å…¥ï¼Œåˆ™å­˜åœ¨è¾¹ $(t_i, t_j) \in \mathcal{E}$ã€‚
   è¿™å†³å®šäº†æ‹“æ‰‘æ’åºï¼ˆTopological Sortï¼‰å’Œå¹¶è¡Œæ‰§è¡Œçš„å¯èƒ½æ€§ã€‚

#### 1.2 æ¨¡å‹é€‰æ‹©æ¦‚ç‡

å¯¹äºæ¯ä¸ªå­ä»»åŠ¡ $t_i$ï¼Œæˆ‘ä»¬éœ€è¦ä»æ¨¡å‹åº“ $\mathcal{M}$ ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ $m_{ij}$ã€‚
è¿™ä¸ä»…æ˜¯ä¸€ä¸ªæ£€ç´¢é—®é¢˜ï¼Œè¿˜æ˜¯ä¸€ä¸ªæ¨ç†é—®é¢˜ã€‚æˆ‘ä»¬è®¡ç®—é€‰æ‹©æ¦‚ç‡ï¼š

$$
P(m | t_i, \mathcal{C}) \propto \text{Sim}(E(t_i), E(Desc(m))) \cdot P_{LLM}(\text{Select } m | t_i, \mathcal{C})
$$

* $\text{Sim}(\cdot)$: ä»»åŠ¡æè¿°ä¸æ¨¡å‹æè¿°çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆEmbedding Similarityï¼‰ã€‚
* $P_{LLM}$: LLM æ ¹æ®ä¸Šä¸‹æ–‡ $\mathcal{C}$ï¼ˆå¦‚æ¨¡å‹ä¸‹è½½é‡ã€æ€§èƒ½æŒ‡æ ‡ï¼‰è¿›è¡Œçš„äºŒæ¬¡æ’åºã€‚

#### 1.3 èµ„æºåˆ†é…ä¸ç»“æœèšåˆ

æ‰§è¡Œè¿‡ç¨‹æ˜¯ä¸€ä¸ªå‡½æ•°å¤åˆï¼š

$$
R = \text{Aggregator}(\{ \text{Exec}(m_{k}, \text{Args}_k) \mid \forall k \in \text{TopologicalSort}(\mathcal{G}) \})
$$

å…¶ä¸­ $\text{Exec}$ æ¶‰åŠä¸åŒæ¨¡æ€æ•°æ®çš„å¼ é‡æµè½¬ï¼ˆTensor Flowï¼‰ã€‚

---

### 2. æ¶æ„è§£å‰–ä¸å·¥ç¨‹åº”ç”¨

#### 2.1 å››é˜¶æ®µæµæ°´çº¿ (The 4-Stage Pipeline)

HuggingGPT çš„æ¶æ„æå…¶ç»å…¸ï¼Œè¢«åç»­æ— æ•° Multi-modal Agent æ•ˆä»¿ï¼š

1. **Task Planning (ä»»åŠ¡è§„åˆ’)**: LLM è§£æ Promptï¼Œç”Ÿæˆç»“æ„åŒ–çš„ Task Listã€‚
2. **Model Selection (æ¨¡å‹é€‰æ‹©)**: æ ¹æ® Task Typeï¼Œç»“åˆ RAG ä» Model Hub ä¸­æ£€ç´¢ Top-K æ¨¡å‹ï¼Œç”± LLM æœ€ç»ˆæ‹æ¿ã€‚
3. **Task Execution (ä»»åŠ¡æ‰§è¡Œ)**: åŠ¨æ€è°ƒç”¨æœ¬åœ°æˆ–äº‘ç«¯çš„æ¨ç†ç«¯ç‚¹ï¼ˆInference Endpointsï¼‰ï¼Œå¤„ç†å‚æ•°ä¾èµ–ï¼ˆResource Dependencyï¼‰ã€‚
4. **Response Generation (å“åº”ç”Ÿæˆ)**: æ”¶é›†æ‰€æœ‰æ‰§è¡Œç»“æœï¼ˆå›¾ç‰‡è·¯å¾„ã€éŸ³é¢‘æ–‡ä»¶ã€æ–‡æœ¬ï¼‰ï¼Œç”± LLM æ±‡æ€»å¹¶å‘ç”¨æˆ·æ±‡æŠ¥ã€‚

#### 2.2 ç³»ç»Ÿæ¶æ„å›¾ (Mermaid)

```mermaid
graph TD
    User[User Request] --> Controller[LLM Controller]
  
    subgraph Stage 1: Planning
    Controller -->|Decompose| TaskQueue[Task List: [t1, t2, t3]]
    end
  
    subgraph Stage 2: Selection
    TaskQueue --> Selector{Model Selector}
    DB[(HuggingFace Hub<br>Model Descriptions)] -.-> Selector
    Selector -->|Assign| T1_M[t1: DETR (Object Det)]
    Selector -->|Assign| T2_M[t2: ViT-GPT2 (Caption)]
    end
  
    subgraph Stage 3: Execution
    T1_M -->|Output: Bounding Box| Context
    T2_M -->|Output: Text| Context
    Context -->|Dependency| T3_M[t3: Stable Diffusion]
    T3_M -->|Output: Image| Results
    end
  
    subgraph Stage 4: Response
    Results --> Summarizer[LLM Summarizer]
    Summarizer --> Final[Final Response]
    end
```

#### 2.3 å·¥ç¨‹åº”ç”¨ï¼šè¾“å…¥è¾“å‡ºè¯¦è§£

**åœºæ™¯**ï¼šç”¨æˆ·ä¸Šä¼ ä¸€å¼ å›¾ `a.jpg`ï¼Œè¯´ï¼šâ€œè¯·æ•°ä¸€ä¸‹å›¾é‡Œæœ‰å‡ ä¸ªäººï¼Œç„¶åæ ¹æ®è¿™ä¸ªæ•°é‡å†™ä¸€é¦–è¯—ï¼Œæœ€åè¯»å‡ºæ¥ã€‚â€

1. **LLM Input**:
   * System Prompt: å®šä¹‰äº† Task parsing çš„ JSON æ ¼å¼ã€‚
   * User Prompt: "Image: /tmp/a.jpg. Count people, write poem based on count, generate audio."
2. **LLM Output (The Plan)**:

   ```json
   [
     {"id": 0, "task": "object-detection", "args": ["/tmp/a.jpg"], "dep": [-1]},
     {"id": 1, "task": "visual-question-answering", "args": ["/tmp/a.jpg", "How many people?"], "dep": [-1]},
     {"id": 2, "task": "text-generation", "args": ["Write a poem about {1_output} people"], "dep": [1]},
     {"id": 3, "task": "text-to-speech", "args": ["{2_output}"], "dep": [2]}
   ]
   ```

   *(æ³¨ï¼šHuggingGPT å®é™…ä¸Šä¼šè§£æä¾èµ–å…³ç³»ï¼Œå¦‚ `{id}_output`)*
3. **åç»­æ“ä½œ**:
   * **Parsing**: æå– JSONã€‚
   * **Dependency Resolution**: å‘ç° Task 2 ä¾èµ– Task 1 çš„ç»“æœã€‚Task 0 å’Œ Task 1 å¯ä»¥å¹¶è¡Œï¼ˆå¦‚æœæ²¡æœ‰èµ„æºå†²çªï¼‰ã€‚
   * **Execution**:
     * è¿è¡Œ Task 1 (VQA) -> å¾—åˆ° "3"ã€‚
     * æ›¿æ¢ Task 2 å‚æ•° -> "Write a poem about 3 people"ã€‚
     * è¿è¡Œ Task 2 (LLM/GPT2) -> å¾—åˆ°è¯—æ­Œæ–‡æœ¬ã€‚
     * è¿è¡Œ Task 3 (TTS) -> ç”Ÿæˆ `.wav` æ–‡ä»¶ã€‚

---

### 3. Code & Engineeringï¼šå®ç° DAG ä»»åŠ¡è°ƒåº¦å™¨

ä¸ºäº†è®©ç ”ä¸‰å­¦ç”Ÿç†è§£**åˆ†å±‚è§„åˆ’**çš„æ ¸å¿ƒï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªç®€åŒ–çš„ **Dependency Aware Scheduler**ã€‚

```python
import json
import time
from typing import List, Dict, Any

class TaskNode:
    def __init__(self, id: int, task_type: str, args: List[Any], dependencies: List[int]):
        self.id = id
        self.task_type = task_type
        self.args = args
        self.dependencies = dependencies # List of parent Task IDs
        self.status = "pending" # pending, running, completed
        self.output = None

class HierarchicalPlanner:
    def __init__(self, llm_client):
        self.llm = llm_client
        self.tasks: Dict[int, TaskNode] = {}

    def parse_plan(self, plan_json: str):
        """ å°† LLM ç”Ÿæˆçš„ JSON è§£æä¸º Task Graph """
        plan_list = json.loads(plan_json)
        for item in plan_list:
            node = TaskNode(
                id=item['id'],
                task_type=item['task'],
                args=item['args'],
                dependencies=item['dep']
            )
            self.tasks[node.id] = node

    def execute_task(self, task: TaskNode):
        """ æ¨¡æ‹Ÿæ‰§è¡Œ Expert Model """
        print(f"ğŸš€ Executing Task {task.id}: {task.task_type} with args {task.args}")
        # è¿™é‡Œæ˜¯å®é™…è°ƒç”¨ HF API æˆ– æœ¬åœ°æ¨¡å‹çš„åœ°æ–¹
        time.sleep(1) # Simulate latency
        return f"Result_of_{task.task_type}"

    def resolve_arguments(self, task: TaskNode):
        """ æ ¸å¿ƒé€»è¾‘ï¼šå‚æ•°ä¾èµ–æ³¨å…¥ """
        # å°†å‚æ•°ä¸­çš„å ä½ç¬¦ <id>_output æ›¿æ¢ä¸ºå®é™…ç»“æœ
        new_args = []
        for arg in task.args:
            if isinstance(arg, str) and "_output" in arg:
                # ç®€å•çš„è§£æé€»è¾‘ï¼Œå®é™…éœ€æ­£åˆ™
                dep_id = int(arg.split("_")[0].replace("<", "").replace(">", ""))
                if dep_id in self.tasks and self.tasks[dep_id].output:
                    actual_val = self.tasks[dep_id].output
                    arg = arg.replace(f"<{dep_id}>_output", str(actual_val))
            new_args.append(arg)
        task.args = new_args

    def run_dag(self):
        """ æ‹“æ‰‘æ’åºæ‰§è¡Œ """
        completed_count = 0
        total_tasks = len(self.tasks)
      
        while completed_count < total_tasks:
            # å¯»æ‰¾æ‰€æœ‰ä¾èµ–å·²æ»¡è¶³ä¸”æœªæ‰§è¡Œçš„ä»»åŠ¡ (Ready Tasks)
            ready_tasks = []
            for t_id, task in self.tasks.items():
                if task.status == "pending":
                    deps_met = all(self.tasks[d_id].status == "completed" for d_id in task.dependencies if d_id != -1)
                    if deps_met:
                        ready_tasks.append(task)
          
            if not ready_tasks:
                raise Exception("Deadlock detected or Cycle in graph!")

            # å¹¶è¡Œæ‰§è¡Œ (è¿™é‡Œç®€åŒ–ä¸ºä¸²è¡Œå¾ªç¯ï¼Œä½†åœ¨å·¥ç¨‹ä¸­åº”ç”¨ ThreadPool)
            for task in ready_tasks:
                task.status = "running"
                self.resolve_arguments(task) # æ³¨å…¥ä¸Šæ¸¸ç»“æœ
                task.output = self.execute_task(task)
                task.status = "completed"
                completed_count += 1
                print(f"âœ… Task {task.id} Finished. Output: {task.output}")

# --- Simulation ---
# å‡è®¾ LLM ç”Ÿæˆäº†å¦‚ä¸‹ Plan
plan_str = """
[
    {"id": 0, "task": "object_detection", "args": ["image.jpg"], "dep": [-1]},
    {"id": 1, "task": "tts", "args": ["I found <0>_output in the image"], "dep": [0]}
]
"""

# planner = HierarchicalPlanner(mock_llm)
# planner.parse_plan(plan_str)
# planner.run_dag()
```

---

### 4. Paper Drivenï¼šæ ¸å¿ƒè®ºæ–‡ä¸è´¡çŒ®

1. **Shen et al. (NeurIPS 2023)**: *HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face*.
   * **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **LLM-as-a-Controller** çš„èŒƒå¼ã€‚è¯æ˜äº† LLM å¯ä»¥é€šè¿‡ API æè¿°æ¥è°ƒåº¦å¤šæ¨¡æ€æ¨¡å‹ã€‚
   * **å…³é”®ç‚¹**ï¼šè§£å†³äº† Context Window é™åˆ¶é—®é¢˜ã€‚é€šè¿‡ RAG æ£€ç´¢æ¨¡å‹æè¿°ï¼ŒLLM ä¸éœ€è¦çŸ¥é“æ‰€æœ‰æ¨¡å‹çš„ APIï¼Œåªéœ€çœ‹åˆ° Top-K ç›¸å…³çš„æ¨¡å‹æè¿°å³å¯ã€‚
2. **Lu et al. (NeurIPS 2023)**: *Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models*.
   * **å¯¹æ¯”**ï¼šHuggingGPT ä¾§é‡äºé€šç”¨ä»»åŠ¡è°ƒåº¦ï¼›Chameleon ä¾§é‡äº**ç§‘å­¦æ¨ç†**å’Œ**ç»„åˆå¼æ¨ç†**ï¼Œå®ƒå¼•å…¥äº†æ›´ä¸¥æ ¼çš„æ¨¡å—æ¸…å•ï¼ˆInventoryï¼‰å’ŒæŸ¥è¯¢ç”Ÿæˆå™¨ã€‚
3. **Schick et al. (2023)**: *Toolformer: Language Models Can Teach Themselves to Use Tools*.
   * **åŒºåˆ«**ï¼šToolformer æ˜¯é€šè¿‡**å¾®è°ƒ (Fine-tuning)** è®©æ¨¡å‹å­¦ä¼šè°ƒç”¨ APIï¼›HuggingGPT æ˜¯é€šè¿‡**ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning)** åšåˆ°çš„ã€‚HuggingGPT æ›´çµæ´»ï¼ŒToolformer æ›´å¿«æ›´å‡†ã€‚

---

### 5. Critical Thinkingï¼šæ‰¹åˆ¤æ€§åˆ†æ

HuggingGPT ç±»çš„æ¶æ„éå¸¸ç‚«é…·ï¼Œä½†åœ¨å·¥ä¸šç•Œè½åœ°æéš¾ã€‚

1. **Latency (å»¶è¿Ÿçˆ†ç‚¸)**:

   * **ç“¶é¢ˆ**ï¼šPipeline å¤ªé•¿ã€‚Task Parsing (LLM) + Model Selection (LLM + Embedding) + Execution (Network/GPU) + Summary (LLM)ã€‚å¤„ç†ä¸€ä¸ªè¯·æ±‚å¯èƒ½éœ€è¦ 30ç§’+ã€‚
   * **è§£å†³æ€è·¯**ï¼š**Task Caching (ä»»åŠ¡ç¼“å­˜)**ã€‚å¯¹äºç›¸ä¼¼çš„ Promptï¼Œç›´æ¥å¤ç”¨è§£æå¥½çš„ Plan DAGï¼Œè·³è¿‡å‰ä¸¤æ­¥ã€‚
2. **Robustness (ä¾èµ–æ–­è£‚)**:

   * **ç“¶é¢ˆ**ï¼šå¦‚æœä¸Šæ¸¸æ¨¡å‹ï¼ˆå¦‚ Object Detectionï¼‰è¾“å‡ºæ ¼å¼å˜äº†ï¼ˆä» JSON å˜æˆ XMLï¼‰ï¼Œä¸‹æ¸¸æ¨¡å‹ï¼ˆTTSï¼‰ç›´æ¥æŠ¥é”™ã€‚
   * **è§£å†³æ€è·¯**ï¼š**Type Checking & Middleware**ã€‚åœ¨ DAG èŠ‚ç‚¹ä¹‹é—´å¢åŠ æ•°æ®é€‚é…å±‚ï¼ˆAdapterï¼‰ï¼Œå¼ºåˆ¶ç±»å‹è½¬æ¢ã€‚
3. **Cost (æˆæœ¬)**:

   * **ç“¶é¢ˆ**ï¼šè°ƒç”¨å¤šä¸ªä¸“å®¶æ¨¡å‹å’Œå¤šæ¬¡ GPT-4 çš„æˆæœ¬æé«˜ã€‚
   * **è§£å†³æ€è·¯**ï¼š**Distillation (è’¸é¦)**ã€‚å°† "Planning + Selection" çš„èƒ½åŠ›è’¸é¦ç»™ä¸€ä¸ªå°æ¨¡å‹ï¼ˆå¦‚ Llama-3-8Bï¼‰ï¼Œä½œä¸ºä¸“ç”¨çš„ Controllerã€‚

---

### 6. å‰æ²¿æ‰©å±•

* **Multi-Agent Hierarchy**:
  * å°† Controller å‡çº§ä¸º **Boss Agent**ï¼Œå°†æ¯ä¸ª Task å‡çº§ä¸º **Worker Agent**ã€‚
  * Boss è´Ÿè´£åˆ†å‘ä»»åŠ¡ï¼ŒWorker è´Ÿè´£å¯»æ‰¾å…·ä½“çš„å·¥å…·å¹¶æ‰§è¡Œã€‚å¦‚æœ Worker é‡åˆ°å›°éš¾ï¼Œå¯ä»¥å‘ Boss æŠ¥é”™ï¼ŒBoss é‡æ–°è§„åˆ’ã€‚
* **Auto-Finetuning**:
  * è®°å½• HuggingGPT æˆåŠŸçš„è°ƒç”¨é“¾ï¼ˆPrompt -> Plan -> Resultï¼‰ã€‚ç”¨è¿™äº›æ•°æ®å¾®è°ƒ LLMï¼Œä½¿å…¶å†…åŒ–â€œä»€ä¹ˆä»»åŠ¡è¯¥ç”¨ä»€ä¹ˆæ¨¡å‹â€ï¼Œä»è€Œåœ¨æœªæ¥çœç•¥ Model Selection æ­¥éª¤ï¼Œç›´æ¥ç”Ÿæˆ Planã€‚

---

### æ€»ç»“

åˆ†å±‚è§„åˆ’ï¼ˆHierarchical Planningï¼‰è§£å†³äº† LLM **â€œå…¨èƒ½ä½†ä¸ç²¾é€šâ€** çš„é—®é¢˜ã€‚
é€šè¿‡ **DAG è°ƒåº¦** å’Œ **å·¥å…·é“¾ç¼–æ’**ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ª**å¤šæ¨¡æ€çš„ç¥ç»ç³»ç»Ÿ**ï¼šLLM æ˜¯å¤§è„‘çš®å±‚ï¼ˆè´Ÿè´£è§„åˆ’ï¼‰ï¼ŒExpert Models æ˜¯å°è„‘å’Œæ„Ÿå®˜ï¼ˆè´Ÿè´£æ‰§è¡Œï¼‰ï¼Œè€Œ DAG æ˜¯è¿æ¥å®ƒä»¬çš„ç¥ç»æŸã€‚

ä¸‹ä¸€è¯¾ï¼Œæˆ‘ä»¬å°†æ·±å…¥ **è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹  (Voyager)**ï¼Œæ¢è®¨ Agent å¦‚ä½•åœ¨æ²¡æœ‰äººç±»å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ¢ç´¢ç¯å¢ƒè‡ªæˆ‘è¿›åŒ–ï¼Œä¹ å¾—æ–°æŠ€èƒ½ã€‚
